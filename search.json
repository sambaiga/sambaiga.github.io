[
  {
    "objectID": "blog/2025/12/2025-12-1-bayesian-regression..html",
    "href": "blog/2025/12/2025-12-1-bayesian-regression..html",
    "title": "Bayesian Regression: A Real-World Battery Degradation Case Study",
    "section": "",
    "text": "Predictive modeling in industrial settings is rarely just about accuracy. Decisions informed by models often carry financial, safety, and operational risks. In such environments, understanding uncertainty can be just as important as generating a good point prediction.\nThis article offers a practical introduction to Bayesian regression through a real-world case study: predicting lithium-ion battery degradation. Instead of treating model parameters as fixed, unknown values, the Bayesian approach frames them as probability distributions‚Äîexplicitly modeling uncertainty to give engineers and data scientists a richer, more actionable understanding of system behavior.\n\nüõ†Ô∏è Action: If you prefer to learn by doing, you can reproduce this article using the accompanying resources:\n\n\nRepository: Fork the bayesian-modelling repository and follow the setup instructions in the README.md\nNotebook: Launch 2025-12-1-bayesian-regression.ipynb in the notebook folder to follow along step-by-step.\n\nThis post is part of the Bayesian Modelling for Industrial Applications series ‚Äì Part 2 . If you are new to Bayesian methods, you may want to start with the first post Part 1, which introduces the foundational concepts of Bayesian inference.\n\n\nIntroduction\nWelcome back to our series on Bayesian Modelling for Industrial Applications. In Part 1, we explored how Bayesian thinking provides a principled framework for decision-making under uncertainty when evidence is limited.\nIn this post, we extend that foundation to continuous prediction problems, showing how Bayesian regression transforms noisy industrial data into actionable insights with uncertainty explicitly quantified rather than ignored.\n\nWhy Bayesian regression?\nImagine managing a fleet of electric vehicles. One of your biggest challenges is predicting battery State of Health (SoH) over time. Early predictions inform warranty decisions, maintenance planning, and safety margins.\nIndustrial systems, like vehicle battery packs, are inherently complex: they are characterized by noisy, non-repeatable measurements due to sensor noise, unit variability, and stochastic physical processes.\nTraditional, purely deterministic regression methods (which produce a single ‚Äúbest-fit‚Äù curve) fail to capture this complexity. A deterministic model might predict one degradation curve, but in practice, real batteries age differently even under similar conditions. This reliance on a single point estimate overlooks critical, high-risk aspects of the industrial data:\n\nMeasurement uncertainty inherent in sensors and data acquisition systems\n\nUnit-to-unit variability in components (e.g., subtle differences between nominally identical batteries)\n\nRandom and nonlinear degradation behaviour\n\nLimited early-life observations, a common constraint in industrial testing\n\nBayesian regression addresses this reality by treating uncertainty as a first-class citizen. Instead of delivering a single prediction, it provides ranges of plausible outcomes, allowing decisions to be made with risk explicitly accounted for.\n\n\nModeling distributions with bayes‚Äô theorem\nBayesian regression models uncertainty by treating parameters as probability distributions rather than fixed values. Each coefficient represents a range of plausible effects, informed by both prior knowledge and observed data.\nThis framework is built on three core components:\n\nPriors, which encode existing engineering knowledge or physical constraints\n\nLikelihood, which links the model to noisy real-world measurements\n\nPosterior, which combines prior information and data into an updated belief\n\nThe posterior distribution enables credible intervals, allowing us to quantify how confident we are in both parameter estimates and future predictions‚Äîan essential capability in industrial decision-making.\n\n\n\nCase Study: Predicting battery degradation\nLithium-ion batteries are critical components in electric vehicles and stationary energy storage systems. Unexpected capacity loss can lead to service interruptions, safety risks, and costly premature replacements.\nThe challenge is predicting future battery health when: - Direct capacity measurements are infrequent and expensive - Early-life data is sparse - Degradation accelerates nonlinearly near end of life\nThe goal is not only to predict degradation, but to quantify uncertainty well enough to support maintenance and replacement decisions.\nWe use battery degradation data from the CALCE Battery Research Group at the University of Maryland. Battery State of Health (\\(\\text{SoH}\\)) is defined as current capacity relative to initial capacity (\\(\\text{SoH} = C/C_{max}\\)). This continuous value degrades non-linearly over the battery‚Äôs life, driven primarily by cycle count and operational conditions, with significant measurement noise. The CALCE dataset provides over 1,200 capacity measurements taken at discrete cycle intervals, alongside features like charging and discharge current and voltage. This comprehensive dataset has become a benchmark in battery research, allowing rigorous comparison of degradation models\n\n\nShow the code\nimport arviz as az\nfrom great_tables import GT, loc, md, style\nfrom IPython.display import clear_output\nfrom lets_plot import (\n    LetsPlot,\n    aes,\n    coord_cartesian,\n    facet_wrap,\n    flavor_high_contrast_dark,\n    geom_area,\n    geom_band,\n    geom_density,\n    geom_histogram,\n    geom_line,\n    geom_point,\n    geom_ribbon,\n    geom_vline,\n    gggrid,\n    ggplot,\n    ggsize,\n    guide_legend,\n    guides,\n    labs,\n    layer_tooltips,\n    scale_color_brewer,\n    scale_color_manual,\n    scale_fill_manual,\n    scale_y_continuous,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nfrom sklearn.preprocessing import  StandardScaler\n\naz.style.use(\"arviz-doc\")\n\nfrom bayes.plot.basic_plots import line_plot, modern_theme, pro_colors, scatter_plot\n\nLetsPlot.setup_html(isolated_frame=False, offline=True, no_js=True, show_status=False)\nnp.random.seed(42)\n\n\n\nChoosing the Beta Likelihood\nThe capacity data being modeled, \\(C\\), represents the battery‚Äôs health and is strictly bounded between zero and its initial (maximum) capacity, \\(C_{\\text{max}}\\). The goal of this analysis is to model the evolution of \\(C\\). In many standard regression approaches, the model‚Äôs likelihood function (which defines the distribution of the noise) is assumed to be Gaussian (Normal). This assumption is fundamentally incompatible with the physical reality of capacity degradation for two key reasons.\n\nGaussian models assume the target variable can take any real value (\\(-\\infty\\) to \\(+\\infty\\)), ignoring the fact that the underlying capacity \\(C\\) cannot fall outside their physical limits (i.e., \\([C_{\\text{min}}, C_{\\text{max}}]\\) )\nWith enough extrapolation, Gaussian models produce impossible values (e.g.,negative capacity). In safety-critical systems, such predictions are dangerous.\n\nThe Beta distribution is consequently chosen as the likelihood function for this regression problem. The Beta distribution is flexible, capable of modeling various shapes (uniform, U-shaped, skewed) depending on its shape parameters (\\(\\alpha\\) and \\(\\beta\\)).Furthermore, since battery capacity degradation is continuous and strictly bounded by \\([C_{\\text{min}}, C_{\\text{max}}]\\), a Beta likelihood provides a natural modeling choice that avoids the ad-hoc truncation required by Gaussian assumptions.\nTo fit the Beta‚Äôs intrinsic \\([0, 1]\\) domain, capacity (\\(C_i\\)) is first scaled into a normalized State of Health (\\(\\tilde{C}_i\\)) using the following transformation:\\[\\tilde{C}_i = \\frac{C_i - C_{\\text{min}}}{C_{\\text{max}} - C_{\\text{min}}}\\]The model then utilizes the Beta distribution for the scaled capacity:\\[\\tilde{C}_i \\sim \\text{Beta}(\\alpha_i, \\beta_i)\\]where \\(\\alpha_i, \\beta_i &gt; 0\\) and \\(\\tilde{C}_i \\in [0, 1]\\). This modeling choice ensures all predicted \\(\\tilde{C}_i\\) values remain physically plausible while allowing for flexible modeling of degradation patterns through the shape parameters.\nwith pm.Model() as battery_model:\n    pm.Beta(\"y_obs\", alpha=alpha, beta=beta_shape, observed=y_data)\n\nParameterization: Mean (\\(\\mu\\)) and Precision (\\(\\phi\\))\nTo make the parameters intuitive, the Beta distribution is typically reparameterized using the mean (\\(\\mu\\)) and the precision (\\(\\phi\\)).The shape parameters, \\(\\alpha_i\\) and \\(\\beta_i\\), which define the exact shape of the distribution for a given observation, are calculated directly from the mean \\(\\mu_{i} \\in (0, 1)\\) and the global precision \\(\\phi &gt; 0\\) such that:\\[\\alpha_i = \\mu_{i} \\cdot \\phi \\quad \\text{and} \\quad \\beta_i = (1 - \\mu_{i}) \\cdot \\phi\\] The precision parameter, \\(\\phi\\), controls the variance: a large \\(\\phi\\) means the predictions are tightly clustered around the mean \\(\\mu_{i}\\), indicating low uncertainty (low variance).\nwith pm.Model() as battery_model:\n    alpha = mu_scaled * phi\n    beta_shape = (1 - mu_scaled) * phi\nThe mean parameter \\(\\mu_{i} \\in (0, 1)\\) must be linked to our predictors. Since the mean is bounded by \\((0, 1)\\), we use the Logit Link Function to map the linear combination of predictors (\\(\\eta_i\\)) to this interval: \\[\\text{logit}(\\mu_{i}) = \\eta_i\\] as such , \\[\\mu_{i} = \\text{logit}^{-1}(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\\]\nwith pm.Model() as battery_model:\n    mu_scaled = pm.Deterministic(\"mu_scaled\", pm.math.sigmoid(logit_mu))\n\nüí° Key Takeaway: The Logit Link function is the mathematical bridge that ensures our mean prediction, \\(\\mu_i\\), respects the physical boundary of \\((0, 1)\\) imposed by the Beta distribution\n\n\n\nThe Linear Predictor: Capturing Degradation\nThe core of our predictive power lies in the linear predictor, \\(\\eta_i\\). It is structured to incorporate both the fundamental, non-linear degradation due to cycling and the linear operational effects from features \\(\\mathbf{x}\\) like voltage and current: \\[\\eta_i = \\underbrace{\\beta_0}_{\\text{Intercept}} + \\underbrace{f(k_i)}_{\\text{Non-linear Decay}} + \\underbrace{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}}_{\\text{Operational Effects}}\\]\nIn this work, the non-linear decay component is modeled as an exponential function: \\[f(k_i) = -A\\cdot(1-e^{-\\lambda \\cdot k_i})\\] where \\(k_i\\) is the cycle count, \\(\\lambda\\) is the degradation rate, and \\(A&gt;0\\) is a learnable amplitude parameter controlling the strength of decay. This captures the physical reality that battery capacity degrades rapidly at first and then more slowly over time. where:\nwith pm.Model() as battery_model:\n    degradation = pm.math.exp(-lambda_rate * cycle_data)\n    degradation_term = -degr_amp * (1 - degradation)\n    logit_mu = intercept + degradation_term + pm.math.dot(x_data, beta)\nThe components of \\(\\eta_i\\) are as follows:\n\nIntercept (\\(\\beta_0\\)): The baseline capacity on the logit scale when operational effects are zero and the cycle count (\\(k_i\\)) is zero.\nDegradation Term (\\(e^{-\\lambda \\cdot k_i}\\)): This is the non-linear exponential decay over the cycle count \\(k_i\\), controlled by the rate \\(\\lambda\\). This term ensures the capacity prediction naturally trends downward toward zero capacity over time.\nOperational Effects (\\(\\mathbf{x}_{i}^{\\top} \\boldsymbol{\\beta}\\)): This is a standard linear combination, where \\(\\boldsymbol{\\beta}\\) is the vector of coefficients for the standardized operational features \\(\\mathbf{x}_{i}\\).\n\nThis models how factors like maximum temperature accelerate or slow down the degradation.\n\nüß† Self-Test: You are modeling \\(\\text{SoH}\\), which must stay in \\([0, 1]\\). Your linear predictor, \\(\\eta_i = \\beta_0 + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\), can produce values ranging from \\(-\\infty\\) to \\(+\\infty\\).What would happen if you skipped the Logit Link Function and simply set \\(\\mu_i = \\eta_i\\)? Why is the Logit Link function mandatory for the Beta regression model?\n\n\n\n\nEncoding Knowledge with Priors\nIn Bayesian modeling, defining priors is a critical step. This step allows domain knowledge accumulated from battery engineering to be embedded directly into the model, ensuring that predictions remain physically plausible even when data is sparse. A prior distribution is assigned to every unknown parameter (\\(\\beta_0, \\boldsymbol{\\beta}, \\lambda, \\phi\\)). These priors act as soft constraints, preventing the model from learning extreme or non-physical relationships.\nThe Intercept (\\(\\beta_0\\))\nThe Intercept \\(\\beta_0\\) represents the initial capacity of the battery fleet on the logit scale. The orange curve in the figure below represents the selected informative prior, \\(\\text{Normal}(\\mu_{\\text{logit\\_start}}, 0.5^2)\\). A standard deviation of \\(\\sigma = 0.5\\) is chosen to balance prior knowledge (centering at \\(\\mu_{\\text{logit\\_start}}\\)) with sufficient uncertainty to allow the observed data to meaningfully influence the final estimate.\nwith pm.Model() as battery_model:\n    eps=1e-8\n    initial_logit_capacity_mean = -np.log(1-eps)\n    intercept = pm.Normal(\"intercept\", mu=initial_logit_capacity_mean, sigma=0.5)\n\n\nShow the code\nfrom bayes.plot.distribution import plot_density\n\n\n\n\nShow the code\nn = 1000\ns1 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.1), n)\ns2 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.2), n)\ns3 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.5), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=0.28, œÉ=0.1)\", \"(Œº=0.28, œÉ=0.2)\", \"(Œº=0.28, œÉ=0.5)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"Normal Distributions Intercept Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -2\n              \n            \n          \n          \n            \n            \n            \n              \n                -1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                -1\n              \n            \n          \n          \n            \n            \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Normal Distributions Intercept Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.2)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.5)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThe narrower blue (\\(\\sigma = 0.1\\)) and green (\\(\\sigma = 0.2\\)) curves represent highly concentrated priors that would strongly restrict the posterior estimates. The wider \\(\\sigma = 0.5\\) (orange) distribution corresponds to a more conservative informative prior, granting the initial capacity estimate \\(\\beta_0\\) a reasonable degree of uncertainty.\nOperational Effects (\\(\\boldsymbol{\\beta}\\))\nThe vector of coefficients \\(\\boldsymbol{\\beta}\\) controls the influence of operational features on capacity fade. Engineering knowledge suggests that, unless a feature is extreme, its immediate effect on capacity should be subtle, as the overall degradation process is primarily driven by cycle count.\n    with pm.Model() as battery_model:\n    beta = pm.Normal(\"beta\", mu=0, sigma=0.2, shape=n_features)\n\n\nShow the code\nfrom bayes.plot.distribution import plot_density\n\n\n\n\nShow the code\n\ns1 = pm.draw(pm.Normal.dist(mu=0, sigma=0.1), n)\ns2 = pm.draw(pm.Normal.dist(mu=0, sigma=0.2), n)\ns3 = pm.draw(pm.Normal.dist(mu=0, sigma=1.0), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=0, œÉ=0.1)\", \"(Œº=0, œÉ=0.2)\", \"(Œº=0, œÉ=1.0)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"Normal Distributions Beta Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -4\n              \n            \n          \n          \n            \n            \n            \n              \n                -3\n              \n            \n          \n          \n            \n            \n            \n              \n                -2\n              \n            \n          \n          \n            \n            \n            \n              \n                -1\n              \n            \n          \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Normal Distributions Beta Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=0.2)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=1.0)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nAs shown in the figure above, a tight informative prior, \\(\\text{Normal}(0, 0.2^2)\\), is used for \\(\\boldsymbol{\\beta}\\). Centering this prior at zero reflects the assumption that, on average, operational features have no effect, while the small standard deviation (\\(0.2\\)) requires strong evidence from the data before attributing a large effect to any single feature. This constraint prevents non-physical, abrupt changes in capacity predictions. In contrast, a broader prior such as \\(\\text{Normal}(0, 1.0^2)\\) (orange curve) allows extreme effects that are considered non-physical.\nDegradation rate \\(\\lambda\\)\nThe degradation rate \\(\\lambda\\) governs the exponential decay term \\(e^{-\\lambda k_i}\\). Since degradation must always occur and capacity cannot increase indefinitely, it is necessary to enforce \\(\\lambda &gt; 0\\). Accordingly, a Log-Normal prior, \\(\\text{LogNormal}(\\ln(0.005), 0.5^2)\\), is used for \\(\\lambda\\).\nwith pm.Model() as battery_model:\n    lambda_rate = pm.Lognormal(\"lambda_rate\", mu=np.log(0.01), sigma=0.5)\n\nüß† Self-Test: Recall that we set the prior for the fade rate \\(\\lambda\\) as \\(\\text{LogNormal}(\\ln(0.01), 0.5^2)\\) (where \\(\\sigma = 0.5\\)). What practical problem would arise if an engineer, overly confident in their historical knowledge, reset the prior to \\(\\text{LogNormal}(\\ln(0.01), 0.1^2)\\) (where \\(\\sigma = 0.1\\))?\n\n\n\nShow the code\n\ns1 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=0.1), n)\ns2 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=0.5), n)\ns3 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=1.0), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=In(0.005), œÉ=0.1)\", \"(Œº=In(0.005), œÉ=0.5)\", \"(Œº=In(0.005), œÉ=1.0)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"LogNormal Distributions Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.05\n              \n            \n          \n          \n            \n            \n            \n              \n                0.1\n              \n            \n          \n          \n            \n            \n            \n              \n                0.15\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.25\n              \n            \n          \n          \n            \n            \n            \n              \n                0.3\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                200\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n          \n            \n              \n                600\n              \n            \n          \n          \n            \n              \n                800\n              \n            \n          \n        \n      \n    \n    \n      \n        LogNormal Distributions Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=0.5)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=1.0)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThis weakly informative prior centers the expected degradation rate around \\(\\mathbf{0.5\\%}\\), while the spread \\(\\sigma = 0.5\\) (green/teal curve) is sufficiently wide to accommodate realistic fleet-level variability. At the same time, it remains substantially tighter than \\(\\sigma = 1.0\\) (orange curve), thereby avoiding non-physical probability mass assigned to unrealistically large degradation rates.\nThis distribution reflects a conservative estimate of uncertainty, allowing greater variation in degradation behavior than a tighter prior (e.g., \\(\\sigma = 0.1\\)) would permit, while still preventing implausible rates.\nDegradation Amplitude (\\(A\\))\nThe parameter degr_amp (\\(A\\)) controls the overall amplitude of the degradation component. Since this amplitude must be non-negative, a Half-Normal distribution is used, which has support only on positive values. The scale parameter \\(\\sigma\\) determines the strength of regularization.\n   with pm.Model() as battery_model:\n    degr_amp = pm.HalfNormal(\"degr_amp\", sigma=0.1)\n\n\nShow the code\n\ns1 = pm.draw(pm.HalfNormal.dist(sigma=0.1), n)\ns2 = pm.draw(pm.HalfNormal.dist(sigma=0.2), n)\ns3 = pm.draw(pm.HalfNormal.dist(sigma=0.5), n)\n\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"œÉ=0.1\", \"œÉ=0.2\", \"œÉ=0.5\"], n),\n    }\n)\nplot=plot_density(df, title=\"Gamma Distributions Phi Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n          \n            \n              \n                6\n              \n            \n          \n        \n      \n    \n    \n      \n        Gamma Distributions Phi Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.1\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.2\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nAs shown in the figure above, \\(\\text{HalfNormal}(\\sigma = 0.1)\\) strongly concentrates probability mass near zero, requiring substantial evidence before attributing a large degradation amplitude. In contrast, broader priors such as \\(\\text{HalfNormal}(\\sigma = 0.5)\\) place non-negligible probability on large, non-subtle amplitudes (up to approximately \\(1.0\\)), increasing the risk of overfitting by allowing the model to explain noise through the amplitude term.\nPrecision Parameter (\\(\\phi\\))\nThe precision parameter \\(\\phi\\) controls the variance of the Beta likelihood and represents the expected level of noise in the \\(\\text{SoH}\\) measurements. Accordingly, a highly informative Gamma prior, \\(\\text{Gamma}(100, 2)\\), is assigned to \\(\\phi\\).\nwith pm.Model() as battery_model:\n    phi = pm.Gamma(\"phi\", alpha=100, beta=2.0)\nThis prior is centered at \\(\\mathbb{E}[\\phi] = \\alpha / \\beta = 50\\) with a relatively small standard deviation (\\(\\sigma_{\\phi} = 5.0\\)), indicating high confidence in this expectation. This choice encodes the belief that sensor noise is low (\\(\\sigma_{\\text{noise}} \\approx 0.14\\)), reflecting the physical reality of precise laboratory-grade measurements.\n\n\nShow the code\n\ns1 = pm.draw(pm.Gamma.dist(alpha=10, beta=1), n)\ns2 = pm.draw(pm.Gamma.dist(alpha=50, beta=5), n)\ns3 = pm.draw(pm.Gamma.dist(alpha=100, beta=2), n)\n\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"Gamma(Œ±=10, Œ≤=1)\", \"Gamma(Œ±=50, Œ≤=5)\", \"Gamma(Œ±=100, Œ≤=2)\"], n),\n    }\n)\nplot=plot_density(df, title=\"Gamma Distributions Phi Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                10\n              \n            \n          \n          \n            \n            \n            \n              \n                20\n              \n            \n          \n          \n            \n            \n            \n              \n                30\n              \n            \n          \n          \n            \n            \n            \n              \n                40\n              \n            \n          \n          \n            \n            \n            \n              \n                50\n              \n            \n          \n          \n            \n            \n            \n              \n                60\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.05\n              \n            \n          \n          \n            \n              \n                0.1\n              \n            \n          \n          \n            \n              \n                0.15\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.25\n              \n            \n          \n        \n      \n    \n    \n      \n        Gamma Distributions Phi Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=10, Œ≤=1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=50, Œ≤=5)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=100, Œ≤=2)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nFrom the figure above, it is evident that \\(\\text{Gamma}(\\alpha = 100, \\beta = 2.0)\\) (orange curve) provides a strong belief in high precision. In contrast, \\(\\text{Gamma}(\\alpha = 10, \\beta = 1.0)\\) yields a lower expected precision with greater spread, allowing excessive uncertainty and risking a flat, unphysical prior predictive distribution. Alternative Gamma priors with the same expected precision but larger variance similarly underestimate the precision of modern sensors.\nThe complete model now combines all these components:\n\n\nShow the code\ndef beta_regression_model(\n    data: pd.DataFrame,\n    features: list[str],\n    target: str = \"capacity\",\n    scaler: StandardScaler | None = None,\n    lower_bound: float = 0.2,\n    upper_bound: float = 1.3,\n    eps: float = 1e-8,\n) -&gt; tuple[pm.Model, StandardScaler]:\n    \"\"\"Beta regression model for bounded battery capacity data using PyMC.\n\n    Capacity (SoH) is scaled to the (0, 1) interval for the Beta distribution.\n\n    Args:\n        data: DataFrame containing 'capacity', 'cycle', and feature columns.\n        features: List of column names used as predictors (X variables).\n        target: Name of the capacity column.\n        scaler: Pre-fitted StandardScaler object, or None to fit a new one.\n        lower_bound: Physical lower bound for capacity (for scaling).\n        upper_bound: Physical upper bound for capacity (for scaling).\n        eps: Small value to avoid boundary issues in Beta distribution.\n\n    Returns:\n        A tuple containing the PyMC model and the fitted/provided StandardScaler.\n    \"\"\"\n    # 1. Prepare Features (X)\n    if scaler is None:\n        scaler = StandardScaler()\n        x_scaled = scaler.fit_transform(data[features])\n    else:\n        x_scaled = scaler.transform(data[features])\n\n    # 2. Prepare Targets (Y)\n    y = data[target].values.astype(np.float64)\n    cycles = data[\"cycle\"].values.astype(np.float64)\n    n_features = len(features)\n\n    # Transform y to (0,1) interval and clip to avoid boundaries (0 or 1)\n    y_scaled = (y - lower_bound) / (upper_bound - lower_bound)\n    y_scaled = np.clip(y_scaled, eps, 1 - eps)\n\n    with pm.Model() as model:\n        # Data Containers\n        x_data = pm.Data(\"x_data\", x_scaled)\n        cycle_data = pm.Data(\"cycle_data\", cycles)\n        y_data = pm.Data(\"y_data\", y_scaled)\n\n        # Priors\n        initial_logit_capacity_mean = -np.log(1 - 1e-6)\n        intercept = pm.Normal(\"intercept\", mu=initial_logit_capacity_mean, sigma=0.5)\n        lambda_rate = pm.Lognormal(\"lambda_rate\", mu=np.log(0.005), sigma=0.5)\n        beta = pm.Normal(\"beta\", mu=0, sigma=0.2, shape=n_features)\n        phi = pm.Gamma(\"phi\", alpha=100, beta=2.0)\n        degr_amp = pm.HalfNormal(\"degr_amp\", sigma=0.1)\n\n        # Linear predictor (eta) on logit scale\n        degradation = pm.math.exp(-lambda_rate * cycle_data)\n        degradation_term = -degr_amp * (1 - degradation)\n        logit_mu = intercept + degradation_term + pm.math.dot(x_data, beta)\n\n        # Convert to probability scale (0,1)\n        mu_scaled = pm.Deterministic(\"mu_scaled\", pm.math.invlogit(logit_mu))\n\n        # Beta likelihood\n        alpha = mu_scaled * phi\n        beta_shape = (1 - mu_scaled) * phi\n        pm.Beta(\"y_obs\", alpha=alpha, beta=beta_shape, observed=y_data)\n\n        # Transform mu back to original scale\n        mu_original = pm.Deterministic(\"mu_original\", mu_scaled * (upper_bound - lower_bound) + lower_bound)\n\n        pm.Deterministic(\"capacity_pred\", mu_original)\n        pm.Deterministic(\"feature_effects\", beta)\n\n    return model, scaler\n\n\n\n\n\nTranslating raw battery data to diagnostics features\nIn the preceding sections, the output side of the Bayesian model was rigorously defined, including the Beta likelihood, the Logit link function, and physics-informed priors for the parameters (\\(\\beta_0, \\boldsymbol{\\beta}, \\lambda, \\phi\\)). However, the quality of the resulting predictions depends critically on the quality of the input features (\\(\\mathbf{x}\\)) that drive the degradation term (\\(\\eta_i = \\dots + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\)).\nRaw capacity measurement curves are noisy and variable. Therefore, before proceeding to Bayesian sampling, it is necessary to dedicate a structured process to translating real-world operational data into robust, physically meaningful diagnostic features.\nThis motivates the crucial step of feature engineering.\n\nData alignment and cleaning\nBefore extracting diagnostic features, a uniform time base must be established, as the formulas used for feature extraction require comparable voltage and current values across cycles.\n\nCycle Alignment (Standardization): Linear interpolation is used to resample all voltage and current time-series arrays to a uniform length (e.g., 500 points). This standardization enables direct cycle-to-cycle comparison, as illustrated by the transition from the raw data (Figures 1 and 2) to the interpolated curves (Figures 3 and 4).\n\n\n\n\n\nFigure 1: Charging Voltage curve at the beginning of life\n\n\n\n\n\nFigure 2: Discharge Voltage curve at the beginning of life\n\n\n\nAs shown in Figures 1 and 2, voltage curves differ in length due to variations in charge and discharge durations. Linear interpolation standardizes these curves to a fixed length (e.g., 500 points), enabling direct comparison across cycles, as illustrated in Figures 3 and 4.\n\n\n\n\nFigure 3: Inteporated Charging Voltage curve at the beginning of life\n\n\n\n\n\nFigure 4: Inteporated Discharge Voltage curve at the beginning of life\n\n\n\n\nData Filtering: Cycles exhibiting non-meaningful behavior (e.g., flat voltage profiles, excessive noise, or unrealistic starting or peak voltages) are removed to ensure that all inputs correspond to valid charging or discharging events.\n\n\n\nDiagnostic Feature Extraction\nWith aligned and cleaned curves, it is now possible to reliably extract cycle-specific diagnostic features that quantify the battery‚Äôs underlying physical degradation processes. These features are sensitive to aging mechanisms such as active material loss and internal resistance growth\n\nüß† Reflection: We chose to derive these physically meaningful features instead of feeding the entire, aligned time-series data (Figures 3 & 4). Why are these manually engineered features often preferred in industrial applications? Consider the trade-offs in model complexity, training speed, and the crucial interpretability of the final Bayesian coefficients (\\(\\beta\\)).\n\n\n\nShow the code\n\n\n# Fix the typo and create data\ndata = pd.DataFrame({\n    \"Diagnostic Feature\": [\n        \"Voltage Gap\",\n        \"Voltage Hysteresis\", \n        \"IC Peak Metrics\",\n        \"Hysteresis Proxy Resistance\"\n    ],\n    \"Formula\": [\n        \"ŒîVÃÑ = VÃÑ_c - VÃÑ_d\",\n        \"ŒîV(x) = V_c(x) - V_d(x)\",\n        \"IC = dQ/dV\",\n        \"R_proxy ‚àù ŒîV(x)/I_diff\"\n    ],\n    \"Physical Meaning\": [\n        \"Average polarization; quantifies internal losses\",\n        \"Loss mechanisms at specific state-of-charge\",\n        \"Phase transitions; indicates active material loss\",\n        \"Proxy for internal resistance growth\"\n    ]\n})\n\n\n# Create publication-quality table\ntable = (\n    GT(data)\n    .tab_header(\n        title=md(\"**Table 1: Battery Degradation Diagnostic Features**\"),\n        subtitle=\"Mathematical definitions and physical interpretations of key battery health indicators\"\n    )\n    .cols_label(\n        **{\n            \"Diagnostic Feature\": md(\"**Diagnostic Feature**\"),\n            \"Formula\": md(\"**Formula**\"),\n            \"Physical Meaning\": md(\"**Physical Meaning**\")\n        }\n    )\n    .tab_options(\n        table_width=\"100%\",\n        container_width=\"100%\",\n        table_font_size=\"14px\",\n        heading_title_font_size=\"18px\",\n        heading_subtitle_font_size=\"14px\",\n        column_labels_border_bottom_style=\"solid\",\n        column_labels_border_bottom_width=\"3px\",\n        column_labels_border_bottom_color=\"#3498db\",\n        table_body_border_bottom_style=\"solid\",\n        table_body_border_bottom_width=\"1px\",\n        table_body_border_bottom_color=\"#dee2e6\"\n    )\n    .tab_source_note(\n        source_note=\"Formulas assume constant temperature and current rates unless otherwise specified.\"\n    )\n)\n\n# Display\ntable.show()\n\n\n\n\n\n\n\n\nTable 1: Battery Degradation Diagnostic Features\n\n\nMathematical definitions and physical interpretations of key battery health indicators\n\n\nDiagnostic Feature\nFormula\nPhysical Meaning\n\n\n\n\nVoltage Gap\nŒîVÃÑ = VÃÑ_c - VÃÑ_d\nAverage polarization; quantifies internal losses\n\n\nVoltage Hysteresis\nŒîV(x) = V_c(x) - V_d(x)\nLoss mechanisms at specific state-of-charge\n\n\nIC Peak Metrics\nIC = dQ/dV\nPhase transitions; indicates active material loss\n\n\nHysteresis Proxy Resistance\nR_proxy ‚àù ŒîV(x)/I_diff\nProxy for internal resistance growth\n\n\n\nFormulas assume constant temperature and current rates unless otherwise specified.\n\n\n\n\n\n\n\n\n\n\n\nStatistical feature aggregation\nThe diagnostic signals derived above (e.g., incremental capacity curves) remain high-resolution time- or cycle-series data. To produce robust, concise, and comparable inputs for the Bayesian regression model, a final aggregation step is performed by extracting statistical moments from each diagnostic signal \\(s(x)\\). This aggregation reduces hundreds of data points per cycle into a small number of highly informative scalar features.\n\n\nShow the code\ndata = pd.DataFrame({\n    \"Statistical Feature\": [\n        \"Mean\",\n        \"Standard Deviation\", \n        \"Skewness\",\n        \"Kurtosis\",\n        \"RMS (Root-Mean-Square)\",\n        \"Entropy\",\n        \"Crest Factor\",\n        \"AUC (Area Under the Curve)\"\n    ],\n    \"Role in Degradation Modeling\": [\n        \"Captures the overall trend or shift of the diagnostic signal.\",\n        \"Measures variability and cycle-to-cycle noise.\",\n        \"Indicates asymmetry or bias in the signal distribution.\",\n        \"Quantifies the presence of extreme values or anomalies.\",\n        \"Represents the overall magnitude and stress level of the signal.\",\n        \"Measures irregularity or disorder, often increasing with non-uniform degradation.\",\n        \"Compares peak magnitude to average signal level, highlighting abnormal peaks.\",\n        \"Captures cumulative effects such as total energy loss or degradation trends.\"\n    ]\n})\n\n\n\n\nShow the code\ntable = (\n    GT(data)\n    .tab_header(\n        title=md(\"Statistical Features for Battery Degradation Modeling\"),\n        subtitle=\"Key signal processing metrics used to quantify degradation patterns\"\n    )\n    .cols_label(\n        **{\n            \"Statistical Feature\": md(\"**Statistical Feature**\"),\n            \"Role in Degradation Modeling\": md(\"**Role in Degradation Modeling**\")\n        }\n    )\n    .tab_options(\n        table_width=\"100%\",\n        container_width=\"100%\",\n        table_font_size=\"14px\",\n        heading_title_font_size=\"18px\",\n        heading_subtitle_font_size=\"14px\",\n        column_labels_border_bottom_style=\"solid\",\n        column_labels_border_bottom_width=\"3px\",\n        column_labels_border_bottom_color=\"#3498db\",\n        table_body_border_bottom_style=\"solid\",\n        table_body_border_bottom_width=\"1px\",\n        table_body_border_bottom_color=\"#dee2e6\"\n    )\n    .tab_source_note(\n        source_note=\"Bayesian Modelling|Anthony Faustine@ 2025\"\n    )\n)\n\ntable.show()\n\n\n\n\n\n\n\n\nStatistical Features for Battery Degradation Modeling\n\n\nKey signal processing metrics used to quantify degradation patterns\n\n\nStatistical Feature\nRole in Degradation Modeling\n\n\n\n\nMean\nCaptures the overall trend or shift of the diagnostic signal.\n\n\nStandard Deviation\nMeasures variability and cycle-to-cycle noise.\n\n\nSkewness\nIndicates asymmetry or bias in the signal distribution.\n\n\nKurtosis\nQuantifies the presence of extreme values or anomalies.\n\n\nRMS (Root-Mean-Square)\nRepresents the overall magnitude and stress level of the signal.\n\n\nEntropy\nMeasures irregularity or disorder, often increasing with non-uniform degradation.\n\n\nCrest Factor\nCompares peak magnitude to average signal level, highlighting abnormal peaks.\n\n\nAUC (Area Under the Curve)\nCaptures cumulative effects such as total energy loss or degradation trends.\n\n\n\nBayesian Modelling|Anthony Faustine@ 2025\n\n\n\n\n\n\n\n\n\nThese statistical summaries (e.g., \\(\\text{Mean}(\\text{IC})\\), \\(\\text{Std}(\\Delta V)\\)) form the input vector \\(\\mathbf{x}\\) in the linear predictor \\(\\eta_i = \\dots + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\). Such summaries of early-cycle behavior often preserve key degradation signatures while significantly reducing model complexity.\n\n\nFeature selection\nAfter extracting a broad set of diagnostic and statistical features, feature selection is required. Using all available features can lead to overfitting, increased model complexity, and multicollinearity, which compromises interpretability of the Bayesian coefficients (\\(\\boldsymbol{\\beta}\\)). The final four features selected for regression (\\(\\mathbf{x}\\)) are:\n\ncharge_current_auc\ncharge_current_mean\ndischarge_voltage_auc\ndischarge_voltage_crest\n\n\n\nLoad pre-processed CALCE dataset\nThe raw CALCE dataset is a widely used public resource in battery prognostics and can be downloaded via the CALCE dataset link.\nFor this notebook, however, we use pre-processed data that has been cleaned and formatted. This pre-processing reuses the techniques and codes originally published in this paper Ref. Using the cleaned data allows us to focus immediately on the Bayesian modeling aspects without the overhead of complex data preparation\n\n\nShow the code\nFIGSHARE_DOWNLOAD_URL = \"https://ndownloader.figshare.com/files/59415941\"\nfeatures = [\"charge_current_auc\", \"charge_current_mean\", \"discharge_voltage_crest\", \"discharge_voltage_auc\"]\ntarget = \"capacity\"\ndata = pd.read_parquet(FIGSHARE_DOWNLOAD_URL, engine=\"pyarrow\")\nupper_bound, lower_bound = data[target].max(), data[target].min()\ndf = data[data.CellType == \"CS2\"].copy()\ntest_df = df[df.BatteryID != \"CALCE_CS2_38\"]\ntrain_df = df[df.BatteryID == \"CALCE_CS2_38\"]\n\n\n\n\nShow the code\nplots = []\nfor feature_name in features:\n    plot = scatter_plot(train_df, y_col=feature_name)\n    plots.append(plot + labs(title=feature_name) + modern_theme(font_size=9))\n\nplot=gggrid(plots, ncol=2) + ggsize(650, 450)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  -0.1\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          charge_current_auc\n        \n      \n      \n        \n          charge_current_auc\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  -0.1\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          charge_current_mean\n        \n      \n      \n        \n          charge_current_mean\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  1.1\n                \n              \n            \n            \n              \n                \n                  1.15\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          discharge_voltage_crest\n        \n      \n      \n        \n          discharge_voltage_crest\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  0.54\n                \n              \n            \n            \n              \n                \n                  0.56\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          discharge_voltage_auc\n        \n      \n      \n        \n          discharge_voltage_auc\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n\n\n\nThe figure below plots the four selected features against cycle number for a representative battery. These plots empirically validate the selection process, as all four features exhibit clear, monotonic changes with cycling and, critically, show a distinct shift or acceleration in slope as the battery enters the failure state (\\(\\text{SoH} \\le 80\\%\\), shown in green). This strong visual correlation provides high confidence that these inputs will effectively drive the degradation component of our Bayesian model.\n\n\n\nBayesian model building\nBefore constructing the Bayesian Beta regression model, it is necessary to define a rigorous evaluation strategy. The central question addressed here is whether a model trained on data from a single battery can successfully generalize to other batteries whose degradation trajectories were not observed during training.\nThis setting reflects a common real-world scenario in which detailed historical data may be available for only a limited number of prototype units, while the deployed model must operate reliably across an entire manufacturing batch.\nTo ensure a fair and controlled evaluation, all batteries considered in this study are restricted to a single cell chemistry type (‚ÄúCS2‚Äù). By holding the underlying electrochemical properties constant, the analysis isolates unit-to-unit variability rather than confounding the results with chemistry-dependent effects.\nA one-shot generalization split is employed. A single representative battery (CALCE_CS2_38) is designated as the training set (train_df). The model learns the degradation rate (\\(\\lambda\\)) and operational sensitivities (\\(\\boldsymbol{\\beta}\\)) exclusively from this unit‚Äôs historical data. All remaining batteries of the same chemistry are assigned to the test set (test_df). Model performance is therefore evaluated based on its ability to predict capacity fade for previously unseen batteries using only the generalizable parameters inferred from the training unit.\nWith the input data rigorously cleaned, aligned, scaled, and reduced to the four most informative operational features (\\(\\mathbf{x}\\)), the Bayesian regression model can now be implemented and fitted. As defined in the Beta Likelihood and Priors subsection, the model is specified as a Bayesian Beta regression with a logit link function, enabling the modeling of bounded battery capacity (\\(\\tilde{C}\\)).\n\n\nShow the code\n# from bayes.regression.beta_degradation import beta_regression_model\nmodel, scaler = beta_regression_model(\n    train_df, features, target=target, upper_bound=upper_bound, lower_bound=lower_bound\n)\nmodel\n\n\n\\[\n            \\begin{array}{rcl}\n            \\text{intercept} &\\sim & \\operatorname{Normal}(1e-06,~0.5)\\\\\\text{lambda\\_rate} &\\sim & \\operatorname{LogNormal}(-5.3,~0.5)\\\\\\text{beta} &\\sim & \\operatorname{Normal}(0,~0.2)\\\\\\text{phi} &\\sim & \\operatorname{Gamma}(100,~f())\\\\\\text{degr\\_amp} &\\sim & \\operatorname{HalfNormal}(0,~0.1)\\\\\\text{mu\\_scaled} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{mu\\_original} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{capacity\\_pred} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{feature\\_effects} &\\sim & \\operatorname{Deterministic}(f(\\text{beta}))\\\\\\text{y\\_obs} &\\sim & \\operatorname{Beta}(f(\\text{phi},~\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}),~f(\\text{phi},~\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\n            \\end{array}\n            \\]\n\n\n\nPrior Predictive Check\nFollowing standard Bayesian practice, model validation begins with a Prior Predictive Check (PPC). The PPC involves simulating data from the model using only the prior distributions, without conditioning on any observed measurements. This procedure serves as a critical sanity check, verifying that the encoded engineering knowledge produces physically plausible behavior.\n\n\nShow the code\nwith model:\n    prior_pred = pm.sample_prior_predictive(samples=1000)\n\nfig, ax = plt.subplots(figsize=(4, 1.8))\naz.plot_ppc(prior_pred, group=\"prior\", ax=ax)\nplt.xlabel(\"Capacity\")\nplt.ylabel(\"Density\");\n\n\nSampling: [beta, degr_amp, intercept, lambda_rate, phi, y_obs]\n\n\n\n\n\n\n\n\n\nThe figure below, compare the predicted prior distribution (green line) against the observed data (blue line). This plot is essential for validating that our model‚Äôs structural assumptions align with physical reality\n\n\nShow the code\ny_prior = prior_pred.prior[\"capacity_pred\"].stack(sample=[\"chain\", \"draw\"]).values\ny_obs = train_df[target].values\npost_mean = y_prior.mean()\nn_draws = 500\nrng = np.random.default_rng(42)\ndraw_idx = rng.choice(y_prior.shape[0], size=n_draws, replace=False)\ny_prior_subset = y_prior[:, draw_idx].flatten()\ndf_prior = pd.DataFrame(\n    {\n        \"SoH\": np.concatenate([y_obs, y_prior_subset]),\n        \"type\": [\"observed\"] * len(y_obs) + [\"prior\"] * len(y_prior_subset),\n    }\n)\nplot=plot_density(\n    df_prior,\n    x_col=\"SoH\",\n    color_col=\"type\",\n    x_label=\"Capacity\",\n    fig_size=(500, 400),\n    title=\"Prior comparison\",\n    subtitle=\"Prior Predictive Check\",\n)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Prior comparison\n      \n    \n    \n      \n        Prior Predictive Check\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Capacity\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                observed\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                prior\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nFrom the two figures above, the PPC confirms the structural validity of the model:\n\nHigh-Confidence Initial Capacity: The predicted capacity distribution exhibits a dominant peak near \\(\\mu \\approx 1.0\\), reflecting the highly informative precision prior \\(\\phi \\sim \\text{Gamma}(100, 2.0)\\) (mean \\(\\phi = 50\\)). This enforces a strong prior belief in low sensor noise and high initial measurement confidence.\nRealistic Degradation Envelope: The prior predictive distribution remains tightly constrained across the capacity range. This behavior is driven by the informative degradation-rate prior on \\(\\lambda\\), which minimizes the probability of immediate or catastrophic capacity loss and enforces physically plausible degradation trajectories.\nAcknowledgment of Failure Modes: While constrained, the prior allocates non-negligible probability mass to lower capacity regions (e.g., \\(\\tilde{C} \\approx 0.4\\)‚Äì\\(0.7\\)). This reflects uncertainty in the degradation amplitude and rate parameters, allowing for degradation and failure scenarios without overstating their likelihood.\n\nThe PPC demonstrates that the model respects physical bounds, reflects realistic degradation behavior, and balances strong prior knowledge with controlled uncertainty. The model is therefore suitable for posterior inference.\n\n\n\nRunning inference\nWith the model fully specified, priors validated through the PPC, and input features prepared, posterior inference is performed using Markov Chain Monte Carlo (MCMC) sampling. This step approximates the posterior distribution by updating prior beliefs using the observed data, forming the core of Bayesian inference.\n\n\nShow the code\nwith model:\n    idata = pm.sample(2000, tune=2000, target_accept=0.95, random_seed=42)\nclear_output()\n\n\nAs discussed in Part 1, the MCMC process uses the No-U-Turn Sampler (NUTS) to explore the parameter space. The primary arguments guide this process:\n\ntune=2000: Specifies 2000 initial samples that are used solely to adapt the sampler‚Äôs step size and are then discarded. A high tuning value is crucial for complex, highly curved posteriors (like those involving Beta distributions) to ensure stable exploration.\ndraws=2000: Specifies 2000 final samples kept from the chain. These collected samples form the final Posterior Distribution for every model parameter (\\(\\lambda\\), \\(\\beta\\), \\(\\phi\\)).\ntarget_accept=0.95: Forces the sampler to take smaller, more cautious steps. This high acceptance rate is necessary to avoid divergences in challenging models, ensuring a high-quality, accurate representation of the posterior distribution, though it increases computation time.\n\nThe resulting idata object now contains thousands of samples for every single model parameter, representing our comprehensive, uncertainty-quantified solution. The next step is to ensure these samples are reliable\n\nModel diagnostics\nAfter sampling, convergence diagnostics are evaluated to ensure the reliability of posterior estimates. The validity of all subsequent inferences depends on whether the Markov chains have adequately explored the parameter space.\n\n\nShow the code\nvars = [\"beta\", \"intercept\", \"lambda_rate\", \"phi\", \"degr_amp\"]\ndata_summary = az.summary(idata, var_names=vars, kind=\"diagnostics\")[[\"ess_bulk\", \"ess_tail\", \"r_hat\"]]\nGT(data_summary.reset_index()).tab_header(title=\"\", subtitle=\"Diagnostics Summary\").cols_label(\n    {\n        \"ess_bulk\": \"ESS Bulk\",\n        \"ess_tail\": \"ESS Tail.\",\n        \"r_hat\": \"R-hat\",\n    }\n)\n\n\n\n\n\n\n\n\n\n\n\nDiagnostics Summary\n\n\nindex\nESS Bulk\nESS Tail.\nR-hat\n\n\n\n\nbeta[0]\n4381.0\n4866.0\n1.0\n\n\nbeta[1]\n4268.0\n4571.0\n1.0\n\n\nbeta[2]\n4218.0\n4234.0\n1.0\n\n\nbeta[3]\n4394.0\n4732.0\n1.0\n\n\nintercept\n4426.0\n4803.0\n1.0\n\n\nlambda_rate\n5309.0\n4684.0\n1.0\n\n\nphi\n7368.0\n5104.0\n1.0\n\n\ndegr_amp\n3758.0\n4646.0\n1.0\n\n\n\n\n\n\n\n\nTwo primary diagnostics are considered:\n\n\\(\\hat{R}\\) (Gelman‚ÄìRubin statistic): All parameters exhibit \\(\\hat{R} = 1.0\\), indicating excellent chain mixing and agreement across chains.\nEffective Sample Size (ESS): ESS values exceed 400 for all parameters (ranging from approximately 4,200 to 7,300), confirming that a sufficient number of independent samples were obtained for stable estimation of posterior means and credible intervals.\n\nThese diagnostics collectively indicate successful convergence and robust posterior sampling.\n\n\nAnalysing the posterior distribution\nWith convergence confirmed, the sampled chains provide a reliable approximation of the posterior distribution. The marginal posterior densities and corresponding trace plots for the core model parameters are examined to quantify degradation dynamics and assess the influence of operational features.\nThis analysis enables principled uncertainty quantification of degradation rates and feature effects, supporting interpretable and decision-relevant predictions for battery health forecasting.\n\n\nShow the code\naz.plot_trace(idata, var_names=vars, compact=True);\n\n\n\n\n\n\n\n\n\nThe analyze_parameter function below acts as a post-processing utility dedicated to generating publication-ready summary tables from the output of the MCMC sampling.\n\n\nShow the code\ndef analyze_parameter(\n    idata,\n    parameter: str,\n    features: list[str] | None = None,\n    hdi_prob: float = 0.95,\n    title: str = \"Parameter Summary\",\n    subtitle: str | None = None,\n) -&gt; GT:\n    \"\"\"Generates a formatted summary table for a single parameter using Great Tables.\n\n    This function extracts posterior summary statistics from ArviZ InferenceData and\n    returns a beautifully styled table suitable for reports, notebooks, or publications.\n\n    Args:\n        idata: ArviZ InferenceData object containing posterior samples.\n        parameter: Name of the parameter to summarize (e.g., \"beta\", \"alpha\", \"sigma\").\n        features: List of feature names to label rows. Required and used only when\n            ``parameter == \"beta\"``. Length must match the number of coefficients.\n        hdi_prob: Highest density interval probability (default: 0.95).\n        title: Main title for the table.\n        subtitle: Optional subtitle. If None and parameter is \"beta\", defaults to\n            \"Beta coefficient analysis\".\n\n    Returns:\n        A Great Tables (GT) object ready for display or further customization.\n\n    Raises:\n        ValueError: If ``features`` is provided for non-beta parameters or has wrong length.\n\n    Example:\n        &gt;&gt;&gt; gt = analyze_parameter(idata, \"beta\", features=X.columns.tolist())\n        &gt;&gt;&gt; gt  # displays nicely in Jupyter\n    \"\"\"\n    if features is not None and parameter != \"beta\":\n        raise ValueError(\"`features` should only be provided when parameter == 'beta'\")\n\n    # Get summary statistics from ArviZ\n    summary_df = az.summary(\n        idata,\n        var_names=[parameter],\n        hdi_prob=hdi_prob,\n        kind=\"stats\",\n        fmt=\"wide\",\n    ).reset_index(names=\"feature\")\n\n    # Assign meaningful feature names for beta coefficients\n    if parameter == \"beta\":\n        if features is None:\n            raise ValueError(\"`features` must be provided when analyzing 'beta' parameter\")\n        if len(features) != len(summary_df):\n            raise ValueError(\n                f\"Length of features ({len(features)}) must equal number of beta coefficients ({len(summary_df)})\"\n            )\n        summary_df[\"feature\"] = features\n\n    conditions = [\n        summary_df[\"hdi_2.5%\"] &gt; 0,  # Entire interval is positive\n        summary_df[\"hdi_97.5%\"] &lt; 0,  # Entire interval is negative\n    ]\n    choices = [\"Positive\", \"Negative\"]\n    summary_df[\"certainty\"] = np.select(conditions, choices, default=\"Uncertain\")\n\n    # Set default subtitle for beta coefficients\n    if subtitle is None and parameter == \"beta\":\n        subtitle = \"Beta coefficient analysis\"\n\n    gt_table = (\n        GT(summary_df)\n        .tab_header(\n            title=md(f\"**{title}**\"),\n            subtitle=md(subtitle) if subtitle else None,\n        )\n        .fmt_number(\n            columns=[\"mean\", \"sd\", \"hdi_2.5%\", \"hdi_97.5%\"],\n            decimals=3,\n        )\n        .data_color(\n            columns=[\"certainty\"],\n            palette=[\"#E1DFDD\", \"#F18F01\", \"#F18F01\"],\n            domain=[\"Uncertain\", \"Negative\", \"Positive\"],\n        )\n        .cols_label(\n            feature=md(\"**Feature**\"),\n            mean=md(\"**Mean**\"),\n            sd=md(\"**SD**\"),\n            **{\"hdi_2.5%\": md(\"**HDI 2.5%**\")},\n            **{\"hdi_97.5%\": md(\"**HDI 97.5%**\")},\n        )\n        .cols_align(align=\"center\", columns=[\"mean\", \"sd\", \"hdi_2.5%\", \"hdi_97.5%\", \"Certainty\"])\n        .tab_options(\n            table_font_size=\"14px\",\n            heading_title_font_size=\"20px\",\n            heading_subtitle_font_size=\"16px\",\n            row_group_font_weight=\"bold\",\n        )\n    )\n\n    return gt_table\n\n\n\n\nIdentifying Reliable Degradation Drivers (\\(\\boldsymbol{\\beta}\\) Coefficients)\nThe regression coefficients \\(\\boldsymbol{\\beta}\\) quantify the relationship between the engineered operational features (e.g., current and voltage metrics) and battery State of Health (\\(\\text{SoH}\\)) through the logit link function, \\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\). The credibility of each predictor is assessed by examining whether its \\(95%\\) Highest Density Interval (HDI) includes zero.\n\n\nShow the code\ntable=analyze_parameter(idata, \"beta\", features=features)\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nParameter Summary\n\n\nBeta coefficient analysis\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\ncharge_current_auc\n‚àí0.159\n0.145\n‚àí0.437\n0.132\nUncertain\n\n\ncharge_current_mean\n‚àí0.147\n0.145\n‚àí0.420\n0.145\nUncertain\n\n\ndischarge_voltage_crest\n‚àí0.446\n0.041\n‚àí0.528\n‚àí0.369\nNegative\n\n\ndischarge_voltage_auc\n0.020\n0.040\n‚àí0.063\n0.092\nUncertain\n\n\n\n\n\n\n\n\nThe posterior summary indicates that only one feature emerges as a statistically reliable degradation driver at the \\(95%\\) credibility level: the discharge_voltage_crest factor. Its \\(95%\\) HDI lies entirely below zero (from \\(-0.528\\) to \\(-0.369\\)), indicating strong evidence of a negative association with capacity retention. This result implies, with high certainty, that increases in this factor accelerate capacity fade. The posterior mean coefficient for the discharge_voltage_crest factor is \\(\\beta = -0.446\\). Interpreted on the odds scale as \\[\\text{Odds Ratio} = \\exp(-0.446) \\approx 0.64\\].\nThus, a one-unit increase in the crest factor is associated with an approximately \\(36%\\) reduction in the odds of maintaining high battery capacity (\\(1 - 0.64\\)).\nIn contrast, the \\(95%\\) HDIs for the remaining three features include zero (e.g., for charge_current_auc, HDI \\([-0.437,,0.132]\\)). As a result, the model cannot rule out the possibility that their true effects are negligible or even slightly positive. These features therefore do not constitute statistically reliable degradation drivers under the current model specification. Consequently, maintenance and monitoring efforts can be focused on the discharge_voltage_crest factor as the dominant operational indicator of degradation.\n\nüõ†Ô∏è Action: Refit the Beta regression model using only the discharge_voltage_crest factor as an operational covariate. Evaluate whether predictive performance on the test set remains comparable and whether the posterior mean and HDI for this coefficient remain stable. Such consistency would further support the conclusion that the remaining features primarily contributed noise rather than explanatory signal.\n\n\n\nQuantifying the degradation rate (\\(\\lambda_{\\text{rate}}\\))\nThe parameter \\(\\lambda_{\\text{rate}}\\) governs the speed of capacity fade induced by cycling. By adopting a weakly informative Lognormal prior with increased dispersion (\\(\\sigma = 1.0\\)), the data is allowed to dominate the posterior estimation of this parameter.\n\n\nShow the code\ntable=analyze_parameter(idata, \"lambda_rate\", title=\"Lambda Rate Summary\", subtitle=\"Degradation rate parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nLambda Rate Summary\n\n\nDegradation rate parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\nlambda_rate\n0.003\n0.001\n0.002\n0.004\nPositive\n\n\n\n\n\n\n\n\nThe posterior summary yields a highly precise estimate of the degradation rate, with a posterior mean of \\(\\mathbf{0.003}\\) per unit of cycle data. This value is lower than the prior expectation (centered around \\(0.005\\)), indicating that although degradation is inevitable, it progresses more gradually than initially assumed.\nThe remaining uncertainty is minimal, as evidenced by a small posterior standard deviation (\\(\\text{SD} = 0.001\\)) and a narrow \\(95%\\) HDI of \\([0.002,,0.004]\\). These results confirm that the MCMC sampler has effectively leveraged the data to tightly constrain the degradation speed.\n\n\nModel precision (\\(\\phi\\))\nThe precision parameter \\(\\phi\\) controls the dispersion of the Beta likelihood, quantifying how tightly the observed \\(\\text{SoH}\\) measurements cluster around the model-predicted mean after accounting for all modeled effects.\n\n\nShow the code\ntable=analyze_parameter(idata, \"phi\", title=\"Phi Summary\", subtitle=\"Phi  parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nPhi Summary\n\n\nPhi parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\nphi\n233.461\n10.140\n213.389\n252.803\nPositive\n\n\n\n\n\n\n\n\nThe posterior distribution of \\(\\phi\\) exhibits a high degree of concentration, with a posterior mean of \\(233.461\\) and a narrow \\(95%\\) HDI. This large mean precision implies very low residual variance in \\(\\text{SoH}\\), indicating that the combined model structure‚Äîincorporating the exponential degradation term, the cycling rate \\(\\lambda\\), and the operational features \\(\\boldsymbol{\\beta}\\) explains the majority of observed variability across the battery fleet. The narrow HDI further indicates that this high precision is estimated with substantial certainty.\n\nDegr amp parameter (\\(A\\))\nThe degradation amplitude parameter, \\(\\text{degr\\_amp}\\), quantifies the maximum potential capacity fade attributable solely to the cycling process and operates on the log-odds scale. The posterior mean of \\(0.367\\) represents the maximum reduction in \\(\\text{logit}(\\mu)\\) induced by cycling over the battery‚Äôs lifetime, thereby determining the vertical extent of the degradation curve.\n\n\nShow the code\n\ntable=analyze_parameter(idata, \"degr_amp\", title=\"Degr amp\", subtitle=\"A  parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nDegr amp\n\n\nA parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\ndegr_amp\n0.367\n0.053\n0.265\n0.468\nPositive\n\n\n\n\n\n\n\n\nThe \\(95%\\) HDI for \\(\\text{degr\\_amp}\\) is narrow and entirely positive, spanning \\([0.265,,0.468]\\). This provides strong statistical evidence that degradation due to cycling is both certain and quantitatively well-defined, rather than an artifact of noise. The strictly positive support of this parameter confirms that capacity loss is an unavoidable consequence of repeated cycling.\n\n\n\nPosterior predictive check\nFollowing parameter interpretation, a Posterior Predictive Check (PPC) is performed to assess model adequacy. This step evaluates whether the model, using posterior parameter samples, can generate synthetic data that closely resembles the observed measurements.\nUsing PyMC‚Äôs sample_posterior_predictive function, samples are drawn from the likelihood conditioned on the converged posterior chains. The resulting PPC compares three distributions: the observed data, the prior predictive distribution, and the posterior predictive distribution.\n\n\nShow the code\nwith model:\n    post_pred = pm.sample_posterior_predictive(idata, var_names=[\"y_obs\"], random_seed=42)\n\ny_post = post_pred.posterior_predictive[\"y_obs\"].stack(sample=[\"chain\", \"draw\"]).values\n# Mean of the posterior predictive\npost_mean = y_post.mean()\nn_draws = 500\nrng = np.random.default_rng(42)\ndraw_idx = rng.choice(y_post.shape[0], size=n_draws, replace=False)\ny_post_subset = y_post[:, draw_idx].flatten()\ny_post_subset = y_post_subset * (upper_bound - lower_bound) + lower_bound\n\n\ndf_posterior = pd.DataFrame(\n    {\n        \"SoH\": np.concatenate([y_obs, y_post_subset]),\n        \"type\": [\"observed\"] * len(y_obs) + [\"posterior\"] * len(y_post_subset),\n    }\n)\n\n\nSampling: [y_obs]\n\n\n\n\n\n\n\n\nThe figure below shows a Posterior Predictive Check (PPC), which is the gold standard for evaluating model fit in Bayesian statistics. It compares three key distributions for the capacity: the data we observed, our initial beliefs (Prior), and the model‚Äôs final predictions (Posterior).\n\n\nShow the code\ndf = pd.concat([df_prior, df_posterior])\nplot=plot_density(\n    df,\n    x_col=\"SoH\",\n    color_col=\"type\",\n    x_label=\"Capacity\",\n    fig_size=(700, 350),\n    title=\"Prior and Posterior Comparison\",\n    subtitle=\"Beta-regression\",\n)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.3\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.7\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                0.9\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.2\n              \n            \n          \n          \n            \n            \n            \n              \n                1.3\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Prior and Posterior Comparison\n      \n    \n    \n      \n        Beta-regression\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Capacity\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                observed\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                prior\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                posterior\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThe posterior predictive distribution aligns closely with the observed capacity distribution, indicating that the model successfully captures the underlying data-generating process. In contrast, the prior predictive distribution is smoother and exhibits broader structure, reflecting weaker and less targeted assumptions before observing data.\nThe shift and sharpening from prior to posterior predictive distributions demonstrate that the observed data provided substantial information and that the model effectively updated its initial beliefs.\nThe posterior predictive distribution is strongly skewed toward high capacity values near \\(1.0\\), consistent with the predominance of early- and mid-life measurements. A smaller secondary mode around \\(0.3\\)‚Äì\\(0.4\\) corresponds to a limited number of end-of-life observations. This agreement between synthetic and observed data provides strong evidence of model adequacy and predictive reliability.\n\n\n\nPredict capacity for a new battery\nThe final objective of this modeling effort is to transition from parameter estimation to practical prognosis by generating a full Posterior Predictive Distribution (PPD) for the capacity of a new or future battery state. This process converts uncertainty-aware parameter estimates into actionable prognostic predictions.\nThe PPD explicitly incorporates two fundamental sources of uncertainty:\n\nEpistemic uncertainty, arising from uncertainty in the estimated model parameters (e.g., the width of the HDI for \\(\\lambda_{\\text{rate}}\\)).\nAleatoric uncertainty, representing irreducible noise in the measurement process, as captured by the precision parameter \\(\\phi\\).\n\nAs a result, the model produces not a single point estimate but a credible interval (HDI) that probabilistically bounds the true capacity value at each cycle.\nTo assess generalization performance and demonstrate practical utility for risk management, four cells from the CALCE dataset that were excluded during training are selected for evaluation. For each battery, the same four operational features used in model training are extracted and supplied to the fitted model.\n\n\nShow the code\nfrom bayes.regression.beta_degradation import get_posterior_predictions\n\n\n\n\nShow the code\nnew_df = test_df[test_df[\"BatteryID\"].isin([\"CALCE_CS2_34\", \"CALCE_CS2_36\", \"CALCE_CS2_37\", \"CALCE_CS2_33\"])].copy()\n\n\nPosterior predictions for unseen batteries are generated using the `get_posterior_predictions procedure, which applies the fitted Bayesian model to new input data. The process consists of the following steps:\n\nFeature Transformation: The operational features of the new battery are transformed using the same scaling object fitted during training, ensuring consistency between training and inference domains. The corresponding cycle counts are extracted separately, as they directly enter the exponential degradation component of the model.\n\n\n    x_new = scaler.transform(data[features])\n\nDummy Target Initialization: A placeholder target array is supplied to satisfy the dimensional requirements of the PyMC model‚Äôs observed variable. These values are ignored during posterior prediction..\n\n    y_dummy_scaled = np.full(shape=(X_new_scaled.shape[0],), fill_value=0.5)\n\nModel update: The new feature matrix, cycle data, and dummy target are injected into the model using pm.set_data, reconfiguring the model for prediction without retraining.\n\n    with battery_model:\n        pm.set_data({\n            \"x_data\": x_new,\n            \"cycle_data\": cycle_new,\n            \"y_obs\": y_dummy_scaled\n        })\n\nPosterior Predictive Sampling: Samples are drawn from the Posterior Predictive Distribution using pm.sample_posterior_predictive, incorporating both posterior parameter uncertainty and observation noise..\n\n    with battery_model:\n        post_pred = pm.sample_posterior_predictive(\n            idata,\n            var_names=[\"y_obs\"],\n            random_seed=42,\n            predictions=True,\n        )\n\n\nShow the code\npred_df = get_posterior_predictions(\n    idata, model, scaler, new_df, features, alpha=0.1, upper_bound=upper_bound, lower_bound=lower_bound\n)\nclear_output()\n\n\nPredictions for each test battery are visualized using the plot_hdi_regression function. The plots display the observed capacity measurements (points) overlaid with the model‚Äôs posterior predictive mean and the corresponding \\(90%\\) HDI. The blue line represents the posterior predictive mean where the shaded region represents the \\(90%\\) HDI, quantifying predictive uncertainty.\n\n\nShow the code\nfrom bayes.plot.regres_plot import plot_hdi_regression\n\n\n\n\nShow the code\nplot=plot_hdi_regression(\n    pred_df,\n    x_column=\"cycle\",\n    y_column=\"capacity\",\n    group_column=\"BatteryID\",\n    pred_column=\"pred_median\",\n    x_label=\"Cycle Number\",\n    y_label=\"Capacity (Ah)\",\n    title_prefix=\"Battery Capacity vs. Cycle with Posterior Predictions\",\n    subtitle=\"90% HDI accounts for  uncertainty.\",\n    alpha=0.1,\n) + ggsize(800, 500)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_33\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_34\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                500\n              \n            \n          \n          \n            \n            \n            \n              \n                1,000\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_36\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                500\n              \n            \n          \n          \n            \n            \n            \n              \n                1,000\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_37\n        \n      \n    \n    \n      \n        Battery Capacity vs. Cycle with Posterior Predictions\n      \n    \n    \n      \n        90% HDI accounts for  uncertainty.\n      \n    \n    \n      \n        Capacity (Ah)\n      \n    \n    \n      \n        Cycle Number\n      \n    \n    \n      \n        \n          \n            Legend\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                90% HDI\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n                \n                  \n                    \n                  \n                \n              \n            \n            \n              \n                Observed Data\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n                \n                  \n                    \n                  \n                \n              \n            \n            \n              \n                Posterior Mean\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayesian Modelling| Anthony Faustine @ 2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nKey Observations\n\nThe posterior predictive mean closely tracks the observed capacity trajectory across the full battery lifetime, including the non-linear degradation phase near end-of-life.\nApproximately \\(90\\%\\) (or more) of observed data points fall within the predictive HDI, indicating well-calibrated uncertainty estimates.\nFor batteries CS2_33, CS2_36, and CS2_37, the HDI remains narrow even during late-life degradation, reflecting high model confidence and low residual noise.\nFor battery CS2_34, the HDI widens toward the final cycles, appropriately reflecting increased predictive uncertainty in the late-life regime.\n\n\nEvaluating Predictive Performance\nPredictive performance on the held-out batteries is quantified using both accuracy and uncertainty-based metrics.\n\n\nShow the code\nfrom bayes.metrics.interval import get_interval_metrics\nfrom bayes.metrics.regression import regression_report\n\nmetrics_list = []\nfor battery_id, df in pred_df.groupby(\"BatteryID\"):\n    reg_report = regression_report(df[\"capacity\"], df[\"pred_median\"])\n    interval_report = get_interval_metrics(\n        df[\"pred_median\"].values,\n        df[\"capacity\"].values,\n        df[\"hdi_low\"].values,\n        df[\"hdi_high\"].values,\n        alpha=0.1,\n    )\n    full_report = pd.concat([reg_report, interval_report], ignore_index=True)\n    full_report[\"BatteryID\"] = battery_id\n    metrics_list.append(full_report)\nmetrics_df = pd.concat(metrics_list, ignore_index=True)\n\nmetrics_df = metrics_df.pivot_table(\n    index=[\"BatteryID\"],\n    columns=\"Metric\",\n    values=\"Value\",\n).reset_index()\n\n\n\ndef make_metrics_table(metrics_df, title=\"Model Evaluation Results\"):\n    \"\"\"Generate a formatted GT table from cross-validation metrics.\"\"\"\n    # Sort to present best models first\n    df = metrics_df.sort_values([\"BatteryID\", \"MAE\"]).reset_index(drop=True)\n\n    # Build base table\n    gt = (\n        GT(df[[\"BatteryID\", \"MAE\", \"RMSE\", \"R2\", \"NMPI\", \"PICP\"]])\n        .tab_header(title=title, subtitle=\"Per-battery performance\")\n        .cols_label(BatteryID=\"Test Cell\", MAE=\"MAE\", RMSE=\"RMSE\", R2=md(\"R&lt;sup&gt;2&lt;/sup&gt;\"))\n        .fmt_number(columns=[\"MAE\", \"RMSE\", \"NMPI\", \"PICP\"], decimals=3)\n        .fmt_number(columns=\"R2\", decimals=3)\n        .tab_spanner(label=\"Error Metrics\", columns=[\"MAE\", \"RMSE\", \"NMPI\", \"PICP\"])\n        .tab_style(\n            style=style.text(weight=\"bold\"),\n            locations=loc.body(columns=\"Model\"),\n        )\n        .tab_options(\n            table_font_size=\"small\",\n            # row_strip_color=\"#fafafa\"\n        )\n    )\n    for col in [\"MAE\", \"RMSE\", \"R2\", \"NMPI\", \"PICP\"]:\n        best_idx = df[col].idxmax() if col in [\"R2\", \"PICP\"] else df[col].idxmin()\n        gt = gt.tab_style(style=style.fill(color=\"#E1DFDD\"), locations=loc.body(rows=best_idx, columns=col))\n\n    return gt\n\n\n\n\nShow the code\nmake_metrics_table(metrics_df, title=\"Model Evaluation Results\")\n\n\n\n\n\n\n\n\nModel Evaluation Results\n\n\nPer-battery performance\n\n\nTest Cell\nError Metrics\nR2\n\n\nMAE\nRMSE\nNMPI\nPICP\n\n\n\n\nCALCE_CS2_33\n0.010\n0.010\n0.100\n1.000\n0.990\n\n\nCALCE_CS2_34\n0.040\n0.050\n0.100\n0.750\n0.910\n\n\nCALCE_CS2_36\n0.020\n0.020\n0.100\n0.990\n0.990\n\n\nCALCE_CS2_37\n0.010\n0.010\n0.100\n1.000\n1.000\n\n\n\n\n\n\n\n\nAccuracy Metrics\nPoint-prediction accuracy is evaluated using the coefficient of determination (\\(R^2\\)), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). Together, these metrics assess how well the model‚Äôs posterior predictive mean captures the observed capacity degradation trajectory.\n\n\\(R^2\\) quantifies the proportion of variance in the observed capacity explained by the model. Values close to 1 indicate that the predicted degradation curve accurately follows the overall trend and slope of capacity fade.\nMAE represents the average absolute deviation between predicted and observed capacity values, expressed in State-of-Health (SoH) units. MAE provides a physically interpretable measure of typical prediction error.\nRMSE penalizes larger deviations more strongly than MAE and is therefore particularly sensitive to localized mismatches, especially during the highly nonlinear end-of-life degradation phase.\n\nFrom the results summarized above, the model achieves \\(R^2\\) values between \\(0.910\\) and \\(1.0\\), with MAE in the range of \\(0.010\\)‚Äì\\(0.040\\) SoH units, indicating strong point-prediction accuracy across all evaluated cells. RMSE values range from \\(0.01\\) to \\(0.05\\) SoH units, confirming that large deviations are generally rare.\nHowever, Cell 34 exhibits the highest RMSE (\\(0.05\\)), along with comparatively lower \\(R^2\\) and higher MAE than the remaining cells. This combination indicates that, while the model captures the overall degradation trend for Cell 34, it experiences larger localized errors‚Äîparticularly near late-life degradation relative to other cells. These deviations suggest the presence of sharper nonlinear behavior or cell-specific degradation mechanisms not fully represented by the global model parameters.\nUncertainty Quality Metrics\nBeyond point accuracy, a central goal of Bayesian modeling is to provide reliable and interpretable uncertainty estimates. This is assessed using Prediction Interval Coverage Probability (PICP) and Normalized Mean Prediction Interval (NMPI).\n\nPICP measures the fraction of observed capacity values that fall within the model‚Äôs \\(90%\\) Highest Density Interval (HDI). A well-calibrated model should achieve PICP close to the nominal level (0.95), indicating that the predicted uncertainty accurately reflects real variability.\nNMPI quantifies the average width of the predictive interval, normalized by the observed capacity range. NMPI reflects the sharpness of predictions: lower values indicate tighter uncertainty bounds, which are essential for actionable maintenance and risk-based decision-making.\n\nResults show consistently low NMPI values (approximately \\(\\mathbf{0.1}\\)) across all test cells. This confirms that the predictive intervals are narrow relative to the capacity range, indicating high confidence in the model‚Äôs predictions. At the same time, PICP exceeds \\(0.90\\) for most cells, demonstrating that this confidence is not over-stated and that the uncertainty bounds are well calibrated.\nCell 34 again deviates from this pattern, exhibiting reduced PICP (\\(75\\%\\)). This indicates that a larger fraction of its observed capacity measurements fall outside the predicted \\(90%\\) HDI, suggesting either increased intrinsic variability or degradation dynamics that differ from those captured by the global model.\n\nüõ†Ô∏è Action: To diagnose the degraded predictive performance observed for Cell 34, compare all test cells by plotting \\(R^2\\) versus PICP to identify accuracy‚Äìreliability trade-offs. Examine whether Cell 34‚Äôs operational features fall outside the training-feature distribution, indicating extrapolation beyond the model‚Äôs learned domain. Finally, compare the capacity degradation distribution of Cell 34 with the training cell to assess whether it follows a distinct degradation regime.\n\n\n\n\nConclusion\nThis post demonstrates that a single-level Bayesian Beta regression model, informed by physics-aware priors and carefully engineered features, can deliver highly accurate capacity predictions together with trustworthy uncertainty bounds. The model achieves near-perfect predictive accuracy while maintaining well-calibrated \\(95%\\) credible intervals, validating the use of Beta likelihoods and informed priors for battery degradation modeling.\nHowever, the current formulation assumes that the degradation rate (\\(\\lambda_{\\text{rate}}\\)) and operational sensitivities (\\(\\boldsymbol{\\beta}\\)) are shared across the entire battery fleet. In practice, manufacturing variability and latent defects introduce unit-specific degradation behavior. As a result, even a highly accurate global model may underpredict risk for batteries from unfavorable batches or overestimate degradation for higher-quality units.\nComing Next: In Part 3, this limitation will be addressed by introducing Hierarchical Bayesian Models. These models learn a robust global degradation trend while allowing each individual battery to exhibit informed local deviations in parameters such as \\(\\lambda_{\\text{rate}}\\) and \\(\\boldsymbol{\\beta}\\).\n\nThe data and full code in pymc5 is available on my GITHUB page.\n\n\n\nReferences\n\nZhang, H., Li, Y., Zheng, S. et al.¬†Battery lifetime prediction across diverse ageing conditions with inter-cell deep learning. Nat Mach Intell 7, 270‚Äì277 (2025). https://doi.org/10.1038/s42256-024-00972-x\nFerrari, S. L. P., & Cribari-Neto, F. (2004). Beta regression for modelling rates and proportions. Journal of Applied Statistics, 31(7), 799‚Äì815.\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.\nMcElreath, R. (2020). Statistical Rethinking (2nd ed.). CRC Press.\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nFaustine, Anthony. 2025. ‚ÄúBayesian Regression: A Real-World\nBattery Degradation Case Study.‚Äù December 17, 2025. https://sambaiga.github.io/blog/2025/12/2025-12-1-bayesian-regression..html."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page includes a list of talks, lectures, workshops, and presentations I‚Äôve given, along with links to their slides."
  },
  {
    "objectID": "talks/index.html#section",
    "href": "talks/index.html#section",
    "title": "Talks",
    "section": "2024",
    "text": "2024\n\n\n\n    \n    \n                  \n            September 17, 2024\n        \n        \n            Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with  PV \n            IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids | Oslo, Norway\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            July 2, 2024\n        \n        \n            Scalable and Efficient MLP-based  Fully Parameterised Quantile for Probabilistic Power Forecasting\n            44th International Symposium on Forecasting | Dijon, France\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n            \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Anthony Faustine",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n    \n        \n            \n                Anthony Faustine, and Pereira, Lucas, ‚ÄúFPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates,‚Äù IEEE Transactions on Smart Grid 2024, doi: 10.1109/TSG.2022.3148699\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Quantile regression\n            \n                 / Low voltage substation\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Analysis notebook\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine,Nunes, Nuno Jardim , and Pereira, Lucas, ‚ÄúEfficiency Through Simplicity: MLP-Based Approach for Net-Load Forecasting With Uncertainty Estimates in Low-Voltage Distribution Networks,‚Äù IEEE Transactions on Power Systems 2024, doi: 10.1109/TPWRS.2024.3400123\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Quantile regression\n            \n                 / Low voltage substation\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Hafsa Bousbiat,Anthony Faustine, Christoph Klemenjak, and Lucas Pereira ‚ÄúUnlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines,‚Äù IEEE Transactions on Industrial Informatics  vol. 19, no. 5, pp. 7002-7010 May 2023, doi: 10.1109/TII.2022.3206322.\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúImproved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks,‚Äù Energies 2020, 13, 3374, doi: 10.3390/en13133374\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n                 / nilm state prediction\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Benjamin V√∂lker, Andreas Reinhardt, Anthony Faustine and Pereira, Lucas, ‚ÄúWatt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective,‚Äù Energies 2021, 14(3), 719, doi: 10.1109/10.3390/en14030719\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n                    \n                            disagregation\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n                 / nilm state prediction\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine,  Lucas Pereira and Christoph Klemenjak ‚ÄúAdaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring,‚Äù  IEEE Transactions on Smart Grid  vol. 12, no. 1, pp. 398-406, Jan. 2021, doi: 10.1109/TSG.2020.3010621.\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n                 / nilm state prediction\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine, and Pereira, Lucas, ‚ÄúMulti-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network,‚Äù Energies  2020, 13, 4154., doi: 10.1109/TSG.2022.3148699\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n                 / nilm state prediction\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#conference-papers",
    "href": "research/index.html#conference-papers",
    "title": "Anthony Faustine",
    "section": "Conference papers",
    "text": "Conference papers\n\n\n\n    \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúEnhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power,‚Äù  CIRED Chicago Workshop 2024: Resilience of Electric Distribution Systems, Chicago, USA,  2025, pp. 27-31, doi: 10.1049/icp.2024.2555\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúScalable and Efficient MLP-based  Fully Parameterised Quantile for Probabilistic Power Forecasting,‚Äù 44th International Symposium on Forecasting, Dijon, France  2024\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúApplying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring,‚Äù ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece  2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10096324\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            buildings\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúConformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation,‚Äù 2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm), Oslo, Norway  2024, pp. 59-64, doi: 10.1109/SmartGridComm60555.2024.10738106\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html",
    "href": "research/conferences/conformalmlpf-2024/index.html",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html#important-links",
    "href": "research/conferences/conformalmlpf-2024/index.html#important-links",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html#abstract",
    "href": "research/conferences/conformalmlpf-2024/index.html#abstract",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "Abstract",
    "text": "Abstract\nProbabilistic net-load forecasting in Low-Voltage (LV) distribution networks is essential in light of the increased variability introduced by the widespread integration of renewable energy sources (RES). Various probabilistic approaches based on neural networks have been proposed to solve this challenge. This study introduces lightweight neural network-based conformal prediction (Conformal-MLPF) for net-load forecasting within an LV power distribution network. It uses Split Conformal prediction to transform a lightweight MLP-based point forecast into a probabilistic forecast. Our validation on two real-life LV substations datasets suggests that the proposed Conformal-MLPF achieves a better tradeoff between forecasting performance and model complexity without requiring restrictive assumptions about data distribution."
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html#important-figure",
    "href": "research/conferences/conformalmlpf-2024/index.html#important-figure",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "Important figure",
    "text": "Important figure\nFigure 5: Derogation decisions across pandemic violations and pandemic backsliding\n\n\n\nFigure 5: Derogation decisions across pandemic violations and pandemic backsliding"
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html#bibtex-citation",
    "href": "research/conferences/conformalmlpf-2024/index.html#bibtex-citation",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{ChaudhryHeiss:2021,\n    Author = {Suparna Chaudhry and Anthony Faustine},\n    Note = {Working paper},\n    Title = {Derogations and Democratic Backsliding: Exploring the Pandemic's Effects on Civic Spaces},\n    Year = {2021}}"
  },
  {
    "objectID": "research/conferences/ICASSP 2023/index.html",
    "href": "research/conferences/ICASSP 2023/index.html",
    "title": "Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/ICASSP 2023/index.html#important-links",
    "href": "research/conferences/ICASSP 2023/index.html#important-links",
    "title": "Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/ICASSP 2023/index.html#abstract",
    "href": "research/conferences/ICASSP 2023/index.html#abstract",
    "title": "Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring",
    "section": "Abstract",
    "text": "Abstract\nIndustrial loads offer challenges for Non-intrusive Load Monitoring (NILM), such as phase imbalance associated with 3-phase lines. However, very little NILM research has been developed so far with this respect. This work presents a load recognition technique for NILM applying low complexity Fortesque Transform (FT). The FT decomposes the unbalanced 3-phase current waveform extracted from 3-phase aggregate power measurements to balance the given load. The 3-phases current waveform is transformed into an image-like representation using a compressedeuclidean distance matrix to improve the recognition ability further. The image representation is used as input to Convolutional Neural Network (CNN) classifier to learn the patterns of labeled data. Experimental evaluation of the industrial aggregated dataset shows that FT improves recognition performance by 5.8%, compared to the case without FT."
  },
  {
    "objectID": "research/conferences/ICASSP 2023/index.html#bibtex-citation",
    "href": "research/conferences/ICASSP 2023/index.html#bibtex-citation",
    "title": "Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@INPROCEEDINGS{10096324,\n  author={Faustine, Anthony and Pereira, Lucas},\n  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \n  title={Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring}, \n  year={2023},\n  volume={},\n  number={},\n  pages={1-5},\n  keywords={Load monitoring;Symmetric matrices;Power measurement;Power demand;Transforms;Signal processing;Convolutional neural networks;NILM;Industrial Appliances;Three-Phase;Fortesque Transform;Symmetrical Components},\n  doi={10.1109/ICASSP49357.2023.10096324}}"
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html",
    "href": "research/articles/recurrence-nilm/index.html",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html#important-links",
    "href": "research/articles/recurrence-nilm/index.html#important-links",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html#abstract",
    "href": "research/articles/recurrence-nilm/index.html#abstract",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "Abstract",
    "text": "Abstract\nPower demand forecasting is becoming a crucial tool for the planning and operation of Low Voltage (LV) distribution systems. Most importantly, the high penetration of Photovoltaics (PV) power generation as part of Distributed Energy Resource (DER)s has transformed the power demand forecasting problem at the distribution level into net-load forecasting. This paper introduces a novel and scalable approach to probabilistic forecasting at LV substation with PV generation. It presents a multi-variates probabilistic forecasting approach, leveraging Quantile Regression (QR). The proposed architecture uses a computationally efficient feed-forward neural net to capture the complex interaction between the historical load demands and covariate variables such as solar irradiance. It is empirically demonstrated that the proposed method can efficiently produce well-calibrated forecasts, both auto-regressively or in a single forward pass. Furthermore, a benchmark against four state-of-the-art forecasting approaches shows that the proposed approach offers a desirable trade-off between forecasting accuracies, calibrated uncertainty, and computation complexity."
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html#important-figures",
    "href": "research/articles/recurrence-nilm/index.html#important-figures",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2: Predicted probability of impartiality by levels of equality and democracy\n\n\n\n\n\nFigure 4: Tangents to the prediction function at 25 and 50"
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html#citation",
    "href": "research/articles/recurrence-nilm/index.html#citation",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{10529636,\n  author={Faustine, Anthony and Nunes, Nuno Jardim and Pereira, Lucas},\n  journal={IEEE Transactions on Power Systems}, \n  title={Efficiency Through Simplicity: MLP-Based Approach for Net-Load Forecasting With Uncertainty Estimates in Low-Voltage Distribution Networks}, \n  year={2025},\n  volume={40},\n  number={1},\n  pages={46-56},\n  keywords={Forecasting;Uncertainty;Probabilistic logic;Predictive models;Substations;Load modeling;Distribution networks;Deep Neural Networks (DNN) Feed-forward Neural Network (FFN) Low Voltage (LV) distribution substation;Multilayer Perceptron (MLP);net-load;probabilistic forecasting;Photovoltaics (PV) generation;quantile regression (QR)},\n  doi={10.1109/TPWRS.2024.3400123}}"
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html",
    "href": "research/articles/multilabel-nilm/index.html",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html#important-links",
    "href": "research/articles/multilabel-nilm/index.html#important-links",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html#abstract",
    "href": "research/articles/multilabel-nilm/index.html#abstract",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "Abstract",
    "text": "Abstract\nThe increased penetration of Renewable Energy Sources (RES) as part of a decentralized and distributed power system makes net-load forecasting a critical component in the planning and operation of power systems. However, compared to the transmission level, producing accurate short-term net-load forecasts at the distribution level is complex due to the small number of consumers. Moreover, owing to the stochastic nature of RES, it is necessary to quantify the uncertainty of the forecasted net-load at any given time, which is critical for the real-world decision process. This work presents parameterized deep quantile regression for short-term probabilistic net-load forecasting at the distribution level. To be precise, we use a Deep Neural Network (DNN) to learn both the quantile fractions and quantile values of the quantile function. Furthermore, we propose a scoring metric that reflects the trade-off between predictive uncertainty performance and forecast accuracy. We evaluate the proposed techniques on historical real-world data from a low-voltage distribution substation and further assess its robustness when applied in real-time. The experiment‚Äôs outcomes show that the resulting forecasts from our approach are well-calibrated and provide a desirable trade-off between forecasting accuracies and predictive uncertainty performance that are very robust even when applied in real-time."
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html#important-figures",
    "href": "research/articles/multilabel-nilm/index.html#important-figures",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR\n\n\n\n\n\nFigure 3a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR"
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html#citation",
    "href": "research/articles/multilabel-nilm/index.html#citation",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{9701598,\n  author={Faustine, Anthony and Pereira, Lucas},\n  journal={IEEE Transactions on Smart Grid}, \n  title={FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates}, \n  year={2022},\n  volume={13},\n  number={3},\n  pages={2440-2451},\n  keywords={Forecasting;Uncertainty;Predictive models;Probabilistic logic;Renewable energy sources;Load modeling;Additives;Net-load;forecasting;uncertainity;deep neural network;quantile regression},\n  doi={10.1109/TSG.2022.3148699}}"
  },
  {
    "objectID": "research/articles/fqr-2024/index.html",
    "href": "research/articles/fqr-2024/index.html",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/fqr-2024/index.html#important-links",
    "href": "research/articles/fqr-2024/index.html#important-links",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/fqr-2024/index.html#abstract",
    "href": "research/articles/fqr-2024/index.html#abstract",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "Abstract",
    "text": "Abstract\nThe increased penetration of Renewable Energy Sources (RES) as part of a decentralized and distributed power system makes net-load forecasting a critical component in the planning and operation of power systems. However, compared to the transmission level, producing accurate short-term net-load forecasts at the distribution level is complex due to the small number of consumers. Moreover, owing to the stochastic nature of RES, it is necessary to quantify the uncertainty of the forecasted net-load at any given time, which is critical for the real-world decision process. This work presents parameterized deep quantile regression for short-term probabilistic net-load forecasting at the distribution level. To be precise, we use a Deep Neural Network (DNN) to learn both the quantile fractions and quantile values of the quantile function. Furthermore, we propose a scoring metric that reflects the trade-off between predictive uncertainty performance and forecast accuracy. We evaluate the proposed techniques on historical real-world data from a low-voltage distribution substation and further assess its robustness when applied in real-time. The experiment‚Äôs outcomes show that the resulting forecasts from our approach are well-calibrated and provide a desirable trade-off between forecasting accuracies and predictive uncertainty performance that are very robust even when applied in real-time."
  },
  {
    "objectID": "research/articles/fqr-2024/index.html#important-figures",
    "href": "research/articles/fqr-2024/index.html#important-figures",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR\n\n\n\n\n\nFigure 3a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR"
  },
  {
    "objectID": "research/articles/fqr-2024/index.html#citation",
    "href": "research/articles/fqr-2024/index.html#citation",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{9701598,\n  author={Faustine, Anthony and Pereira, Lucas},\n  journal={IEEE Transactions on Smart Grid}, \n  title={FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates}, \n  year={2022},\n  volume={13},\n  number={3},\n  pages={2440-2451},\n  keywords={Forecasting;Uncertainty;Predictive models;Probabilistic logic;Renewable energy sources;Load modeling;Additives;Net-load;forecasting;uncertainity;deep neural network;quantile regression},\n  doi={10.1109/TSG.2022.3148699}}"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Data Visualization with R \n                \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Fall 2024 (asynchronous online)\n                \n                \n                \n                 Spring 2025 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Comparative Public Administration \n                \n            \n            \n                PMAP 8441/4441 | \n                Georgia State University\n                \n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n\n            \n                \n                \n                 Fall 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2025\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "project/index.html#section",
    "href": "project/index.html#section",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Data Visualization with R \n                \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Fall 2024 (asynchronous online)\n                \n                \n                \n                 Spring 2025 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Comparative Public Administration \n                \n            \n            \n                PMAP 8441/4441 | \n                Georgia State University\n                \n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n\n            \n                \n                \n                 Fall 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2025\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "I‚Äôm Anthony Faustine, a Lead Data Scientist at Eaton‚Äôs Centre for Intelligent Power (CIP) in Dublin, Ireland. At Eaton, my focus is on applying machine learning and data-driven innovation initiatives that contribute to building intelligent power systems for a sustainable energy future.\nPlease feel free to explore my website to learn more about my work, and research interests. I‚Äôm always interested in connecting with others who share my passion for data science, AI, and innovation.\nRead more ‚Üí"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Bayesian Regression: A Real-World Battery Degradation Case Study\n      \n      Learn how Bayesian regression works in practice using PyMC, through a real-world battery degradation case study.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          bayesian\n          \n        \n        \n      \n\n      \n        \n        December 17, 2025\n        \n      \n    \n  \n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        Understanding Bayesian Thinking for Industrial Applications\n      \n      Learn how Bayesian thinking can enhance decision-making in industrial applications. This article lay the foundation of bayesion modelling with Pymc and their practical use cases.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          bayesian\n          \n        \n        \n      \n\n      \n        \n        October 10, 2025\n        \n      \n    \n  \n  \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Bayesian Regression: A Real-World Battery Degradation Case Study\n      \n      Learn how Bayesian regression works in practice using PyMC, through a real-world battery degradation case study.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          bayesian\n          \n        \n        \n      \n\n      \n        \n        December 17, 2025\n        \n      \n    \n  \n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        Understanding Bayesian Thinking for Industrial Applications\n      \n      Learn how Bayesian thinking can enhance decision-making in industrial applications. This article lay the foundation of bayesion modelling with Pymc and their practical use cases.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          bayesian\n          \n        \n        \n      \n\n      \n        \n        October 10, 2025\n        \n      \n    \n  \n  \n\nNo matching items"
  },
  {
    "objectID": "blog/2024/06/index.html",
    "href": "blog/2024/06/index.html",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "",
    "text": "Most change efforts fail not because people resist, but because leaders target the wrong unit of change the individual. As Kurt Lewin showed, transformation begins not with lone heroes, but with the social forces of the group. Understanding this shift from motivating individuals to mobilizing teams is the key to making change stick."
  },
  {
    "objectID": "blog/2024/06/index.html#the-lone-wolf-vs.-the-pack",
    "href": "blog/2024/06/index.html#the-lone-wolf-vs.-the-pack",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "",
    "text": "Most change efforts fail not because people resist, but because leaders target the wrong unit of change the individual. As Kurt Lewin showed, transformation begins not with lone heroes, but with the social forces of the group. Understanding this shift from motivating individuals to mobilizing teams is the key to making change stick."
  },
  {
    "objectID": "blog/2024/06/index.html#introduction",
    "href": "blog/2024/06/index.html#introduction",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "Introduction",
    "text": "Introduction\nChange is the only constant in today‚Äôs global business environment. Yet despite endless meetings, emails, and training sessions, many organizations still struggle to make change stick (Al-Haddad and Kotnour 2015). Leaders pour effort into motivating individuals, only to watch employees slip back into old habits.\nThe problem is not a lack of effort, it is a flaw in perspective. Too many change strategies focus on individuals, treating transformation as a personal mindset issue rather than a collective movement. It is the lone wolf approach to change: trying to inspire one person at a time while ignoring the social currents that shape behavior.\nAs social psychologist Kurt Lewin discovered decades ago, the real force behind sustainable change is not found in individual effort, it lies in the group dynamic, the pack. Understanding how groups form, interact, and influence one another can transform the way organizations lead change today.\nThis post revisits Lewin‚Äôs insights on group dynamics and explores how leaders can shift from managing people in isolation to mobilizing the collective energy of their teams."
  },
  {
    "objectID": "blog/2024/06/index.html#the-heart-of-the-matter-target-the-group-not-the-individual",
    "href": "blog/2024/06/index.html#the-heart-of-the-matter-target-the-group-not-the-individual",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "The Heart of the Matter: Target the Group, Not the Individual",
    "text": "The Heart of the Matter: Target the Group, Not the Individual\nAt the core of Kurt Lewin‚Äôs work is a groundbreaking argument: meaningful change doesn‚Äôt happen at the individual level, but at the group level. According to Lewin, group behaviour is shaped by a field of forces and symbolic interactions that dictate the group‚Äôs structure and, in turn, modify individual behaviour (Burnes 2007).\nWhy is the group the primary focus? Because an individual even one open to change is constantly under immense group pressure to conform to existing norms. These group routines and patterns are not just passive habits; they are actively valued and enforced by the group to maintain stability (Burnes 2007).\nThis aligns with modern findings, such as Kotter‚Äôs, that resistance is more likely to arise from the system and its ingrained norms than from isolated individuals (Al-Haddad and Kotnour 2015). Therefore, any effective change strategy must concentrate on influencing the team‚Äôs collective norms, roles, and values."
  },
  {
    "objectID": "blog/2024/06/index.html#the-prerequisite-creating-a-felt-need-for-change",
    "href": "blog/2024/06/index.html#the-prerequisite-creating-a-felt-need-for-change",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "The Prerequisite: Creating a ‚ÄúFelt Need‚Äù for Change",
    "text": "The Prerequisite: Creating a ‚ÄúFelt Need‚Äù for Change\nBefore any change can begin, a critical prerequisite must be met: there must be a felt need. This is the deep, internal realization, both at the individual and group levels that the current way of doing things is no longer sufficient and that change is necessary (Burnes 2004, 2007).\nIf the collective felt-need within a group or organization is low, introducing change becomes problematic. You cannot impose transformation on a group that does not perceive a problem. Lewin argued that successful change begins with engaging group members, helping them understand the why behind the change, and fostering a shared commitment to move forward.\nThis ‚Äúfelt need‚Äù is not merely emotional; it represents a psychological unfreezing of the group‚Äôs current norms and equilibrium. Without this shared readiness, even the best-designed change initiatives are likely to fail."
  },
  {
    "objectID": "blog/2024/06/index.html#the-method-action-research-and-collaborative-change",
    "href": "blog/2024/06/index.html#the-method-action-research-and-collaborative-change",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "The Method: Action Research and Collaborative Change",
    "text": "The Method: Action Research and Collaborative Change\nUnderstanding group behaviour is one thing; changing it is another. Lewin‚Äôs framework is not just theoretical it is profoundly practical. He proposed a participative method known as Action Research, a cyclical process for diagnosing and addressing organisational challenges (Burnes 2004, 2007).\nAction Research emphasises that effective change is achieved with people, not to people. It involves a series of iterative steps that make the change process collaborative and adaptive:\n\nAnalyze the situation: The group collaboratively diagnoses the problem, identifying underlying issues that influence behaviour.\n\nIdentify alternatives: Members discuss and explore possible solutions.\n\nChoose and act: The group selects and implements the most appropriate solution.\n\nReflect and repeat: Results are evaluated, and the process is refined based on collective learning.\n\nThis method reflects Lewin‚Äôs belief that behaviour can only be understood and modified within a group context. It embodies the principle of self-management, where groups play an active role in shaping their own behaviour (Burnes 2004). Change, therefore, becomes a participative and collaborative process ‚Äî one that transforms resistance into ownership."
  },
  {
    "objectID": "blog/2024/06/index.html#the-framework-lewins-3-step-model-for-group-transformation",
    "href": "blog/2024/06/index.html#the-framework-lewins-3-step-model-for-group-transformation",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "The Framework: Lewin‚Äôs 3-Step Model for Group Transformation",
    "text": "The Framework: Lewin‚Äôs 3-Step Model for Group Transformation\nWith the group engaged and a collaborative process in motion, Lewin proposed that organisational change unfolds through his well-known 3-Step Model (Burnes 2004):\n\nUnfreeze: This stage creates the felt need for change by disrupting existing norms, routines, and assumptions. It prepares the group to let go of old patterns and accept that change is necessary.\n\nChange (Move): Guided by the principles of Action Research, the group develops and adopts new behaviours, attitudes, and values. This is the implementation phase, where learning and experimentation take place.\n\nRefreeze: The final stage solidifies the new equilibrium. New group norms are embedded in organisational culture and supported by systems and shared values, ensuring the change endures.\n\nLewin believed that because individuals are inherently influenced by group norms and social forces, sustainable transformation must occur at the group level not solely at the level of individual behaviour (Al-Haddad and Kotnour 2015). As he noted, group routines and patterns have intrinsic value; they‚Äôre not merely the product of opposing forces but serve a positive function in maintaining group identity and cohesion (Burnes 2004, 2007)."
  },
  {
    "objectID": "blog/2024/06/index.html#a-modern-reflection-is-refreezing-still-relevant",
    "href": "blog/2024/06/index.html#a-modern-reflection-is-refreezing-still-relevant",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "A Modern Reflection: Is Refreezing Still Relevant?",
    "text": "A Modern Reflection: Is Refreezing Still Relevant?\nWhile Lewin‚Äôs model remains foundational, modern change scholars question whether the concept of ‚Äúrefreezing‚Äù fits today‚Äôs rapidly changing business environment. In an age of constant disruption and agile methodologies, achieving a stable equilibrium may seem unrealistic.\nContemporary perspectives suggest that organisations should cultivate a culture of continuous learning and adaptability a state of being ‚Äúpermanently unfrozen.‚Äù Nevertheless, Lewin‚Äôs insight into the power of group dynamics remains timeless. Whether or not we ‚Äúrefreeze,‚Äù successful change still depends on engaging the collective forces that shape how people think, feel, and act together."
  },
  {
    "objectID": "blog/2024/06/index.html#conclusion",
    "href": "blog/2024/06/index.html#conclusion",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "Conclusion",
    "text": "Conclusion\nThe enduring genius of Kurt Lewin‚Äôs work is its simple, powerful truth: organizations don‚Äôt change, people do but people change most effectively in groups. Your next change initiative will succeed or fail based on your ability to look past individual behaviours and see the powerful, invisible currents of group norms, pressures, and values. To change the individual, you must first have the courage to engage, understand, and transform the group. So, is your organization empowering lone wolves ‚Äî or building a stronger pack?"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Thanks for stopping by!\nI am a Lead Data Scientist at the Centre for Intelligent Power (CIP) at Eaton in Dublin, Ireland. My expertise is centered at the intersection of applied machine learning and strategic innovation management.\nBeyond my technical specialization, I function as a strategic AI leader and builder. With a proven background in both industry and academia, I specialize in developing reliable, production-ready AI solution, while fostering team growth by mentoring engineers and leading high-impact, cross-functional projects.\nMy research is dedicated to the strategic intersection of applied machine learning and innovation management. I develop and investigate machine learning solutions aimed at addressing crucial business and sustainability challenges. Complementary to this, I also study the strategic frameworks and management practices necessary for organizations to effectively manage technology and drive sustained success in dynamic market environments."
  },
  {
    "objectID": "about/index.html#recent-blogs-publications",
    "href": "about/index.html#recent-blogs-publications",
    "title": "Anthony Faustine",
    "section": "Recent Blogs & Publications",
    "text": "Recent Blogs & Publications\n\n\nBlog Posts\n\n\n\n\n\n\n\n\n2025\n\n\n\n\n\nNo matching items\n\nSee all blog posts ‚Üí\n\n\nPublications\n\n\n\n\n\n\n\n\nJournal articles\n\n\n\n\n\nNo matching items\n\nSee all publications ‚Üí\n\n\n\n\n\n\n\n\nNoteCopyright\n\n\n\nUnless otherwise noted, all content is ¬© Anthony Faustine and licensed under the Creative Commons."
  },
  {
    "objectID": "blog/2024/07/index.html",
    "href": "blog/2024/07/index.html",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "",
    "text": "Ever been on a team where everything just clicks? communication is effortless, everyone knows their role, and the team anticipates each other‚Äôs moves, achieving goals with remarkable efficiency. This seamless collaboration is not a magic. It is often the result of something called a shared mental model, the invisible architecture that supports high-performance teamwork.\nThis post explores what shared mental models are, why they are a critical predictor of team effectiveness, and how leaders can consciously cultivate them, especially in the complex environments of virtual and hybrid teams."
  },
  {
    "objectID": "blog/2024/07/index.html#introduction",
    "href": "blog/2024/07/index.html#introduction",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "",
    "text": "Ever been on a team where everything just clicks? communication is effortless, everyone knows their role, and the team anticipates each other‚Äôs moves, achieving goals with remarkable efficiency. This seamless collaboration is not a magic. It is often the result of something called a shared mental model, the invisible architecture that supports high-performance teamwork.\nThis post explores what shared mental models are, why they are a critical predictor of team effectiveness, and how leaders can consciously cultivate them, especially in the complex environments of virtual and hybrid teams."
  },
  {
    "objectID": "blog/2024/07/index.html#what-is-a-shared-mental-model",
    "href": "blog/2024/07/index.html#what-is-a-shared-mental-model",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "What is a Shared Mental Model?",
    "text": "What is a Shared Mental Model?\nTeam effectiveness and coordination are critical aspects of a high-performance team. Team members who share a similar and organized understanding of team tasks and goals ‚Äî and who understand each other‚Äôs working environments ‚Äî are more likely to perform well.\nThis shared understanding and knowledge about the mission, goals, and other relevant environments among team members are called mental models. The shared mental model is one of the most frequently used concepts in team cognition (Schelble et al. 2022). (2022) further identify the shared mental model as a critical predictor of team effectiveness.\nLungeanu, DeChurch, and Contractor (2022) define shared mental models as team properties reflecting how team members organize knowledge and understanding about the team‚Äôs purpose, the nature of the work, and how they work together. Thus, team mental models are a collective mental representation among team members of how they interact in performing task-work (Larson and DeChurch 2020).\nThey represent the organized mental representations of the various component pieces relevant to a team‚Äôs overall task (Schelble et al. 2022). As (Schelble et al. 2022) point out, shared mental models measure whether or not team members share a common understanding of their shared tasks, roles, interdependencies, and strategies.\nSchelble et al. (2022) break shared mental models into two types:\n\nTask mental model ‚Äî covers aspects specific to understanding and completing a shared task.\n\nTeam mental model ‚Äî focuses on factors related to cooperation and communication within a team."
  },
  {
    "objectID": "blog/2024/07/index.html#the-impact-on-team-success",
    "href": "blog/2024/07/index.html#the-impact-on-team-success",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "The Impact on Team Success",
    "text": "The Impact on Team Success\nShared mental models can continually develop over time, becoming more effective and influencing various team outcomes, such as objective performance, team viability, member well-being, and strategic alignment (Lungeanu, DeChurch, and Contractor 2022).\nTeams with shared mental models can recognize one another‚Äôs needs and information requirements (Lungeanu, DeChurch, and Contractor 2022; Schelble et al. 2022), which enhances coordination and mutual support.\nWhile this may be more intuitive in physical teams, virtual teams ‚Äî now an integral part of modern work ‚Äî require special attention in developing and maintaining shared mental models among members.\nUnlike face-to-face teams, creating and sustaining mental models is harder in virtual environments. Leaders must therefore compensate for challenges such as communication barriers and cultural differences. These issues can impact relationship building, which is essential for developing and sustaining shared mental models.\nAs underlined in (Larson and DeChurch 2020), face-to-face teams tend to have stronger shared mental models than virtual ones.\nTo improve team effectiveness and performance in virtual settings, leaders should aim to create a conducive environment for shared mental models. This can be achieved by:\n\nCultivating high-quality, interpersonal communication.\n\nCreating psychological safety.\n\nAdopting a leadership style that aligns well with virtual collaboration (Larson and DeChurch 2020)."
  },
  {
    "objectID": "blog/2024/07/index.html#leaderships-role-in-building-a-shared-mind",
    "href": "blog/2024/07/index.html#leaderships-role-in-building-a-shared-mind",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "Leadership‚Äôs Role in Building a Shared Mind",
    "text": "Leadership‚Äôs Role in Building a Shared Mind\nLungeanu, DeChurch, and Contractor (2022) highlight that leadership particularly shared leadership plays a crucial role in creating and shaping shared mental models in teams. This applies to both face-to-face and virtual teams.\nFor instance, (Lungeanu, DeChurch, and Contractor 2022) note that when leadership responsibilities are shared among members, the team tends to show greater commitment and information sharing. This dynamic fosters trust and enhances performance.\nTeams that embrace shared leadership and have diverse skills, experiences, and perspectives are more likely to develop and maintain strong shared mental models.\nFurthermore, connected leadership as opposed to fragmented leadership offers several advantages for improving similarity in team mental models. It promotes accuracy, synchronization of effort, and cohesion or trust.\nFor example, (Lungeanu, DeChurch, and Contractor 2022) argue that hierarchical and coordinated leadership are better at promoting shared mental models than factional or isolated forms of leadership. They also emphasize that boundaries among members of shared leadership groups are permeable, allowing reciprocal leadership processes that reduce conflict and tension."
  },
  {
    "objectID": "blog/2024/07/index.html#conclusion",
    "href": "blog/2024/07/index.html#conclusion",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "Conclusion",
    "text": "Conclusion\nUltimately, a shared mental model is not just a nice-to-have; it‚Äôs the cognitive foundation upon which effective teams are built. It serves as the shared ‚Äúmap‚Äù that enables a group of individuals to navigate complex tasks together with clarity and confidence.\nWhile the rise of virtual work presents new challenges, the core principle remains: effective leadership is the catalyst. By fostering open communication, psychological safety, and a connected leadership structure, leaders can intentionally design the conditions for these powerful shared understandings to emerge and thrive."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vit√¶",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "now/index.html",
    "href": "now/index.html",
    "title": "What I‚Äôm doing now",
    "section": "",
    "text": "As of April 27, 2025, I‚Äôm spending all my time on these things:\n\nStaying at home pretty much 24/7 (STILL) because of the COVID-19 pandemic\nRaising 6 kids (17.5, 15, 12.5, 9.5, 7.5, and 3) and trying to stay sane (family blog)\nLiving in Atlanta and working as an assistant professor in the Department of Public Management and Policy at the Andrew Young School of Policy Studies at Georgia State University\n\nTeaching data visualization, program evaluation, comparative public administration, nonprofit management, and microeconomics at the Andrew Young School of Policy Studies at Georgia State University\n\nWorking as a part time data science mentor for Posit Academy\n\nConverting my dissertation into multiple articles and sending them out to journals + continuing my research on authoritarianism and international NGOs\n\nWorking on several articles on NGO restrictions with Suparna Chaudhry and Marc Dotson\n\nReading some sort of religiously themed text every day (books)"
  },
  {
    "objectID": "research/articles/deepnilmtk-2024/index.html",
    "href": "research/articles/deepnilmtk-2024/index.html",
    "title": "Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines",
    "section": "",
    "text": "Nonintrusive load monitoring (NILM) techniques are increasingly becoming a key instrument for identifying the power consumption of individual appliances based on a single metering point. Particularly, deep learning (DL) models are gaining interest in this regard. However, the challenges brought by the NILM datasets and the nonavailability of common experimental guidelines tend to compromise comparison, research transparency, and replicability. The limited adoption of efficient research instruments and lack of best practices guidelines contribute in huge part to this problem, where no features, encouraging standardized formats for benchmarking, and results sharing are offered. To address these issues, we first present a brief overview of recent best practices for DL and highlight how deep NILM research can benefit from these practices. Furthermore, we suggest a novel open-source toolkit leveraging these practices, i.e., Deep-NILMTK. The proposed toolkit offers a common testing bed for NILM algorithms independently of the underlying deep learning framework with a modular NILM pipeline that can easily be customized. Furthermore, Deep-NILMTK introduces the concept of experiment templating to offer predesigned experiments allowing to enhancing research efficiency. Leveraging this concept and DL best practices, we present a case study of creating an online NILM benchmark repository1 considering eight of the most popular deep NILM algorithms. All sources relative to the tool are made publicly available on Github2 along with the corresponding documentation."
  },
  {
    "objectID": "research/articles/deepnilmtk-2024/index.html#abstract",
    "href": "research/articles/deepnilmtk-2024/index.html#abstract",
    "title": "Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines",
    "section": "",
    "text": "Nonintrusive load monitoring (NILM) techniques are increasingly becoming a key instrument for identifying the power consumption of individual appliances based on a single metering point. Particularly, deep learning (DL) models are gaining interest in this regard. However, the challenges brought by the NILM datasets and the nonavailability of common experimental guidelines tend to compromise comparison, research transparency, and replicability. The limited adoption of efficient research instruments and lack of best practices guidelines contribute in huge part to this problem, where no features, encouraging standardized formats for benchmarking, and results sharing are offered. To address these issues, we first present a brief overview of recent best practices for DL and highlight how deep NILM research can benefit from these practices. Furthermore, we suggest a novel open-source toolkit leveraging these practices, i.e., Deep-NILMTK. The proposed toolkit offers a common testing bed for NILM algorithms independently of the underlying deep learning framework with a modular NILM pipeline that can easily be customized. Furthermore, Deep-NILMTK introduces the concept of experiment templating to offer predesigned experiments allowing to enhancing research efficiency. Leveraging this concept and DL best practices, we present a case study of creating an online NILM benchmark repository1 considering eight of the most popular deep NILM algorithms. All sources relative to the tool are made publicly available on Github2 along with the corresponding documentation."
  },
  {
    "objectID": "research/articles/deepnilmtk-2024/index.html#figure",
    "href": "research/articles/deepnilmtk-2024/index.html#figure",
    "title": "Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines",
    "section": "Figure",
    "text": "Figure\nFigure 1: Simulation results\n\n\n\nFigure 1: Simulation results"
  },
  {
    "objectID": "research/articles/deepnilmtk-2024/index.html#citation",
    "href": "research/articles/deepnilmtk-2024/index.html#citation",
    "title": "Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@article{WitesmanHeiss:2016,\n    Author = {Eva Witesman and Anthony Faustine},\n    Doi = {10.1007/s11266-016-9684-5},\n    Journal = {Voluntas: International Journal of Voluntary and Nonprofit Organizations},\n    Month = {8},\n    Number = {4},\n    Pages = {1500--1528},\n    Title = {Nonprofit Collaboration and the Resurrection of Market Failure: How a Resource-Sharing Environment Can Suppress Social Objectives},\n    Volume = {28},\n    Year = {2016}}"
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html",
    "href": "research/articles/mlpf-2024/index.html",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html#important-links",
    "href": "research/articles/mlpf-2024/index.html#important-links",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html#abstract",
    "href": "research/articles/mlpf-2024/index.html#abstract",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "Abstract",
    "text": "Abstract\nPower demand forecasting is becoming a crucial tool for the planning and operation of Low Voltage (LV) distribution systems. Most importantly, the high penetration of Photovoltaics (PV) power generation as part of Distributed Energy Resource (DER)s has transformed the power demand forecasting problem at the distribution level into net-load forecasting. This paper introduces a novel and scalable approach to probabilistic forecasting at LV substation with PV generation. It presents a multi-variates probabilistic forecasting approach, leveraging Quantile Regression (QR). The proposed architecture uses a computationally efficient feed-forward neural net to capture the complex interaction between the historical load demands and covariate variables such as solar irradiance. It is empirically demonstrated that the proposed method can efficiently produce well-calibrated forecasts, both auto-regressively or in a single forward pass. Furthermore, a benchmark against four state-of-the-art forecasting approaches shows that the proposed approach offers a desirable trade-off between forecasting accuracies, calibrated uncertainty, and computation complexity."
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html#important-figures",
    "href": "research/articles/mlpf-2024/index.html#important-figures",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2: Predicted probability of impartiality by levels of equality and democracy\n\n\n\n\n\nFigure 4: Tangents to the prediction function at 25 and 50"
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html#citation",
    "href": "research/articles/mlpf-2024/index.html#citation",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{10529636,\n  author={Faustine, Anthony and Nunes, Nuno Jardim and Pereira, Lucas},\n  journal={IEEE Transactions on Power Systems}, \n  title={Efficiency Through Simplicity: MLP-Based Approach for Net-Load Forecasting With Uncertainty Estimates in Low-Voltage Distribution Networks}, \n  year={2025},\n  volume={40},\n  number={1},\n  pages={46-56},\n  keywords={Forecasting;Uncertainty;Probabilistic logic;Predictive models;Substations;Load modeling;Distribution networks;Deep Neural Networks (DNN) Feed-forward Neural Network (FFN) Low Voltage (LV) distribution substation;Multilayer Perceptron (MLP);net-load;probabilistic forecasting;Photovoltaics (PV) generation;quantile regression (QR)},\n  doi={10.1109/TPWRS.2024.3400123}}"
  },
  {
    "objectID": "research/articles/nilm-review/index.html",
    "href": "research/articles/nilm-review/index.html",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/nilm-review/index.html#important-links",
    "href": "research/articles/nilm-review/index.html#important-links",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/nilm-review/index.html#abstract",
    "href": "research/articles/nilm-review/index.html#abstract",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "Abstract",
    "text": "Abstract\nThe key advantage of smart meters over traditional metering devices is their ability to transfer consumption information to remote data processing systems. Besides enabling the automated collection of a customer‚Äôs electricity consumption for billing purposes, the data collected by these devices makes the realization of many novel use cases possible. However, the large majority of such services are tailored to improve the power grid‚Äôs operation as a whole. For example, forecasts of household energy consumption or photovoltaic production allow for improved power plant generation scheduling. Similarly, the detection of anomalous consumption patterns can indicate electricity theft and serve as a trigger for corresponding investigations. Even though customers can directly influence their electrical energy consumption, the range of use cases to the users‚Äô benefit remains much smaller than those that benefit the grid in general. In this work, we thus review the range of services tailored to the needs of end-customers. By briefly discussing their technological foundations and their potential impact on future developments, we highlight the great potentials of utilizing smart meter data from a user-centric perspective. Several open research challenges in this domain, arising from the shortcomings of state-of-the-art data communication and processing methods, are furthermore given. We expect their investigation to lead to significant advancements in data processing services and ultimately raise the customer experience of operating smart meters."
  },
  {
    "objectID": "research/articles/nilm-review/index.html#important-figures",
    "href": "research/articles/nilm-review/index.html#important-figures",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2: Predicted probability of impartiality by levels of equality and democracy\n\n\n\n\n\nFigure 4: Tangents to the prediction function at 25 and 50"
  },
  {
    "objectID": "research/articles/nilm-review/index.html#citation",
    "href": "research/articles/nilm-review/index.html#citation",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{10529636,\n  author={Faustine, Anthony and Nunes, Nuno Jardim and Pereira, Lucas},\n  journal={IEEE Transactions on Power Systems}, \n  title={Efficiency Through Simplicity: MLP-Based Approach for Net-Load Forecasting With Uncertainty Estimates in Low-Voltage Distribution Networks}, \n  year={2025},\n  volume={40},\n  number={1},\n  pages={46-56},\n  keywords={Forecasting;Uncertainty;Probabilistic logic;Predictive models;Substations;Load modeling;Distribution networks;Deep Neural Networks (DNN) Feed-forward Neural Network (FFN) Low Voltage (LV) distribution substation;Multilayer Perceptron (MLP);net-load;probabilistic forecasting;Photovoltaics (PV) generation;quantile regression (QR)},\n  doi={10.1109/TPWRS.2024.3400123}}"
  },
  {
    "objectID": "research/articles/weighted-recurrence-nilm/index.html",
    "href": "research/articles/weighted-recurrence-nilm/index.html",
    "title": "Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring",
    "section": "",
    "text": "To this day, hyperparameter tuning remains a cumbersome task in Non-Intrusive Load Monitoring (NILM) research, as researchers and practitioners are forced to invest a considerable amount of time in this task. This paper proposes adaptive weighted recurrence graph blocks (AWRG) for appliance feature representation in event-based NILM. An AWRG block can be combined with traditional deep neural network architectures such as Convolutional Neural Networks for appliance recognition. Our approach transforms one cycle per activation current into an weighted recurrence graph and treats the associated hyper-parameters as learn-able parameters. We evaluate our technique on two energy datasets, the industrial dataset LILACD and the residential PLAID dataset. The outcome of our experiments shows that transforming current waveforms into weighted recurrence graphs provides a better feature representation and thus, improved classification results. It is concluded that our approach can guarantee uniqueness of appliance features, leading to enhanced generalisation abilities when compared to the widely researched V-I image features. Furthermore, we show that the initialisation parameters of the AWRG‚Äôs have a significant impact on the performance and training convergence."
  },
  {
    "objectID": "research/articles/weighted-recurrence-nilm/index.html#abstract",
    "href": "research/articles/weighted-recurrence-nilm/index.html#abstract",
    "title": "Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring",
    "section": "",
    "text": "To this day, hyperparameter tuning remains a cumbersome task in Non-Intrusive Load Monitoring (NILM) research, as researchers and practitioners are forced to invest a considerable amount of time in this task. This paper proposes adaptive weighted recurrence graph blocks (AWRG) for appliance feature representation in event-based NILM. An AWRG block can be combined with traditional deep neural network architectures such as Convolutional Neural Networks for appliance recognition. Our approach transforms one cycle per activation current into an weighted recurrence graph and treats the associated hyper-parameters as learn-able parameters. We evaluate our technique on two energy datasets, the industrial dataset LILACD and the residential PLAID dataset. The outcome of our experiments shows that transforming current waveforms into weighted recurrence graphs provides a better feature representation and thus, improved classification results. It is concluded that our approach can guarantee uniqueness of appliance features, leading to enhanced generalisation abilities when compared to the widely researched V-I image features. Furthermore, we show that the initialisation parameters of the AWRG‚Äôs have a significant impact on the performance and training convergence."
  },
  {
    "objectID": "research/articles/weighted-recurrence-nilm/index.html#figure",
    "href": "research/articles/weighted-recurrence-nilm/index.html#figure",
    "title": "Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring",
    "section": "Figure",
    "text": "Figure\nFigure 1: Simulation results\n\n\n\nFigure 1: Simulation results"
  },
  {
    "objectID": "research/articles/weighted-recurrence-nilm/index.html#citation",
    "href": "research/articles/weighted-recurrence-nilm/index.html#citation",
    "title": "Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{9144492,\n  author={Faustine, Anthony and Pereira, Lucas and Klemenjak, Christoph},\n  journal={IEEE Transactions on Smart Grid}, \n  title={Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring}, \n  year={2021},\n  volume={12},\n  number={1},\n  pages={398-406},\n  keywords={Feature extraction;Monitoring;Aggregates;Neural networks;Current measurement;Voltage measurement;Energy consumption;Non-intrusive load monitoring;load disaggregation;appliance recognition;weighted recurrence graphs;recurrence plots;V-I trajectories;convolutional neural networks;deep neural networks},\n  doi={10.1109/TSG.2020.3010621}}"
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html",
    "href": "research/conferences/cired-chicago-2024/index.html",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "",
    "text": "Paper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html#important-links",
    "href": "research/conferences/cired-chicago-2024/index.html#important-links",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "",
    "text": "Paper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html#abstract",
    "href": "research/conferences/cired-chicago-2024/index.html#abstract",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "Abstract",
    "text": "Abstract\nThis paper presents a probabilistic forecasting approach tailored for low voltage (LV) substations, offering short-term predictions for three crucial variables: voltage, reactive power, and active power. These parameters play a vital role in the resilience of distribution systems, especially in the presence of Distributed Energy Resources (DERs). Evaluation with simulated data shows that active and reactive power forecasts degrade notably with higher EV penetration, whereas voltage forecasting experiences less degradation across all scenarios."
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html#figure",
    "href": "research/conferences/cired-chicago-2024/index.html#figure",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "Figure",
    "text": "Figure\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution.\n\n\n\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution."
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html#bibtex-citation",
    "href": "research/conferences/cired-chicago-2024/index.html#bibtex-citation",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@INPROCEEDINGS{10916091,\n  author={Faustine, Anthony and Pereira, Lucas},\n  booktitle={CIRED Chicago Workshop 2024: Resilience of Electric Distribution Systems}, \n  title={Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power}, \n  year={2025},\n  volume={2024},\n  number={},\n  pages={27-31},\n  keywords={},\n  doi={10.1049/icp.2024.2555}}"
  },
  {
    "objectID": "research/conferences/forecast-symposium-2024/index.html",
    "href": "research/conferences/forecast-symposium-2024/index.html",
    "title": "Scalable and Efficient MLP-based Fully Parameterised Quantile for Probabilistic Power Forecasting",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/forecast-symposium-2024/index.html#important-links",
    "href": "research/conferences/forecast-symposium-2024/index.html#important-links",
    "title": "Scalable and Efficient MLP-based Fully Parameterised Quantile for Probabilistic Power Forecasting",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/forecast-symposium-2024/index.html#abstract",
    "href": "research/conferences/forecast-symposium-2024/index.html#abstract",
    "title": "Scalable and Efficient MLP-based Fully Parameterised Quantile for Probabilistic Power Forecasting",
    "section": "Abstract",
    "text": "Abstract\nIndustrial loads offer challenges for Non-intrusive Load Monitoring (NILM), such as phase imbalance associated with 3-phase lines. However, very little NILM research has been developed so far with this respect. This work presents a load recognition technique for NILM applying low complexity Fortesque Transform (FT). The FT decomposes the unbalanced 3-phase current waveform extracted from 3-phase aggregate power measurements to balance the given load. The 3-phases current waveform is transformed into an image-like representation using a compressedeuclidean distance matrix to improve the recognition ability further. The image representation is used as input to Convolutional Neural Network (CNN) classifier to learn the patterns of labeled data. Experimental evaluation of the industrial aggregated dataset shows that FT improves recognition performance by 5.8%, compared to the case without FT."
  },
  {
    "objectID": "research/conferences/forecast-symposium-2024/index.html#bibtex-citation",
    "href": "research/conferences/forecast-symposium-2024/index.html#bibtex-citation",
    "title": "Scalable and Efficient MLP-based Fully Parameterised Quantile for Probabilistic Power Forecasting",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@INPROCEEDINGS{10096324,\n  author={Faustine, Anthony and Pereira, Lucas},\n  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \n  title={Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring}, \n  year={2023},\n  volume={},\n  number={},\n  pages={1-5},\n  keywords={Load monitoring;Symmetric matrices;Power measurement;Power demand;Transforms;Signal processing;Convolutional neural networks;NILM;Industrial Appliances;Three-Phase;Fortesque Transform;Symmetrical Components},\n  doi={10.1109/ICASSP49357.2023.10096324}}"
  },
  {
    "objectID": "uses/index.html",
    "href": "uses/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "People often ask me what programs I use for my writing and design. In truth, my workflow tends to look like this or this, but here‚Äôs a more detailed list of all the interconnected programs I use.\nI try to keep this updated fairly regularly. As of April 27, 2025 this is what I‚Äôm using:"
  },
  {
    "objectID": "uses/index.html#writing",
    "href": "uses/index.html#writing",
    "title": "Anthony Faustine",
    "section": "Writing",
    "text": "Writing\n\nI permanently ditched Word as a writing environment in 2008 after starting grad school. I do all my writing in pandoc-flavored Markdown (including e-mails and paper-and-pencil writing)‚Äîit‚Äôs incredibly intuitive, imminently readable, flexible, future proof, and lets me ignore formatting and focus on content.\nThe key to my writing workflow is the magical pandoc, which converts Markdown files into basically anything else. I use Quarto to convert Markdown to HTML, PDF (through LaTeX), Word, and any other output format.\nI do my academic writing in several different programs: for stats-heavy stuff, I use Positron, and for prose-heavy stuff, I use iA Writer or Typora. I used to use Ulysses (and still think it‚Äôs a fantastic app!), but I found that I wasn‚Äôt using it as much in the past few years as I‚Äôve switched to Quarto for my writing.\nI store all my bibliographic references, books, and articles in Zotero (see here for why).\nI read and annotate all my PDFs with Zotero, both on desktop and on iOS, since it can export annotations as clean plain text.\nI store all my notes in Obsidian. Before switching to Obsidian I used Bear, which was great but didn‚Äôt support fancier things like math or syntax highlighting. Before that, I used Evernote, but I abandoned it in September 2018 after 9 years of heavy use, given their ongoing privacy controversies and mass layoffs."
  },
  {
    "objectID": "uses/index.html#development",
    "href": "uses/index.html#development",
    "title": "Anthony Faustine",
    "section": "Development",
    "text": "Development\nScience and research\n\nI post almost everything I write or develop on GitHub.\nI use R and either RStudio or Positron for most of my statistical computing, and I‚Äôm a dedicated devotee of the tidyverse. In the interest of full reproducibility and transparency, I make Quarto websites for each of my projects. See a list of these websites.\nI also use Python occasionally. Every few months I play with pandas and numpy and Jupyter, but I‚Äôm far more comfortable with R for scientific computing.\nI adapted the idea for research haikus from Kirby Nielsen.\nI use The Rogue Scholar to create stable DOIs for each of my blog posts.\nWeb\n\nI run my main web server on a DigitalOcean droplet, and I spin up temporary droplets all the time to offload scraping scripts, complicated R models, and to create on-the-fly VPNs.\nI normally access my remote files through SSH in a terminal, but for more complicated things, I‚Äôve found that Mountain Duck is indispensable.\nMy website uses Quarto.\nI use Let‚Äôs Encrypt for SSL.\nMiscellaneous\n\nI use a system-wide hotkey (ctrl + `) to open iTerm2 from anywhere.\nI use Homebrew to install Unix-y programs.\nI‚Äôm partial to both Fira Code and Consolas for my monospaced fonts."
  },
  {
    "objectID": "uses/index.html#desktop-apps",
    "href": "uses/index.html#desktop-apps",
    "title": "Anthony Faustine",
    "section": "Desktop apps",
    "text": "Desktop apps\nGraphic design\n\nThough I regularly use LaTeX (through pandoc), I adore InDesign and use it to make fancier academic and policy documents. I also used it for all the typesetting I did for BYU‚Äôs Neal A. Maxwell Institute, and continue to use it for the books I typset for By Common Consent Press.\nI use Illustrator all the time to enhance graphics I make in R and to make non-data-driven figures and diagrams.\nI use Lightroom and Photoshop too, but less often nowadays.\nDespite my dislike for Word and Excel, I use PowerPoint for all my presentations. It‚Äôs not my favorite, but in the apocryphal words of Churchill, ‚ÄúPowerPoint is the worst form of slide editor, except for all the others.‚Äù\nProductivity\n\nMy secret for avoiding the siren call of the internet is Focus. I have a blocklist that blocks Bluesky, Mastodon, Facebook, and Instagram from 8:00 AM‚Äì5:00 PM and 9:00 PM‚Äì11:59 PM.\nI was an early convert to Todo.txt and used it for years until my tasks and projects got too unwieldy. I switched to Taskpaper for a while, used 2Do for a couple years, and now I‚Äôm a convert to OmniFocus.\n\nFantastical 2‚Äôs natural language input is a glorious thing.\nI use Timery as an interface to Toggl to track my time during the day\nI keep a log of what I work on (and occasionally do more traditional diary-like entries) with Day One on both iOS and macOS.\nI use Espanso to replace and expand a ton of snippets‚Äîsee them all here, and I use Raycast to run dozens of little scripts that help control my computer with the keyboard.\nI use √úbersicht to show weather, iTunes track information, and my todo lists on my desktop.\nI use Dropbox for syncing stuff across my different devices and use Backblaze to back up all the computers in our house to the cloud.\nWith all these little helper apps, I use Bartender to keep my menubar clean."
  },
  {
    "objectID": "uses/index.html#hardware",
    "href": "uses/index.html#hardware",
    "title": "Anthony Faustine",
    "section": "Hardware",
    "text": "Hardware\n\nI use a 2021 14‚Ä≥ M1 Max MacBook Pro, a 13‚Ä≥ iPad Pro, and an iPhone 13.\nI use a Logitech Spotlight Presentation Remote when presenting, and when I teach in hybrid settings, I use an Insta360 Link motion sensing camera and a wireless lavalier microphone."
  },
  {
    "objectID": "blog/2025/10/bayesian-modelling-01.html",
    "href": "blog/2025/10/bayesian-modelling-01.html",
    "title": "Understanding Bayesian Thinking for Industrial Applications",
    "section": "",
    "text": "Introduction\nA company has recently installed a new, expensive machine. A critical question arises: How long will it last before failure?. The lead engineer, drawing on experience with previous models, estimates a lifespan of approximately 10 years. However, only 3 months of real-world test data are available for this specific unit, and a major warranty and service contract decision must be made immediately.\nThis scenario exemplifies a common challenge in industrial applications: making informed decisions with limited data. Bayesian thinking offers a powerful framework to address such problems by combining prior knowledge with observed data to update our beliefs about uncertain parameters.\nIn this article, we will explore the fundamentals of Bayesian thinking and how it can be applied to industrial scenarios like the one described above. We will cover key concepts such as prior distributions, likelihood functions, and posterior distributions, and demonstrate how to implement Bayesian models using Python‚Äôs PyMC library.\nTo immediately dive into the code and reproduce the models discussed in this article, you can use our accompanying resources:\n\nRepository: Fork bayesian-modelling repository and follow the setup instructions in the README.md file.\nNotebook: Launch the bayesian-modelling-01.ipynb notebook located in the notebook folder to follow along step-by-step.\n\n\n\nThe Building Blocks of Bayesian modeling\nTraditional (Frequentist) statistics relies on large datasets the ‚Äúlong run‚Äù to produce confident conclusions, a limitation in industrial contexts where data are often sparse. New products, machines, or processes typically generate only small samples, while valuable expert knowledge such as an engineer‚Äôs lifespan estimate is excluded from conventional models.\nBayesian inference overcomes these issues by combining prior knowledge with new data, enabling faster and more informed decisions when information is limited. This integration of expertise and evidence defines Bayesian thinking.\nThe process of updating our beliefs is formalized by Bayes‚Äô Theorem. \\[\nP(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}\n\\]\nWhile the formula looks mathematical, its components represent a beautifully intuitive learning cycle.\n\\[\n\\mathrm{Posterior} \\propto \\mathrm{Likelihood} \\times \\mathrm{Prior}\n\\]\nLet us break down how this relates to our machine failure problem; where \\(\\theta\\) is the machine‚Äôs expected failure rate, and \\(D\\) is the 3 months of test data.\n\nPrior, Likelihood, and Posterior\n\nPrior \\(P(\\theta)\\) represents the initial belief before observing any data. It incorporates domain expertise and historical knowledge. For instance, historical records may indicate that similar machines have an average lifespan of approximately ten years, implying a low failure rate. This prior belief defines the starting point for ùúÉ \\(\\theta\\).\nLikelihood \\(P(D \\mid \\theta)\\) quantifies the compatibility between the observed data and a given parameter value. It expresses the probability of observing the test outcomes for different possible failure rates. In this context, the likelihood measures how probable it is to observe zero failures within three months if the true average lifespan were, for example, five or fifteen years.\nPosterior \\(P(\\theta \\mid D)\\) represents the updated belief after incorporating the observed data. It integrates prior knowledge with the evidence provided by the likelihood. In the machine-failure example, the posterior distribution expresses the updated estimate of expected lifespan after combining historical information (e.g., the ten-year prior) with the three months of failure-free operational data.\n\n\n\nPyMC: The Probabilistic Programming Engine\nUnderstanding the relationship \\(\\mathrm{Posterior} \\propto \\mathrm{Likelihood} \\times \\mathrm{Prior}\\) is the conceptual heart of Bayesian analysis. However, calculating the actual posterior distribution, \\(P(\\theta \\mid D)\\), often involves complex, multi-dimensional integration that is impossible to solve analytically for real-world industrial problems. This challenge is addressed through Probabilistic Programming Languages (PPLs) such as PyMC.\nPyMC is an open-source Python library for constructing and fitting Bayesian statistical models using advanced computational algorithms, including Markov Chain Monte Carlo (MCMC) and variational inference. It is one of several modern PPLs available in Python, alongside Pyro and TensorFlow Probability (TFP). This tutorial focuses on PyMC due to its clarity, community support, and extensive documentation.\n\n\n\nCase study: A/B Testing with Small Samples\nTo shift from theory to practical implementation, we will apply the Bayesian building blocks Prior, Likelihood, and Posterior to a concrete industrial problem common in tech and e-commerce: A/B Testing\n\nSuppose you are a data scientist at an e-commerce company. The marketing team just launched a new website feature and wants to know:\n\n\nWhat‚Äôs the true conversion rate?\nIs it better than the old version (which historically has an 8% conversion rate)?\nHow much should we trust this estimate with limited data?\n\n\nDuring the first few days of the feature launch, the company has observed 200 visitors with only 15 conversion.\n\n\nLibrary Imports\nImport the required Python libraries for Bayesian modeling:\n\nNumPy for numerical computations\nPandas for data manipulation\nPyMC for Bayesian statistical modeling\nMatplotlib, arviz and altair for visualization\n\n\n\nShow the code\n#import libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nimport pymc as pm\nimport arviz as az\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import clear_output\nfrom great_tables import GT\nimport matplotlib.pyplot as plt\naz.style.use(\"arviz-white\")\nfrom cycler import cycler\ncolors=['#107591','#f69a48','#00c0bf', '#fdcd49',\"#cf166e\", \n        \"#7035b7\", \"#212121\",\"#757575\", \"#E0E0E0\",\"#FAFAFA\"]\nplt.rcParams.update({\n    \"figure.dpi\": 100,\n    \"axes.labelsize\": 12,\n    \"axes.titlesize\": 12,\n    \"figure.titlesize\": 12,\n        \"font.size\": 12,\n        \"legend.fontsize\": 12,\n        \"xtick.labelsize\": 12,\n        \"ytick.labelsize\": 12,\n        \"axes.linewidth\" : 0.5,\n        \"lines.linewidth\" : 1.,\n        \"legend.frameon\" :False,\n        'axes.prop_cycle': cycler(color=colors)\n        \n})\nimport altair \naltair.themes.enable('carbonwhite')\nimport altair as alt\nalt.data_transformers.enable('default', max_rows=None)\nclear_output()\n\n\n\nDefine key variables and parameters\n\n\nShow the code\n# For reproducibility\nnp.random.seed(42)\n\n# A/B Test Parameters\nvisitors = 200\nconversions = 15\nobserved_conversion_rate = conversions / visitors\nhistorical_baseline = 0.08\n\n\n\n\n\nDefine Prior, likelihood, and Posterior in PyMC\nTo model this problem in PyMC, one must first define the Prior and Likelihood distributions\nPrior: Since the conversion rate \\(\\theta\\), can only range between 0 and 1. The Beta distribution is ideal for modeling parameters that are bounded between 0 and 1, such as probabilities or rates.\nThe Beta distribution is controlled by two parameters, \\(\\alpha\\) and \\(\\beta\\). hese parameters are set to formally encode the prior knowledge: the historical 8% conversion rate. The mean of a \\(\\mathrm{Beta}(\\alpha,\\beta)\\) distribution is \\(\\frac{\\alpha}{ \\alpha + \\beta}\\). Since the historical rate is 8% (or 0.08), we need to choose \\(\\alpha\\) and \\(\\beta\\) such that: \\[\n\\frac{\\alpha}{ \\alpha + \\beta} = 0.08\n\\]\nTo determine the strength of this belief, a number that represents the effective sample size (ESS) of the historical knowledge is chosen. Choosing a hypothetical ESS of 100 trials, we can solve for \\(\\alpha\\) and \\(\\beta\\): - ESS \\(= \\alpha + \\beta = 100\\) - \\(\\alpha\\) (hypothetical successes) \\(=100√ó0.08=8\\) - \\(\\beta\\) (hypothetical failures) \\(=100‚àí8=92\\)\nLikelihood the Likelihood is determined by the process that generated the data. Since there is a fixed number of trials (N=200 visitors) and the number of successes (k=15 conversions) is counted, this is a Binomial distribution.\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n    \n    # Sample from posterior distribution\n    trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True)\n\nclear_output()\n\n\nThis small block of code above defines and runs our entire Bayesian analysis. For those seeing PyMC for the first time, here is what each section is doing:\n\nwith pm.Model() as model: This block acts as a container for all the random variables and data in our model. Everything inside this context belongs to the conversion_model\nconversion_rate = pm.Beta(...): We are telling PyMC that the true conversion_rate is a random variable, and our initial belief is described by the Beta(8,92) distribution.\nlikelihood = pm.Binomial(...): This defines the process that generated our observed data. We link the conversion_rate parameter to the actual observed data (n=visitors, observed=conversions) using the appropriate Binomial distribution.\npm.sample(...): This is where the magic happens! The pm.sample function runs the MCMC sampler (the computational engine) to combine the Prior and the Likelihood, effectively calculating the Posterior distribution. We ask the sampler to draw 2000 samples after a 1000-sample tuning period, running 4 independent chains to ensure reliable results.\n\n\n\n\nModel diagnostics\nRunning pm.sample() generates the raw output, but the job isn‚Äôt done yet. Before we trust the results, we must perform Model Diagnostics to ensure our computational engine (the MCMC sampler) has worked correctly. The single most important diagnostic check is confirming Convergence.\n\nModel Convergencevergence\nIn Bayesian inference, Markov Chain Monte Carlo (MCMC) methods are employed to sample from the complex posterior distribution. These samples are relied upon to accurately estimate quantities like the mean conversion rate or its credible interval.\nConvergence is the guarantee that the MCMC chains have explored the entire distribution and are now producing samples that truly represent the target Posterior distribution, and are not just stuck in a starting location.\n\nAnalogy: Imagine trying to understand the shape of a deep, misty valley (the posterior). If your chains haven‚Äôt converged, they might be stuck high up on a ridge, missing the true, deep center. Diagnostics are the tools we use to confirm the chains have found and are walking across the bottom of the true valley.\n\nPyMC uses the supporting library ArviZ for standardizing and analyzing the results, which provides the following diagnostics and plots\n\nTrace Plots: Visual inspection of parameter samples across iterations.\n\nGood trace plots look like fuzzy caterpillars with no trends or jumps.\n\nR-hat (Gelman-Rubin Statistic): Measures how well multiple chains agree.\n\nR-hat ‚âà 1 means convergence.\nR-hat &gt; 1.01 suggests problems.\n\nEffective Sample Size (ESS): Indicates how many independent samples you effectively have.\n\nLow ESS means poor mixing or autocorrelation. Good ESS is typically &gt; 200 per parameter.\n\n\nThe most efficient way to check convergence numerically is using the ArviZ summary function, specifically asking for the diagnostics az.summary(trace, kind=\"diagnostics\"). Alternatively, az.plot_trace(trace) can be used to get a visual sense of convergence.\n\nExample Diagnostic Output\n\n\nShow the code\ndiag_table=az.summary(trace, kind=\"diagnostics\")[['ess_bulk', 'ess_tail', 'r_hat']]\nGT(diag_table).tab_header(\n    title=\"\",\n    subtitle=\"Conversion Rate Model\"\n).cols_label({\n        'ess_bulk': 'ESS Bulk',\n        'ess_tail': 'ESS Tail.',\n        'r_hat': 'R-hat',\n    })\n\n\n\n\n\n\n\n\n\n\n\nConversion Rate Model\n\n\nESS Bulk\nESS Tail.\nR-hat\n\n\n\n\n3966.0\n5559.0\n1.0\n\n\n\n\n\n\n\n\nThis table immediately indicates that the model is reliable and ready for analysis\n\n(R_hat = 1.0,Goal Achieved): Since R_hat is exactly 1.0, this confirms that the four independent MCMC chains have fully converged and agree on the shape of the posterior distribution. The model is reliable.\nESS bulk ‚Äãand ESS tail (3966.0 and 5559.0, Goal Achieved): Both effective sample sizes are significantly greater than the ‚â•400 minimum threshold. This means there are plenty of high-quality, effectively independent samples to accurately estimate the mean, mode, and credible intervals of the true conversion rate.\n\n\nVisual Check: Trace Plots\nWhile the numbers in the summary table are essential, visually inspecting the MCMC chains confirms the story.\n\n\nShow the code\naz.plot_trace(trace);\n\n\n\n\n\n\n\n\n\nThe trace plot above shows excellent convergence for our conversion rate (\\(\\theta\\)) model, supporting the conclusions from our quantitative diagnostics. The plot is split into two panels 1. Right Panel: MCMC Sampling Behavior\n\nThis panel shows the raw sampled values across iterations for each of our four chains. The sampled values oscillate stably around ‚àº0.075 (7.5%) without any noticeable trends, sudden jumps, or long-term drifts.\nThe different lines (chains) are thoroughly intertwined and overlap completely. This ‚Äúfuzzy caterpillar‚Äù appearance is the visual proof that the sampler is efficiently exploring the parameter space and that all chains have converged to the same distribution.\nThe stable behavior confirms the chain has reached stationarity, meaning it is now sampling from the true, converged Posterior distribution.\n\n\nLeft Panel: Posterior Distribution\n\nThis panel shows the estimated Posterior probability density function (PDF) based on the samples. The shape is smooth and unimodal (single peak), indicating a well-behaved posterior without ambiguity.\nThe peak is clearly centered at a value close to our observed rate (7.5%), which is what we expect when combining a prior (8%) and data (7.5%).\nThe spread of the distribution clearly visualizes our remaining uncertainty about the true conversion rate.\n\n\n\n\n\n\n\nPrior Predictive Checks: Validating Model Assumptions\nWhile we have demonstrated that the convergence of the fitted model a complete Bayesian analysis requires us to first validate the assumptions we made before seeing any data (priors). This validation is accomplished through the Prior Predictive Check.\n\nPrior Predictive Checks helps validate the prior assumptions before fitting the model to data. It show what kind of data the model expects to generate based solely on the prior beliefs.\n\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n\n\nGiven our prior belief of an 8% conversion rate, we can simulate what kind of data we would expect to see if this belief were true. This is done by generating synthetic datasets from the prior distribution and comparing them to the actual observed data. This is acomplished in PyMC by running pm.sample_prior_predictive() function.\n\n\nShow the code\nwith conversion_model:\n    prior_pred = pm.sample_prior_predictive()\n\n\nSampling: [conversion_rate, observations]\n\n\n\n\nShow the code\nfig, ax=plt.subplots(figsize=(4, 2.8))\naz.plot_ppc(prior_pred, group=\"prior\", ax=ax)\nplt.xlabel('conversions')\nplt.ylabel('Density');\n\n\n\n\n\n\n\n\n\nThe generated plot from the prior predictive sampling shows the distributions of simulated data (the number of conversions) created by sampling from the \\(\\text{Beta}(8,92)\\) prior distribution. - The distribution of the simulated data (prior predictive line) appear broadly spread out and relatively flat across a wide range (0 to 40+ conversions). This indicates that the prior allows for many different outcomes, and thus it is not overly restrictive. - The dashed line represents the average predicted number of conversions It is also quite flat and non-committal. - It evident that the prior predictive distribution does not overly concentrate around any specific number of conversions, which is desirable when we want to remain open to various possible outcomes. - So the prior predictive check confirms that our chosen Beta(8,92) prior is reasonable though it also quite weakly informative how?\nRed flags to watch for:\n\nImpossible values: Predictions outside the feasible range (e.g., negative conversion, &gt;200 observations)\nUnrealistic concentrations: If priors are too informative, you might see all predictions clustered in a narrow range\nPoor scaling: Predictions that don‚Äôt match the scale of your problem\n\n\nPosterior Predictive Checks\nPosterior Predictive Checks addresses the important test: Does the model actually make sense given the data observed?\nThis moves the process from confirming the samplers or priors to validating the model itself.\n\nPPCs evaluate model fit by comparing the observed data to simulated data from the posterior distribution. If a model accurately represents the data-generating process, the simulated data should resemble the actual observations.\n\nTo achive this it is important to generate new, simulated data from the posterior distribution and compare it to the actual observed data. This is done using the pm.sample_posterior_predictive function in PyMC.\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n    \n    # Sample from posterior distribution\n    trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True)\n\n\nwith conversion_model:\n    posterior_pred = pm.sample_posterior_predictive(trace, random_seed=42,)\n    \nclear_output()\n\n\n\n\nShow the code\nppc_summary = az.summary(posterior_pred, kind='stats', hdi_prob=0.95)\nppc_table = (\n    GT(ppc_summary)\n    .cols_label({\n        'mean': 'Mean',\n        'sd': 'Std-Dev.',\n        'hdi_2.5%': 'HDI 3%',\n        'hdi_97.5%': 'HDI 97%'\n    }).tab_header(\n        title=\"Posterior Predictive Summary Statistics\",\n        subtitle=\"\"\n    )\n)\nppc_table \n\n\n\n\n\n\n\n\nPosterior Predictive Summary Statistics\n\n\n\n\n\nMean\nStd-Dev.\nHDI 3%\nHDI 97%\n\n\n\n\n15.314\n4.806\n6.0\n24.0\n\n\n\n\n\n\n\n\nIt clear that the predicted mean (15.314) is extremely close to the actual observed count (15). This is a strong indication that the conversion_rate model fits the data very well. And thus the choice of the Beta Prior and Binomial Likelihood is appropriate for this data. The model predicts that 95% of the time, the number of conversions will fall between 6 and 24. Since the company observed value of 15 falls well within this 95% HDI, the actual observation is considered highly plausible according to your model.\n\n\nShow the code\nppc_samples = posterior_pred.posterior_predictive['observations'].values.flatten()\npercentile = (ppc_samples &lt;= conversions).mean() * 100\nprint(f\"Observed convervation ({conversions}) is at the {percentile:.1f}th percentile of predictions\")\n\n\nObserved convervation (15) is at the 53.7th percentile of predictions\n\n\nThe value of 53.7 is very close to the ideal 50, which provides further strong evidence (in addition to the mean of 15.314 already seen) that:\n\nThe model is not biased (it is not systematically over- or under-estimating the data).\nThe choice of the Beta-Binomial model is highly appropriate for this data set.\n\n\n\n\nModel Utility and Business Decision\nFollowing the confirmation of the reliability of the Bayesian model through the Posterior Predictive Check (PPC), the focus now shifts from ‚ÄúDoes the model fit the data?‚Äù to the commercially critical question: ‚ÄúIs the model useful for making business decisions?‚Äù\nSpecifically, the derived posterior distribution is utilized to quantify the evidence that the new feature‚Äôs conversion rate (Feature B) is superior to the existing, established conversion rate (Feature A, which has a known baseline rate of 0.08).\nTo achieve this, we need to extract the posterior distribution for the conversion_rate parameter that your model estimated.\n\n\nShow the code\nposterior_samples = trace.posterior.stack(sample=(\"chain\", \"draw\"))\nconversion_samples=posterior_samples['conversion_rate'].values\n\n\nNext, calculate the ‚Äúprobability of superiority‚Äù‚Äîthe probability that the new feature (B) is better than the old (A).\nFinally, the expected uplift in conversion rate can be computed and translated into business value. This involves calculating the difference between the posterior mean conversion rate and the historical rate, then multiplying by the number of visitors and average revenue per conversion.\n\n0.95: Strong Evidence. The new feature is very likely superior. Launching it should be considered.\n0.80: Moderate Evidence. The new feature is likely better, but there is still a 20% chance it is worse. The decision depends on company risk tolerance.\n0.50: No Evidence. The new feature is a toss-up; there is no statistical reason to prefer it over the old feature.\n\n\n\nShow the code\nprob_superiority = (conversion_samples &gt; historical_baseline).mean()\n# Display the result\nprint(f\"The probability that the new feature is better than the old rate (0.08) is: {prob_superiority:.1%}\")\n\n\nThe probability that the new feature is better than the old rate (0.08) is: 39.7%\n\n\nThe calculated Probability of Superiority is approximately 0.40, indicating that there is a 40% chance the new feature‚Äôs conversion rate is better than the old feature‚Äôs with 8% rate. This imply that there is a 60.3% chance the new feature is worse than the baseline.\nSince the probability that the new feature is superior is well below the neutral benchmark of 50%, the data does not support replacing the existing Feature A with the new Feature B based on conversion rate alone. The new feature is highly likely to perform worse.\n\nProbability of Meaningful Improvement.\nDepending on the business context, one might also want to calculate the probability that the new feature is better by a meaningful margin (e.g., &gt;1% improvement). This calculates the chance that the new feature is better than the old feature by a practically significant margin of at least 1 percentage point. This is a crucial business metric. Sometimes a tiny statistical ‚Äúwin‚Äù is not worth the cost of development and deployment. If this probability is low, it confirms the feature is not a major improvement.\n\n\nShow the code\nprob_1pct_improvement = (conversion_samples &gt; historical_baseline+0.01).mean()\nprint(f\" Probability of &gt;1% improvement: {prob_1pct_improvement:.1%}\")\n\n\n Probability of &gt;1% improvement: 18.4%\n\n\n\n\nProbability of Hitting a Target Rate\nThis calculates the chance that the true conversion rate of the new feature is 10% or higher. This is useful if 10% is a specific, ambitious KPI (Key Performance Indicator) or goal set by the marketing or product team. It tells how likely the team is to meet its goal.\n\n\nShow the code\nprob_10pct = (conversion_samples &gt; 0.10).mean()\nprint(f\" Probability of &gt;10% improvement: {prob_10pct:.1%}\")\n\n\n Probability of &gt;10% improvement: 6.3%\n\n\n\nExpected Value of the Change.\nThis is the single-number best estimate of the average change (positive or negative) to expect upon deploying the new feature. This is the foundation for the ‚ÄúExpected Business Impact‚Äù section. If this value is negative, it represents an Expected Loss.\nIf one calculates: expected_uplift√óTotal¬†Visitors√óARPC, the estimated dollar value of the change is obtained.\nThe goal is to translate the statistically determined average change in conversion rate into a clear financial outcome for the business. This calculation provides the most actionable insight for the go/no-go decision on the new feature.\n\n\nShow the code\n# Calculate key business probabilities\nexpected_uplift = conversion_samples.mean() - historical_baseline\n\n# Calculate expected business impact\nmonthly_visitors = 10000\nexpected_additional_conversions = monthly_visitors * expected_uplift\nconversion_value = 50  # Average value per conversion\nexpected_monthly_value = expected_additional_conversions * conversion_value\n\nprint(f\"Expected uplift: {expected_uplift:.3f} ({expected_uplift:.1%} points)\")\nprint(f\"Expected monthly value: ${expected_monthly_value:,.0f}\")\n\n\nExpected uplift: -0.003 (-0.3% points)\nExpected monthly value: $-1,717\n\n\nWe see that the expected uplift is -0.030 (-3.0% points), indicating that, on average, the new feature is expected to decrease the conversion rate by 3 percentage points. The expected financial consequence of deploying the new feature is a loss of $1,717 per month\nThe data suggests that deploying the new feature would likely result in an Expected Monthly Loss of $1,717. This financial quantification is the most compelling reason to reject the new feature based on conversion rate performance.\nThe decision to proceed should only be considered if the feature provides other, unquantified benefits (e.g., improved customer retention, compliance, or brand value) that are estimated to be worth more than $1,717 per month.\n\n\n\nPrior sensitivity analysis\nAssessing how the choice of prior influences the final decision is important. This transparency is a key strength of Bayesian modeling, as it allows testing every assumption. Thus, one must determine how much the choice of prior affects the outcome by comparing several Beta priors. These comparisons will include priors from very optimistic (expecting a \\(15\\%\\) conversion rate) to skeptical (expecting only \\(4\\%\\)), plus an uninformative prior that lets the data speak entirely for itself.\n\n\nShow the code\n\npriors_to_test = [\n    (\"Very Optimistic\", 15, 85),    # Believes 15% conversion rate\n    (\"Optimistic\", 12, 88),         # Believes 12% conversion rate  \n    (\"Historical Based\", 8, 92),    # Our original prior (8%)\n    ('Strong Historical', 80, 920),      # Mean = 0.08, ESS = 1000\n    (\"Skeptical\", 4, 96),           # Believes 4% conversion rate\n    (\"Uninformative\", 1, 1),        # Let data dominate completely\n]\n\nsensitivity_results = []\ntraces_to_comprare = []\n\nfor idx, (prior_name, alpha, beta) in enumerate(priors_to_test):\n    with pm.Model() as model:\n        conversion_rate = pm.Beta(\"conversion_rate\", alpha=alpha, beta=beta)\n        obs = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n        trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True, progressbar=False)\n        \n \n    posterior_samples = trace.posterior.stack(sample=(\"chain\", \"draw\"))\n    conversion_samples=posterior_samples['conversion_rate'].values\n    expected_uplift = conversion_samples.mean() - historical_baseline\n    expected_additional_conversions = monthly_visitors * expected_uplift\n    expected_monthly_value = expected_additional_conversions * conversion_value\n\n\n    posterior_mean = conversion_samples.mean()\n    posterior_std = conversion_samples.std()\n    prior_std = np.sqrt((alpha * beta) / ((alpha + beta)**2 * (alpha + beta + 1)))\n    prob_better = (conversion_samples &gt; historical_baseline).mean()\n    uncertainty_reduction = max(0, (prior_std - posterior_std) / prior_std)\n    \n    df = pd.DataFrame({\"Prior\": prior_name,\n                       \"Posterior\": conversion_samples}\n                       )\n    \n    sensitivity_results.append({\n        'Prior_Belief': prior_name,\n        'Prior_Parameters': f\"Beta({alpha}, {beta})\",\n        'Prior_Mean': f\"{alpha/(alpha+beta):.3f}\",\n        'Posterior_Mean': f\"{posterior_mean:.3f}\",\n        'Prob_Better': f\"{prob_better:.1%}\",\n        \"Expected_uplift\": f\"${expected_uplift:.1%}\",\n        \"Expected_monthly_value\": f\"${expected_monthly_value:,.0f}\",\n        'Uncertainty_Reduction': f\"{uncertainty_reduction:.1%}\",\n        'Data_Influence': 'Strong' if uncertainty_reduction &gt; 0.8 else 'Moderate'\n    })\n    traces_to_comprare.append(df) \nsensitivity_df = pd.DataFrame(sensitivity_results)\ntraces_to_comprare=pd.concat(traces_to_comprare)\nclear_output()\n\n\n\nLet first visualise the different priors\n\n\nShow the code\ndomain = [\"Very Optimistic\", \"Optimistic\", \"Historical Based\", \"Strong Historical\", \"Skeptical\", \"Uninformative\"]\nreference_data = pd.DataFrame([\n    {'x': 0.08, 'label': 'Old Feature CR (8%)', 'color': 'red', 'label_x': 0.08},\n    {'x': 0.075, 'label': 'New Feature CR (7.5%)', 'color': 'green', 'label_x': 0.075}\n])\n\n\nchart = alt.Chart(traces_to_comprare).transform_density(density='Posterior', groupby=['Prior'], as_=['Posterior', 'density'])\\\n    .mark_line(opacity=0.5)\\\n    .encode(x='Posterior:Q', \n            y='density:Q', \n            color=alt.Color(\"Prior:N\").scale(domain=domain, range=colors[:len(domain)])\n            )\n\nreference_lines = alt.Chart(reference_data).mark_rule(\n    strokeDash=[5, 5], # Dashed line\n    size=1, \n).encode(\n    x='x:Q',\n    color=alt.Color('label:N', \n                    scale=alt.Scale(domain=reference_data['label'].tolist(), \n                                    range=reference_data['color'].tolist()),\n                    legend=alt.Legend(title=\"Conversion Rate\"))\n)\n\n\nfinal=chart + reference_lines \nfinal=final.properties(title='Posterior Distributions by Prior Belief', width=700, height=200).configure_axis(\n    # Set grid to False to remove all grid lines\n    grid=False\n).configure_view(\n    # Optional: Remove the surrounding border of the plot area\n    strokeWidth=0 \n)\nfinal.save('posterior_sensitivity.pdf')\nfinal\n\n\n\n\n\n\n\n\nThe figure above show Posterior belief distributions under different prior assumptions. Each curve in the figure represents a posterior distribution for the conversion rate under a different prior assumption. The horizontal axis shows possible conversion rates, and the vertical axis shows how plausible each value is after combining the prior belief with the observed data.\nDespite very different starting assumptions, the posterior estimates converge around 7‚Äì10%, showing that the data provide a stable, consistent signal largely independent of the chosen prior. All reasonable priors converge to the same conclusion: the new feature likely isn‚Äôt better than the old one.\nTo support decision-making, we compile a summary table comparing key metrics across these prior choices.\n\n\nShow the code\n# Create sensitivity analysis table\nsensitivity_table = (\n    GT(sensitivity_df)\n    .tab_header(\n        title=\"Sensitivity Analysis: Impact of Prior Beliefs\",\n        subtitle=\"How Different Starting Assumptions Affect Final Conclusions\"\n    )\n    .cols_label(\n        Prior_Belief=\"Prior Belief\",\n        Prior_Parameters=\"Prior Distribution\", \n        Prior_Mean=\"Prior Mean\",\n        Posterior_Mean=\"Posterior Mean\",\n        Prob_Better=\"Prob. Better\",\n        Data_Influence=\"Data Influence\",\n        Uncertainty_Reduction=\"Uncertainty Reduction\",\n        Expected_uplift=\"Expected Uplift\",\n        Expected_monthly_value=\"Expected Monthly Value\"\n    )\n    .data_color(\n        columns=[\"Prob_Better\"],\n        palette=[\"#C73E1D\", \"#F18F01\", \"#2E8B57\"],\n        domain=[0.0, 0.5, 1.0]\n    )\n    .data_color(\n        columns=[\"Data_Influence\"],\n        palette=[\"#2E8B57\", \"#F18F01\", \"#C73E1D\"],\n        domain=[\"Strong\", \"Moderate\", \"Weak\"]\n    ).data_color(\n        columns=[\"Uncertainty Reduction\"],\n        palette=[\"#C73E1D\", \"#F18F01\", \"#2E8B57\"],\n        domain=[0.0, 80, 100]\n    ).tab_source_note(\n        \"Analysis shows robustness of conclusions to different prior assumptions ‚Ä¢ \"\n        \"Even skeptical priors converge toward data-driven truth\"\n    )\n    .tab_options(\n        table_width=\"100%\",\n    )\n)\nclear_output()\nsensitivity_table\n\n\n\n\n\n\n\n\nSensitivity Analysis: Impact of Prior Beliefs\n\n\nHow Different Starting Assumptions Affect Final Conclusions\n\n\nPrior Belief\nPrior Distribution\nPrior Mean\nPosterior Mean\nProb. Better\nExpected Uplift\nExpected Monthly Value\nUncertainty Reduction\nData Influence\n\n\n\n\nVery Optimistic\nBeta(15, 85)\n0.150\n0.100\n88.2%\n$2.0%\n$9,930\n52.2%\nModerate\n\n\nOptimistic\nBeta(12, 88)\n0.120\n0.090\n71.0%\n$1.0%\n$4,992\n49.1%\nModerate\n\n\nHistorical Based\nBeta(8, 92)\n0.080\n0.077\n39.7%\n$-0.3%\n$-1,717\n44.1%\nModerate\n\n\nStrong Historical\nBeta(80, 920)\n0.080\n0.079\n47.3%\n$-0.1%\n$-263\n10.4%\nModerate\n\n\nSkeptical\nBeta(4, 96)\n0.040\n0.064\n11.8%\n$-1.6%\n$-8,231\n29.0%\nModerate\n\n\nUninformative\nBeta(1, 1)\n0.500\n0.079\n44.8%\n$-0.1%\n$-456\n93.5%\nStrong\n\n\n\nAnalysis shows robustness of conclusions to different prior assumptions ‚Ä¢ Even skeptical priors converge toward data-driven truth\n\n\n\n\n\n\n\n\n\nKey observations:\n\nPrior Pull: The data moderate extreme beliefs. The very optimistic prior (0.150) is pulled down to 0.100, while the skeptical prior (0.040) rises to 0.064‚Äîdemonstrating how new evidence shifts expectations toward a central value.\nPrior Strength vs.¬†Data Influence: The Strong Historical prior (Beta(80, 920)) and Uninformative prior (Beta(1, 1)) yield similar posteriors (~0.079, ~45% Prob_Better), but for opposite reasons. The former is dominated by prior belief; the latter, by data‚Äîshowing that strong evidence can overcome weak or missing priors.\nImpact on Decision Metrics:: The Very Optimistic prior suggests an 88% Prob_Better and +$9.9k expected value, favoring a launch. The Skeptical prior predicts only 12% Prob_Better and ‚Äì$8.2k, suggesting the opposite. Even moderate priors (e.g., 40‚Äì47%) could tip a go/no-go decision depending on the threshold.\n\n\n\n\nCommon bayesian pitfalls and best practices\nWhile Bayesian modeling provides a powerful framework for decision-making under uncertainty, several common mistakes can reduce its effectiveness.\n\nThe first is overconfident priors, where excessively strong prior assumptions prevent new data from influencing the results. This issue is best mitigated by starting with weak or moderately informative priors and refining them as more evidence becomes available.\n\n\nA second pitfall is ignoring prior sensitivity. Conclusions may shift significantly depending on the chosen prior distribution, so it is essential to conduct sensitivity analyses using multiple plausible priors to ensure that insights are robust.\n\n\nThe third major issue involves misinterpreting probability. A statement such as ‚Äú95% probability‚Äù does not imply absolute certainty; rather, it reflects the degree of belief given the available data and assumptions. Probabilities in Bayesian analysis are best interpreted in terms of relative confidence, risk, and decision trade-offs.\n\n\n\nNext steps in the bayesian journey\nWe have laid the groundwork by exploring the core components of the Bayesian approach. The subsequent session will focus on advanced practical implementation. We will apply this framework to critical survival analysis problems, modeling the probability and timing of events such as equipment failure.\n\n\nResources to continue learning\n\nBayesian Modeling and Computation in Python\nBayesian Methods for Hackers\nStatistical Rethinking\nDoing Bayesian Data Analysis\nBayesian Data Analysis\nPyMC Documentation\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nFaustine, Anthony. 2025. ‚ÄúUnderstanding Bayesian Thinking for\nIndustrial Applications.‚Äù October 10, 2025. https://sambaiga.github.io/blog/2025/10/bayesian-modelling-01.html."
  }
]