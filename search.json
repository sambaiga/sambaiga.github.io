[
  {
    "objectID": "pages/project/index.html",
    "href": "pages/project/index.html",
    "title": "Projects",
    "section": "",
    "text": "A curated collection of tools, libraries, and experiments I‚Äôve released publicly, with a focus on usability, research, and open-source development.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLPforecast\n\n\n\nai\n\ntools\n\ndata-science\n\n\n\nA simple and efficient neural network architecture for point forecasting. The architecture minimizes model complexity while improving forecasting performance.\n\n\n\nMay 15, 2024\n\n\n\n\n\n\ntags\n\n\npython,neural-networks,time-series,forecasting\n\n\n\n\n\nstatus\n\n\nactive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwiga\n\n\n\nai\n\ntools\n\ndata-science\n\n\n\nA simple and efficient neural network architecture for providing point forecasting. The architecture has been designed to minimize model complexity while improving forecasting performance.\n\n\n\nMay 15, 2024\n\n\n\n\n\n\ntags\n\n\npython,neural-networks,time-series,forecasting\n\n\n\n\n\nstatus\n\n\nin-progress\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnergy Dashboard\n\n\n\nweb\n\nenergy\n\ntools\n\n\n\nReal-time energy consumption monitoring dashboard with anomaly detection capabilities.\n\n\n\nMar 10, 2024\n\n\n\n\n\n\ntags\n\n\nreact,d3.js,python,energy-analytics\n\n\n\n\n\nstatus\n\n\ncompleted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep-NILMtk\n\n\n\nai\n\nresearch\n\nenergy\n\n\n\nA deep learning toolkit for Non-Intrusive Load Monitoring (NILM) using PyTorch\n\n\n\nOct 1, 2023\n\n\n\n\n\n\ntags\n\n\npytorch,deep-learning,energy,time-series\n\n\n\n\n\nstatus\n\n\nactive\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/blog/posts/2025/12/2025-12-1-bayesian-regression..html",
    "href": "pages/blog/posts/2025/12/2025-12-1-bayesian-regression..html",
    "title": "Bayesian Regression: A Real-World Battery Degradation Case Study",
    "section": "",
    "text": "Introduction\nWelcome back to our series on Bayesian Modelling for Industrial Applications. In Part 1, we explored how Bayesian thinking provides a principled framework for decision-making under uncertainty when evidence is limited.\nIn this post, we extend that foundation to continuous prediction problems, offering a practical introduction to Bayesian regression through a real-world case study: predicting lithium-ion battery degradation. If you are new to Bayesian methods, you may want to start with the first post Part 1, which introduces the foundational concepts of Bayesian inference.\n\nüõ†Ô∏è Action: If you prefer to learn by doing, you can reproduce this article using the accompanying resources:\n\n\nRepository: Fork the bayesian-modelling repository and follow the setup instructions in the README.md\nNotebook: Launch 2025-12-1-bayesian-regression.ipynb in the notebook folder to follow along step-by-step.\n\nImagine managing a fleet of electric vehicles. One of your biggest challenges is predicting battery State of Health over time. This is important problem as early predictions inform warranty decisions, maintenance planning, and safety margins. However, industrial systems, like vehicle battery packs, are inherently complex: they are characterized by noisy, non-repeatable measurements due to sensor noise, unit variability, and stochastic physical processes. Traditional deterministic regression methods yield a single best-fit curve and therefore fail to capture the inherent variability and risk present in real-world industrial data which is characterised by:\n\nMeasurement uncertainty inherent in sensors and data acquisition systems\n\nUnit-to-unit variability in components (e.g., subtle differences between nominally identical batteries)\n\nRandom and nonlinear degradation behaviour\n\nLimited early-life observations, a common constraint in industrial testing\n\nBayesian regression addresses this reality by treating model parameters as probability distributions rather than fixed values. Each coefficient represents a range of plausible effects, informed by both prior knowledge and observed data so that uncertainty and risk are explicitly accounted for. This framework is built on three core components:\n\nPriors, which encode existing engineering knowledge or physical constraints\n\nLikelihood, which links the model to noisy real-world measurements\n\nPosterior, which combines prior information and data into an updated belief\n\nThe posterior distribution enables credible intervals, allowing us to quantify how confident we are in both parameter estimates and future predictions‚Äîan essential capability in industrial decision-making.\n\n\nCase Study: Predicting battery degradation\nLithium-ion batteries are critical components in electric vehicles and stationary energy storage systems. Unexpected capacity loss can lead to service interruptions, safety risks, and costly premature replacements.\nThe challenge is predicting future battery health when:\n\nDirect capacity measurements are infrequent and expensive\nEarly-life data is sparse\nDegradation accelerates nonlinearly near end of life\n\nThe goal is not only to predict degradation, but to quantify uncertainty well enough to support maintenance and replacement decisions. We use battery degradation data from the CALCE Battery Research Group at the University of Maryland. The CALCE dataset provides over 1,200 capacity measurements taken at discrete cycle intervals, alongside features like charging and discharge current and voltage. This comprehensive dataset has become a benchmark in battery research, allowing rigorous comparison of degradation models\n\n\nShow the code\nimport arviz as az\nfrom great_tables import GT, loc, md, style\nfrom IPython.display import clear_output\nfrom lets_plot import (\n    LetsPlot,\n    aes,\n    coord_cartesian,\n    facet_wrap,\n    flavor_high_contrast_dark,\n    geom_area,\n    geom_band,\n    geom_density,\n    geom_histogram,\n    geom_line,\n    geom_point,\n    geom_ribbon,\n    geom_vline,\n    gggrid,\n    ggplot,\n    ggsize,\n    guide_legend,\n    guides,\n    labs,\n    layer_tooltips,\n    scale_color_brewer,\n    scale_color_manual,\n    scale_fill_manual,\n    scale_y_continuous,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nfrom sklearn.preprocessing import  StandardScaler\n\naz.style.use(\"arviz-doc\")\n\nfrom bayes.plot.basic_plots import line_plot, modern_theme, pro_colors, scatter_plot\n\nLetsPlot.setup_html(isolated_frame=False, offline=True, no_js=True, show_status=False)\nnp.random.seed(42)\n\n\n\nChoosing the Beta Likelihood\nThe capacity data being modeled, C, represents the battery‚Äôs health and is strictly bounded between $[C_{}, C_{}]. The goal of this analysis is to model the evolution of C_i. In many standard regression approaches, the model‚Äôs likelihood function (which defines the distribution of the noise) is assumed to be Gaussian (Normal). This assumption is fundamentally incompatible with the physical reality of capacity degradation for two key reasons.\n\nGaussian models assume the target variable can take any real value (-\\infty to +\\infty), ignoring the fact that the underlying capacity C cannot fall outside their physical limits (i.e., [C_{\\text{min}}, C_{\\text{max}}] )\nWith enough extrapolation, Gaussian models produce impossible values (e.g.,negative capacity). In safety-critical systems, such predictions are dangerous.\n\nThe Beta distribution is chosen as the likelihood due to its flexible shape, governed by \\alpha and \\beta, which can model a wide range of behaviors. Because battery capacity degradation is continuous and strictly bounded within [C_{\\text{min}}, C_{\\text{max}}], the Beta likelihood provides a natural alternative to Gaussian models that require ad-hoc truncation.\nTo match the Beta distribution‚Äôs [0,1] support, capacity C_i is linearly scaled to \\tilde{C}i = \\frac{C_i - C{\\text{min}}}{C_{\\text{max}} - C_{\\text{min}}} and modeled as \\tilde{C}_i \\sim \\text{Beta}(\\alpha_i, \\beta_i). This guarantees physically plausible predictions while retaining flexibility through the shape parameters.\nwith pm.Model() as battery_model:\n    pm.Beta(\"y_obs\", alpha=alpha, beta=beta_shape, observed=y_data)\n\nParameterization: Mean (\\mu) and Precision (\\phi)\nTo make the parameters intuitive, the Beta distribution is typically reparameterized using the mean (\\mu) and the precision (\\phi).The shape parameters, \\alpha_i and \\beta_i, which define the exact shape of the distribution for a given observation, are calculated directly from the mean \\mu_{i} \\in (0, 1) and the global precision \\phi &gt; 0 such that:\\alpha_i = \\mu_{i} \\cdot \\phi \\quad \\text{and} \\quad \\beta_i = (1 - \\mu_{i}) \\cdot \\phi The precision parameter, \\phi, controls the variance: a large \\phi means the predictions are tightly clustered around the mean \\mu_{i}, indicating low uncertainty (low variance).\nwith pm.Model() as battery_model:\n    alpha = mu_scaled * phi\n    beta_shape = (1 - mu_scaled) * phi\nThe mean parameter \\mu_{i} \\in (0, 1) must be linked to our predictors. Since the mean is bounded by (0, 1), we use the Logit Link Function to map the linear combination of predictors (\\eta_i) to this interval: \\text{logit}(\\mu_{i}) = \\eta_i as such , \\mu_{i} = \\text{logit}^{-1}(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\nwith pm.Model() as battery_model:\n    mu_scaled = pm.Deterministic(\"mu_scaled\", pm.math.sigmoid(logit_mu))\n\nüí° Key Takeaway: The Logit Link function is the mathematical bridge that ensures our mean prediction, \\mu_i, respects the physical boundary of (0, 1) imposed by the Beta distribution\n\n\n\nThe Linear Predictor: Capturing Degradation\nThe core of our predictive power lies in the linear predictor, \\eta_i. It is structured to incorporate both the fundamental, non-linear degradation due to cycling and the linear operational effects from features \\mathbf{x} like voltage and current: \\eta_i = \\underbrace{\\beta_0}_{\\text{Intercept}} + \\underbrace{f(k_i)}_{\\text{Non-linear Decay}} + \\underbrace{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}}_{\\text{Operational Effects}}\nIn this work, the non-linear decay component is modeled as an exponential function: f(k_i) = -A\\cdot(1-e^{-\\lambda \\cdot k_i}) where k_i is the cycle count, \\lambda is the degradation rate, and A&gt;0 is a learnable amplitude parameter controlling the strength of decay. This captures the physical reality that battery capacity degrades rapidly at first and then more slowly over time. where:\nwith pm.Model() as battery_model:\n    degradation = pm.math.exp(-lambda_rate * cycle_data)\n    degradation_term = -degr_amp * (1 - degradation)\n    logit_mu = intercept + degradation_term + pm.math.dot(x_data, beta)\nThe components of \\eta_i are as follows:\n\nIntercept (\\beta_0): The baseline capacity on the logit scale when operational effects are zero and the cycle count (k_i) is zero.\nDegradation Term (e^{-\\lambda \\cdot k_i}): This is the non-linear exponential decay over the cycle count k_i, controlled by the rate \\lambda. This term ensures the capacity prediction naturally trends downward toward zero capacity over time.\nOperational Effects (\\mathbf{x}_{i}^{\\top} \\boldsymbol{\\beta}): This is a standard linear combination, where \\boldsymbol{\\beta} is the vector of coefficients for the standardized operational features \\mathbf{x}_{i}.\n\nThis models how factors like maximum temperature accelerate or slow down the degradation.\n\nüß† Self-Test: You are modeling \\text{SoH}, which must stay in [0, 1]. Your linear predictor, \\eta_i = \\beta_0 + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}, can produce values ranging from -\\infty to +\\infty.What would happen if you skipped the Logit Link Function and simply set \\mu_i = \\eta_i? Why is the Logit Link function mandatory for the Beta regression model?\n\n\n\n\nEncoding Knowledge with Priors\nIn Bayesian modeling, defining priors is a critical step. This step allows domain knowledge accumulated from battery engineering to be embedded directly into the model, ensuring that predictions remain physically plausible even when data is sparse. A prior distribution is assigned to every unknown parameter (\\beta_0, \\boldsymbol{\\beta}, \\lambda, \\phi). These priors act as soft constraints, preventing the model from learning extreme or non-physical relationships.\nThe Intercept (\\beta_0)\nThe Intercept \\beta_0 represents the initial capacity of the battery fleet on the logit scale. The orange curve in the figure below represents the selected informative prior, \\text{Normal}(\\mu_{\\text{logit\\_start}}, 0.5^2). A standard deviation of \\sigma = 0.5 is chosen to balance prior knowledge (centering at \\mu_{\\text{logit\\_start}}) with sufficient uncertainty to allow the observed data to meaningfully influence the final estimate.\nwith pm.Model() as battery_model:\n    eps=1e-8\n    initial_logit_capacity_mean = -np.log(1-eps)\n    intercept = pm.Normal(\"intercept\", mu=initial_logit_capacity_mean, sigma=0.5)\n\n\nShow the code\nfrom bayes.plot.distribution import plot_density\n\n\n\n\nShow the code\nn = 1000\ns1 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.1), n)\ns2 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.2), n)\ns3 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.5), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=0.28, œÉ=0.1)\", \"(Œº=0.28, œÉ=0.2)\", \"(Œº=0.28, œÉ=0.5)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"Normal Distributions Intercept Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -2\n              \n            \n          \n          \n            \n            \n            \n              \n                -1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                -1\n              \n            \n          \n          \n            \n            \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Normal Distributions Intercept Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.2)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.5)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThe narrower blue (\\sigma = 0.1) and green (\\sigma = 0.2) curves represent highly concentrated priors that would strongly restrict the posterior estimates. The wider \\sigma = 0.5 (orange) distribution corresponds to a more conservative informative prior, granting the initial capacity estimate \\beta_0 a reasonable degree of uncertainty.\nOperational Effects (\\boldsymbol{\\beta})\nThe vector of coefficients \\boldsymbol{\\beta} controls the influence of operational features on capacity fade. Engineering knowledge suggests that, unless a feature is extreme, its immediate effect on capacity should be subtle, as the overall degradation process is primarily driven by cycle count.\n    with pm.Model() as battery_model:\n    beta = pm.Normal(\"beta\", mu=0, sigma=0.2, shape=n_features)\n\n\nShow the code\nfrom bayes.plot.distribution import plot_density\n\n\n\n\nShow the code\n\ns1 = pm.draw(pm.Normal.dist(mu=0, sigma=0.1), n)\ns2 = pm.draw(pm.Normal.dist(mu=0, sigma=0.2), n)\ns3 = pm.draw(pm.Normal.dist(mu=0, sigma=1.0), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=0, œÉ=0.1)\", \"(Œº=0, œÉ=0.2)\", \"(Œº=0, œÉ=1.0)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"Normal Distributions Beta Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -4\n              \n            \n          \n          \n            \n            \n            \n              \n                -3\n              \n            \n          \n          \n            \n            \n            \n              \n                -2\n              \n            \n          \n          \n            \n            \n            \n              \n                -1\n              \n            \n          \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Normal Distributions Beta Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=0.2)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=1.0)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nAs shown in the figure above, a tight informative prior, \\text{Normal}(0, 0.2^2), is used for \\boldsymbol{\\beta}. Centering this prior at zero reflects the assumption that, on average, operational features have no effect, while the small standard deviation (0.2) requires strong evidence from the data before attributing a large effect to any single feature. This constraint prevents non-physical, abrupt changes in capacity predictions. In contrast, a broader prior such as \\text{Normal}(0, 1.0^2) (orange curve) allows extreme effects that are considered non-physical.\nDegradation rate \\lambda\nThe degradation rate \\lambda governs the exponential decay term e^{-\\lambda k_i}. Since degradation must always occur and capacity cannot increase indefinitely, it is necessary to enforce \\lambda &gt; 0. Accordingly, a Log-Normal prior, \\text{LogNormal}(\\ln(0.005), 0.5^2), is used for \\lambda.\nwith pm.Model() as battery_model:\n    lambda_rate = pm.Lognormal(\"lambda_rate\", mu=np.log(0.01), sigma=0.5)\n\nüß† Self-Test: Recall that we set the prior for the fade rate \\lambda as \\text{LogNormal}(\\ln(0.01), 0.5^2) (where \\sigma = 0.5). What practical problem would arise if an engineer, overly confident in their historical knowledge, reset the prior to \\text{LogNormal}(\\ln(0.01), 0.1^2) (where \\sigma = 0.1)?\n\n\n\nShow the code\n\ns1 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=0.1), n)\ns2 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=0.5), n)\ns3 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=1.0), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=In(0.005), œÉ=0.1)\", \"(Œº=In(0.005), œÉ=0.5)\", \"(Œº=In(0.005), œÉ=1.0)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"LogNormal Distributions Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.05\n              \n            \n          \n          \n            \n            \n            \n              \n                0.1\n              \n            \n          \n          \n            \n            \n            \n              \n                0.15\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.25\n              \n            \n          \n          \n            \n            \n            \n              \n                0.3\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                200\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n          \n            \n              \n                600\n              \n            \n          \n          \n            \n              \n                800\n              \n            \n          \n        \n      \n    \n    \n      \n        LogNormal Distributions Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=0.5)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=1.0)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThis weakly informative prior centers the expected degradation rate around \\mathbf{0.5\\%}, while the spread \\sigma = 0.5 (green/teal curve) is sufficiently wide to accommodate realistic fleet-level variability. At the same time, it remains substantially tighter than \\sigma = 1.0 (orange curve), thereby avoiding non-physical probability mass assigned to unrealistically large degradation rates. This distribution reflects a conservative estimate of uncertainty, allowing greater variation in degradation behavior than a tighter prior (e.g., \\sigma = 0.1) would permit, while still preventing implausible rates.\nDegradation Amplitude (A)\nThe parameter degr_amp (A) controls the overall amplitude of the degradation component. Since this amplitude must be non-negative, a Half-Normal distribution is used, which has support only on positive values. The scale parameter \\sigma determines the strength of regularization.\n   with pm.Model() as battery_model:\n    degr_amp = pm.HalfNormal(\"degr_amp\", sigma=0.1)\n\n\nShow the code\n\ns1 = pm.draw(pm.HalfNormal.dist(sigma=0.1), n)\ns2 = pm.draw(pm.HalfNormal.dist(sigma=0.2), n)\ns3 = pm.draw(pm.HalfNormal.dist(sigma=0.5), n)\n\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"œÉ=0.1\", \"œÉ=0.2\", \"œÉ=0.5\"], n),\n    }\n)\nplot=plot_density(df, title=\"Gamma Distributions Phi Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n          \n            \n              \n                6\n              \n            \n          \n        \n      \n    \n    \n      \n        Gamma Distributions Phi Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.1\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.2\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nAs shown in the figure above, \\text{HalfNormal}(\\sigma = 0.1) strongly concentrates probability mass near zero, requiring substantial evidence before attributing a large degradation amplitude. In contrast, broader priors such as \\text{HalfNormal}(\\sigma = 0.5) place non-negligible probability on large, non-subtle amplitudes (up to approximately 1.0), increasing the risk of overfitting by allowing the model to explain noise through the amplitude term.\nPrecision Parameter (\\phi)\nThe precision parameter \\phi controls the variance of the Beta likelihood and represents the expected level of noise in the \\text{SoH} measurements. Accordingly, a highly informative Gamma prior, \\text{Gamma}(100, 2), is assigned to \\phi.\nwith pm.Model() as battery_model:\n    phi = pm.Gamma(\"phi\", alpha=100, beta=2.0)\nThis prior is centered at \\mathbb{E}[\\phi] = \\alpha / \\beta = 50 with a relatively small standard deviation (\\sigma_{\\phi} = 5.0), indicating high confidence in this expectation. This choice encodes the belief that sensor noise is low (\\sigma_{\\text{noise}} \\approx 0.14), reflecting the physical reality of precise laboratory-grade measurements.\n\n\nShow the code\n\ns1 = pm.draw(pm.Gamma.dist(alpha=10, beta=1), n)\ns2 = pm.draw(pm.Gamma.dist(alpha=50, beta=5), n)\ns3 = pm.draw(pm.Gamma.dist(alpha=100, beta=2), n)\n\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"Gamma(Œ±=10, Œ≤=1)\", \"Gamma(Œ±=50, Œ≤=5)\", \"Gamma(Œ±=100, Œ≤=2)\"], n),\n    }\n)\nplot=plot_density(df, title=\"Gamma Distributions Phi Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                10\n              \n            \n          \n          \n            \n            \n            \n              \n                20\n              \n            \n          \n          \n            \n            \n            \n              \n                30\n              \n            \n          \n          \n            \n            \n            \n              \n                40\n              \n            \n          \n          \n            \n            \n            \n              \n                50\n              \n            \n          \n          \n            \n            \n            \n              \n                60\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.05\n              \n            \n          \n          \n            \n              \n                0.1\n              \n            \n          \n          \n            \n              \n                0.15\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.25\n              \n            \n          \n        \n      \n    \n    \n      \n        Gamma Distributions Phi Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=10, Œ≤=1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=50, Œ≤=5)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=100, Œ≤=2)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nFrom the figure above, it is evident that \\text{Gamma}(\\alpha = 100, \\beta = 2.0) (orange curve) provides a strong belief in high precision. In contrast, \\text{Gamma}(\\alpha = 10, \\beta = 1.0) yields a lower expected precision with greater spread, allowing excessive uncertainty and risking a flat, unphysical prior predictive distribution. Alternative Gamma priors with the same expected precision but larger variance similarly underestimate the precision of modern sensors.\nThe complete model now combines all these components:\n\n\nShow the code\ndef beta_regression_model(\n    data: pd.DataFrame,\n    features: list[str],\n    target: str = \"capacity\",\n    scaler: StandardScaler | None = None,\n    lower_bound: float = 0.2,\n    upper_bound: float = 1.3,\n    eps: float = 1e-8,\n) -&gt; tuple[pm.Model, StandardScaler]:\n    \"\"\"Beta regression model for bounded battery capacity data using PyMC.\n\n    Capacity (SoH) is scaled to the (0, 1) interval for the Beta distribution.\n\n    Args:\n        data: DataFrame containing 'capacity', 'cycle', and feature columns.\n        features: List of column names used as predictors (X variables).\n        target: Name of the capacity column.\n        scaler: Pre-fitted StandardScaler object, or None to fit a new one.\n        lower_bound: Physical lower bound for capacity (for scaling).\n        upper_bound: Physical upper bound for capacity (for scaling).\n        eps: Small value to avoid boundary issues in Beta distribution.\n\n    Returns:\n        A tuple containing the PyMC model and the fitted/provided StandardScaler.\n    \"\"\"\n    # 1. Prepare Features (X)\n    if scaler is None:\n        scaler = StandardScaler()\n        x_scaled = scaler.fit_transform(data[features])\n    else:\n        x_scaled = scaler.transform(data[features])\n\n    # 2. Prepare Targets (Y)\n    y = data[target].values.astype(np.float64)\n    cycles = data[\"cycle\"].values.astype(np.float64)\n    n_features = len(features)\n\n    # Transform y to (0,1) interval and clip to avoid boundaries (0 or 1)\n    y_scaled = (y - lower_bound) / (upper_bound - lower_bound)\n    y_scaled = np.clip(y_scaled, eps, 1 - eps)\n\n    with pm.Model() as model:\n        # Data Containers\n        x_data = pm.Data(\"x_data\", x_scaled)\n        cycle_data = pm.Data(\"cycle_data\", cycles)\n        y_data = pm.Data(\"y_data\", y_scaled)\n\n        # Priors\n        initial_logit_capacity_mean = -np.log(1 - 1e-6)\n        intercept = pm.Normal(\"intercept\", mu=initial_logit_capacity_mean, sigma=0.5)\n        lambda_rate = pm.Lognormal(\"lambda_rate\", mu=np.log(0.005), sigma=0.5)\n        beta = pm.Normal(\"beta\", mu=0, sigma=0.2, shape=n_features)\n        phi = pm.Gamma(\"phi\", alpha=100, beta=2.0)\n        degr_amp = pm.HalfNormal(\"degr_amp\", sigma=0.1)\n\n        # Linear predictor (eta) on logit scale\n        degradation = pm.math.exp(-lambda_rate * cycle_data)\n        degradation_term = -degr_amp * (1 - degradation)\n        logit_mu = intercept + degradation_term + pm.math.dot(x_data, beta)\n\n        # Convert to probability scale (0,1)\n        mu_scaled = pm.Deterministic(\"mu_scaled\", pm.math.invlogit(logit_mu))\n\n        # Beta likelihood\n        alpha = mu_scaled * phi\n        beta_shape = (1 - mu_scaled) * phi\n        pm.Beta(\"y_obs\", alpha=alpha, beta=beta_shape, observed=y_data)\n\n        # Transform mu back to original scale\n        mu_original = pm.Deterministic(\"mu_original\", mu_scaled * (upper_bound - lower_bound) + lower_bound)\n\n        pm.Deterministic(\"capacity_pred\", mu_original)\n        pm.Deterministic(\"feature_effects\", beta)\n\n    return model, scaler\n\n\n\n\n\nTranslating raw battery data to diagnostics features\nIn the preceding sections, the output side of the Bayesian model was rigorously defined, including the Beta likelihood, the Logit link function, and physics-informed priors for the parameters (\\beta_0, \\boldsymbol{\\beta}, \\lambda, \\phi). However, the quality of the resulting predictions depends critically on the quality of the input features (\\mathbf{x}) that drive the degradation term (\\eta_i = \\dots + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}).\nRaw capacity measurement curves are noisy and variable. Therefore, before proceeding to Bayesian sampling, it is necessary to dedicate a structured process to translating real-world operational data into robust, physically meaningful diagnostic features.\nThis motivates the crucial step of feature engineering.\n\nData alignment and cleaning\nBefore extracting diagnostic features, a uniform time base must be established, as the formulas used for feature extraction require comparable voltage and current values across cycles.\n\nCycle Alignment (Standardization): Linear interpolation is used to resample all voltage and current time-series arrays to a uniform length (e.g., 500 points). This standardization enables direct cycle-to-cycle comparison, as illustrated by the transition from the raw data (Figures 1 and 2) to the interpolated curves (Figures 3 and 4).\n\n\n\n\n\nFigure 1: Charging Voltage curve at the beginning of life\n\n\n\n\n\nFigure 2: Discharge Voltage curve at the beginning of life\n\n\n\nAs shown in Figures 1 and 2, voltage curves differ in length due to variations in charge and discharge durations. Linear interpolation standardizes these curves to a fixed length (e.g., 500 points), enabling direct comparison across cycles, as illustrated in Figures 3 and 4.\n\n\n\n\nFigure 3: Inteporated Charging Voltage curve at the beginning of life\n\n\n\n\n\nFigure 4: Inteporated Discharge Voltage curve at the beginning of life\n\n\n\n\nData Filtering: Cycles exhibiting non-meaningful behavior (e.g., flat voltage profiles, excessive noise, or unrealistic starting or peak voltages) are removed to ensure that all inputs correspond to valid charging or discharging events.\n\n\n\nDiagnostic Feature Extraction\nWith aligned and cleaned curves, it is now possible to reliably extract cycle-specific diagnostic features that quantify the battery‚Äôs underlying physical degradation processes. These features are sensitive to aging mechanisms such as active material loss and internal resistance growth\n\nüß† Reflection: We chose to derive these physically meaningful features instead of feeding the entire, aligned time-series data (Figures 3 & 4). Why are these manually engineered features often preferred in industrial applications? Consider the trade-offs in model complexity, training speed, and the crucial interpretability of the final Bayesian coefficients (\\beta).\n\n\n\nShow the code\n\n\n# Fix the typo and create data\ndata = pd.DataFrame({\n    \"Diagnostic Feature\": [\n        \"Voltage Gap\",\n        \"Voltage Hysteresis\", \n        \"IC Peak Metrics\",\n        \"Hysteresis Proxy Resistance\"\n    ],\n    \"Formula\": [\n        \"ŒîVÃÑ = VÃÑ_c - VÃÑ_d\",\n        \"ŒîV(x) = V_c(x) - V_d(x)\",\n        \"IC = dQ/dV\",\n        \"R_proxy ‚àù ŒîV(x)/I_diff\"\n    ],\n    \"Physical Meaning\": [\n        \"Average polarization; quantifies internal losses\",\n        \"Loss mechanisms at specific state-of-charge\",\n        \"Phase transitions; indicates active material loss\",\n        \"Proxy for internal resistance growth\"\n    ]\n})\n\n\n# Create publication-quality table\ntable = (\n    GT(data)\n    .tab_header(\n        title=md(\"**Table 1: Battery Degradation Diagnostic Features**\"),\n        subtitle=\"Mathematical definitions and physical interpretations of key battery health indicators\"\n    )\n    .cols_label(\n        **{\n            \"Diagnostic Feature\": md(\"**Diagnostic Feature**\"),\n            \"Formula\": md(\"**Formula**\"),\n            \"Physical Meaning\": md(\"**Physical Meaning**\")\n        }\n    )\n    .tab_options(\n        table_width=\"100%\",\n        container_width=\"100%\",\n        table_font_size=\"14px\",\n        heading_title_font_size=\"18px\",\n        heading_subtitle_font_size=\"14px\",\n        column_labels_border_bottom_style=\"solid\",\n        column_labels_border_bottom_width=\"3px\",\n        column_labels_border_bottom_color=\"#3498db\",\n        table_body_border_bottom_style=\"solid\",\n        table_body_border_bottom_width=\"1px\",\n        table_body_border_bottom_color=\"#dee2e6\"\n    )\n    .tab_source_note(\n        source_note=\"Formulas assume constant temperature and current rates unless otherwise specified.\"\n    )\n)\n\n# Display\ntable.show()\n\n\n\n\n\n\n\n\nTable 1: Battery Degradation Diagnostic Features\n\n\nMathematical definitions and physical interpretations of key battery health indicators\n\n\nDiagnostic Feature\nFormula\nPhysical Meaning\n\n\n\n\nVoltage Gap\nŒîVÃÑ = VÃÑ_c - VÃÑ_d\nAverage polarization; quantifies internal losses\n\n\nVoltage Hysteresis\nŒîV(x) = V_c(x) - V_d(x)\nLoss mechanisms at specific state-of-charge\n\n\nIC Peak Metrics\nIC = dQ/dV\nPhase transitions; indicates active material loss\n\n\nHysteresis Proxy Resistance\nR_proxy ‚àù ŒîV(x)/I_diff\nProxy for internal resistance growth\n\n\n\nFormulas assume constant temperature and current rates unless otherwise specified.\n\n\n\n\n\n\n\n\n\n\n\nStatistical feature aggregation\nThe diagnostic signals derived above (e.g., incremental capacity curves) remain high-resolution time- or cycle-series data. To produce robust, concise, and comparable inputs for the Bayesian regression model, a final aggregation step is performed by extracting statistical moments from each diagnostic signal s(x). This aggregation reduces hundreds of data points per cycle into a small number of highly informative scalar features.\n\n\nShow the code\ndata = pd.DataFrame({\n    \"Statistical Feature\": [\n        \"Mean\",\n        \"Standard Deviation\", \n        \"Skewness\",\n        \"Kurtosis\",\n        \"RMS (Root-Mean-Square)\",\n        \"Entropy\",\n        \"Crest Factor\",\n        \"AUC (Area Under the Curve)\"\n    ],\n    \"Role in Degradation Modeling\": [\n        \"Captures the overall trend or shift of the diagnostic signal.\",\n        \"Measures variability and cycle-to-cycle noise.\",\n        \"Indicates asymmetry or bias in the signal distribution.\",\n        \"Quantifies the presence of extreme values or anomalies.\",\n        \"Represents the overall magnitude and stress level of the signal.\",\n        \"Measures irregularity or disorder, often increasing with non-uniform degradation.\",\n        \"Compares peak magnitude to average signal level, highlighting abnormal peaks.\",\n        \"Captures cumulative effects such as total energy loss or degradation trends.\"\n    ]\n})\n\n\n\n\nShow the code\ntable = (\n    GT(data)\n    .tab_header(\n        title=md(\"Statistical Features for Battery Degradation Modeling\"),\n        subtitle=\"Key signal processing metrics used to quantify degradation patterns\"\n    )\n    .cols_label(\n        **{\n            \"Statistical Feature\": md(\"**Statistical Feature**\"),\n            \"Role in Degradation Modeling\": md(\"**Role in Degradation Modeling**\")\n        }\n    )\n    .tab_options(\n        table_width=\"100%\",\n        container_width=\"100%\",\n        table_font_size=\"14px\",\n        heading_title_font_size=\"18px\",\n        heading_subtitle_font_size=\"14px\",\n        column_labels_border_bottom_style=\"solid\",\n        column_labels_border_bottom_width=\"3px\",\n        column_labels_border_bottom_color=\"#3498db\",\n        table_body_border_bottom_style=\"solid\",\n        table_body_border_bottom_width=\"1px\",\n        table_body_border_bottom_color=\"#dee2e6\"\n    )\n    .tab_source_note(\n        source_note=\"Bayesian Modelling|Anthony Faustine@ 2025\"\n    )\n)\n\ntable.show()\n\n\n\n\n\n\n\n\nStatistical Features for Battery Degradation Modeling\n\n\nKey signal processing metrics used to quantify degradation patterns\n\n\nStatistical Feature\nRole in Degradation Modeling\n\n\n\n\nMean\nCaptures the overall trend or shift of the diagnostic signal.\n\n\nStandard Deviation\nMeasures variability and cycle-to-cycle noise.\n\n\nSkewness\nIndicates asymmetry or bias in the signal distribution.\n\n\nKurtosis\nQuantifies the presence of extreme values or anomalies.\n\n\nRMS (Root-Mean-Square)\nRepresents the overall magnitude and stress level of the signal.\n\n\nEntropy\nMeasures irregularity or disorder, often increasing with non-uniform degradation.\n\n\nCrest Factor\nCompares peak magnitude to average signal level, highlighting abnormal peaks.\n\n\nAUC (Area Under the Curve)\nCaptures cumulative effects such as total energy loss or degradation trends.\n\n\n\nBayesian Modelling|Anthony Faustine@ 2025\n\n\n\n\n\n\n\n\n\nThese statistical summaries form the input vector \\mathbf{x} in the linear predictor \\eta_i = \\dots + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}. Such summaries of early-cycle behavior often preserve key degradation signatures while significantly reducing model complexity.\n\n\nFeature selection\nAfter extracting a broad set of diagnostic and statistical features, feature selection is required. Using all available features can lead to overfitting, increased model complexity, and multicollinearity, which compromises interpretability of the Bayesian coefficients (\\boldsymbol{\\beta}). The final four features selected for regression (\\mathbf{x}) are:\n\ncharge_current_auc\ncharge_current_mean\ndischarge_voltage_auc\ndischarge_voltage_crest\n\n\n\nLoad pre-processed CALCE dataset\nThe raw CALCE dataset is a widely used public resource in battery prognostics and can be downloaded via the CALCE dataset link.\nFor this notebook, however, we use pre-processed data that has been cleaned and formatted. This pre-processing reuses the techniques and codes originally published in this paper Ref. Using the cleaned data allows us to focus immediately on the Bayesian modeling aspects without the overhead of complex data preparation\n\n\nShow the code\nFIGSHARE_DOWNLOAD_URL = \"https://ndownloader.figshare.com/files/59415941\"\nfeatures = [\"charge_current_auc\", \"charge_current_mean\", \"discharge_voltage_crest\", \"discharge_voltage_auc\"]\ntarget = \"capacity\"\ndata = pd.read_parquet(FIGSHARE_DOWNLOAD_URL, engine=\"pyarrow\")\nupper_bound, lower_bound = data[target].max(), data[target].min()\ndf = data[data.CellType == \"CS2\"].copy()\ntest_df = df[df.BatteryID != \"CALCE_CS2_38\"]\ntrain_df = df[df.BatteryID == \"CALCE_CS2_38\"]\n\n\n\n\nShow the code\nplots = []\nfor feature_name in features:\n    plot = scatter_plot(train_df, y_col=feature_name)\n    plots.append(plot + labs(title=feature_name) + modern_theme(font_size=9))\n\nplot=gggrid(plots, ncol=2) + ggsize(650, 450)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  -0.1\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          charge_current_auc\n        \n      \n      \n        \n          charge_current_auc\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  -0.1\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          charge_current_mean\n        \n      \n      \n        \n          charge_current_mean\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  1.1\n                \n              \n            \n            \n              \n                \n                  1.15\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          discharge_voltage_crest\n        \n      \n      \n        \n          discharge_voltage_crest\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  0.54\n                \n              \n            \n            \n              \n                \n                  0.56\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          discharge_voltage_auc\n        \n      \n      \n        \n          discharge_voltage_auc\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n\n\n\nThe figure below plots the four selected features against cycle number for a representative battery. These plots empirically validate the selection process, as all four features exhibit clear, monotonic changes with cycling and, critically, show a distinct shift or acceleration in slope as the battery enters the failure state (\\text{SoH} \\le 80\\%, shown in green). This strong visual correlation provides high confidence that these inputs will effectively drive the degradation component of our Bayesian model.\n\n\n\nBayesian model building\nBefore constructing the Bayesian Beta regression model, it is necessary to define a rigorous evaluation strategy. The central question addressed here is whether a model trained on data from a single battery can successfully generalize to other batteries whose degradation trajectories were not observed during training.\nThis setting reflects a common real-world scenario in which detailed historical data may be available for only a limited number of prototype units, while the deployed model must operate reliably across an entire manufacturing batch.\nTo ensure a fair and controlled evaluation, all batteries considered in this study are restricted to a single cell chemistry type (‚ÄúCS2‚Äù). By holding the underlying electrochemical properties constant, the analysis isolates unit-to-unit variability rather than confounding the results with chemistry-dependent effects.\nA one-shot generalization split is employed. A single representative battery (CALCE_CS2_38) is designated as the training set (train_df). The model learns the degradation rate (\\lambda) and operational sensitivities (\\boldsymbol{\\beta}) exclusively from this unit‚Äôs historical data. All remaining batteries of the same chemistry are assigned to the test set (test_df). Model performance is therefore evaluated based on its ability to predict capacity fade for previously unseen batteries using only the generalizable parameters inferred from the training unit.\nWith the input data rigorously cleaned, aligned, scaled, and reduced to the four most informative operational features (\\mathbf{x}), the Bayesian regression model can now be implemented and fitted. As defined in the Beta Likelihood and Priors subsection, the model is specified as a Bayesian Beta regression with a logit link function, enabling the modeling of bounded battery capacity (\\tilde{C}).\n\n\nShow the code\n# from bayes.regression.beta_degradation import beta_regression_model\nmodel, scaler = beta_regression_model(\n    train_df, features, target=target, upper_bound=upper_bound, lower_bound=lower_bound\n)\nmodel\n\n\n\n            \\begin{array}{rcl}\n            \\text{intercept} &\\sim & \\operatorname{Normal}(1e-06,~0.5)\\\\\\text{lambda\\_rate} &\\sim & \\operatorname{LogNormal}(-5.3,~0.5)\\\\\\text{beta} &\\sim & \\operatorname{Normal}(0,~0.2)\\\\\\text{phi} &\\sim & \\operatorname{Gamma}(100,~f())\\\\\\text{degr\\_amp} &\\sim & \\operatorname{HalfNormal}(0,~0.1)\\\\\\text{mu\\_scaled} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{mu\\_original} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{capacity\\_pred} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{feature\\_effects} &\\sim & \\operatorname{Deterministic}(f(\\text{beta}))\\\\\\text{y\\_obs} &\\sim & \\operatorname{Beta}(f(\\text{phi},~\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}),~f(\\text{phi},~\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\n            \\end{array}\n            \n\n\n\nPrior Predictive Check\nFollowing standard Bayesian practice, model validation begins with a Prior Predictive Check (PPC). The PPC involves simulating data from the model using only the prior distributions, without conditioning on any observed measurements. This procedure serves as a critical sanity check, verifying that the encoded engineering knowledge produces physically plausible behavior.\n\n\nShow the code\nwith model:\n    prior_pred = pm.sample_prior_predictive(samples=1000)\n\nfig, ax = plt.subplots(figsize=(4, 1.8))\naz.plot_ppc(prior_pred, group=\"prior\", ax=ax)\nplt.xlabel(\"Capacity\")\nplt.ylabel(\"Density\");\n\n\n\n\n\n\n\n\n\nThe figure below, compare the predicted prior distribution (green line) against the observed data (blue line). This plot is essential for validating that our model‚Äôs structural assumptions align with physical reality\n\n\nShow the code\ny_prior = prior_pred.prior[\"capacity_pred\"].stack(sample=[\"chain\", \"draw\"]).values\ny_obs = train_df[target].values\npost_mean = y_prior.mean()\nn_draws = 500\nrng = np.random.default_rng(42)\ndraw_idx = rng.choice(y_prior.shape[0], size=n_draws, replace=False)\ny_prior_subset = y_prior[:, draw_idx].flatten()\ndf_prior = pd.DataFrame(\n    {\n        \"SoH\": np.concatenate([y_obs, y_prior_subset]),\n        \"type\": [\"observed\"] * len(y_obs) + [\"prior\"] * len(y_prior_subset),\n    }\n)\nplot=plot_density(\n    df_prior,\n    x_col=\"SoH\",\n    color_col=\"type\",\n    x_label=\"Capacity\",\n    fig_size=(500, 400),\n    title=\"Prior comparison\",\n    subtitle=\"Prior Predictive Check\",\n)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Prior comparison\n      \n    \n    \n      \n        Prior Predictive Check\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Capacity\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                observed\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                prior\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nFrom the two figures above, the PPC confirms the structural validity of the model:\n\nHigh-Confidence Initial Capacity: The predicted capacity distribution exhibits a dominant peak near \\mu \\approx 1.0, reflecting the highly informative precision prior \\phi \\sim \\text{Gamma}(100, 2.0) (mean \\phi = 50). This enforces a strong prior belief in low sensor noise and high initial measurement confidence.\nRealistic Degradation Envelope: The prior predictive distribution remains tightly constrained across the capacity range. This behavior is driven by the informative degradation-rate prior on \\lambda, which minimizes the probability of immediate or catastrophic capacity loss and enforces physically plausible degradation trajectories.\nAcknowledgment of Failure Modes: While constrained, the prior allocates non-negligible probability mass to lower capacity regions (e.g., \\tilde{C} \\approx 0.4‚Äì0.7). This reflects uncertainty in the degradation amplitude and rate parameters, allowing for degradation and failure scenarios without overstating their likelihood.\n\nThe PPC demonstrates that the model respects physical bounds, reflects realistic degradation behavior, and balances strong prior knowledge with controlled uncertainty. The model is therefore suitable for posterior inference.\n\n\n\nRunning inference\nWith the model fully specified, priors validated through the PPC, and input features prepared, posterior inference is performed using Markov Chain Monte Carlo (MCMC) sampling. This step approximates the posterior distribution by updating prior beliefs using the observed data, forming the core of Bayesian inference.\n\n\nShow the code\nwith model:\n    idata = pm.sample(2000, tune=2000, target_accept=0.95, random_seed=42)\nclear_output()\n\n\nAs discussed in Part 1, the MCMC process uses the No-U-Turn Sampler (NUTS) to explore the parameter space. The primary arguments guide this process:\n\ntune=2000: Specifies 2000 initial samples that are used solely to adapt the sampler‚Äôs step size and are then discarded. A high tuning value is crucial for complex, highly curved posteriors (like those involving Beta distributions) to ensure stable exploration.\ndraws=2000: Specifies 2000 final samples kept from the chain. These collected samples form the final Posterior Distribution for every model parameter (\\lambda, \\beta, \\phi).\ntarget_accept=0.95: Forces the sampler to take smaller, more cautious steps. This high acceptance rate is necessary to avoid divergences in challenging models, ensuring a high-quality, accurate representation of the posterior distribution, though it increases computation time.\n\nThe resulting idata object now contains thousands of samples for every single model parameter, representing our comprehensive, uncertainty-quantified solution. The next step is to ensure these samples are reliable\n\nModel diagnostics\nAfter sampling, convergence diagnostics are evaluated to ensure the reliability of posterior estimates. The validity of all subsequent inferences depends on whether the Markov chains have adequately explored the parameter space.\n\n\nShow the code\nvars = [\"beta\", \"intercept\", \"lambda_rate\", \"phi\", \"degr_amp\"]\ndata_summary = az.summary(idata, var_names=vars, kind=\"diagnostics\")[[\"ess_bulk\", \"ess_tail\", \"r_hat\"]]\nGT(data_summary.reset_index()).tab_header(title=\"\", subtitle=\"Diagnostics Summary\").cols_label(\n    {\n        \"ess_bulk\": \"ESS Bulk\",\n        \"ess_tail\": \"ESS Tail.\",\n        \"r_hat\": \"R-hat\",\n    }\n)\n\n\n\n\n\n\n\n\n\n\n\nDiagnostics Summary\n\n\nindex\nESS Bulk\nESS Tail.\nR-hat\n\n\n\n\nbeta[0]\n4381.0\n4866.0\n1.0\n\n\nbeta[1]\n4268.0\n4571.0\n1.0\n\n\nbeta[2]\n4218.0\n4234.0\n1.0\n\n\nbeta[3]\n4394.0\n4732.0\n1.0\n\n\nintercept\n4426.0\n4803.0\n1.0\n\n\nlambda_rate\n5309.0\n4684.0\n1.0\n\n\nphi\n7368.0\n5104.0\n1.0\n\n\ndegr_amp\n3758.0\n4646.0\n1.0\n\n\n\n\n\n\n\n\nTwo primary diagnostics are considered:\n\n\\hat{R} (Gelman‚ÄìRubin statistic): All parameters exhibit \\hat{R} = 1.0, indicating excellent chain mixing and agreement across chains.\nEffective Sample Size (ESS): ESS values exceed 400 for all parameters (ranging from approximately 4,200 to 7,300), confirming that a sufficient number of independent samples were obtained for stable estimation of posterior means and credible intervals.\n\nThese diagnostics collectively indicate successful convergence and robust posterior sampling.\n\n\nAnalysing the posterior distribution\nWith convergence confirmed, the sampled chains provide a reliable approximation of the posterior distribution. The marginal posterior densities and corresponding trace plots for the core model parameters are examined to quantify degradation dynamics and assess the influence of operational features.\nThis analysis enables principled uncertainty quantification of degradation rates and feature effects, supporting interpretable and decision-relevant predictions for battery health forecasting.\n\n\nShow the code\naz.plot_trace(idata, var_names=vars, compact=True);\n\n\n\n\n\n\n\n\n\nThe analyze_parameter function below acts as a post-processing utility dedicated to generating publication-ready summary tables from the output of the MCMC sampling.\n\n\nShow the code\ndef analyze_parameter(\n    idata,\n    parameter: str,\n    features: list[str] | None = None,\n    hdi_prob: float = 0.95,\n    title: str = \"Parameter Summary\",\n    subtitle: str | None = None,\n) -&gt; GT:\n    \"\"\"Generates a formatted summary table for a single parameter using Great Tables.\n\n    This function extracts posterior summary statistics from ArviZ InferenceData and\n    returns a beautifully styled table suitable for reports, notebooks, or publications.\n\n    Args:\n        idata: ArviZ InferenceData object containing posterior samples.\n        parameter: Name of the parameter to summarize (e.g., \"beta\", \"alpha\", \"sigma\").\n        features: List of feature names to label rows. Required and used only when\n            ``parameter == \"beta\"``. Length must match the number of coefficients.\n        hdi_prob: Highest density interval probability (default: 0.95).\n        title: Main title for the table.\n        subtitle: Optional subtitle. If None and parameter is \"beta\", defaults to\n            \"Beta coefficient analysis\".\n\n    Returns:\n        A Great Tables (GT) object ready for display or further customization.\n\n    Raises:\n        ValueError: If ``features`` is provided for non-beta parameters or has wrong length.\n\n    Example:\n        &gt;&gt;&gt; gt = analyze_parameter(idata, \"beta\", features=X.columns.tolist())\n        &gt;&gt;&gt; gt  # displays nicely in Jupyter\n    \"\"\"\n    if features is not None and parameter != \"beta\":\n        raise ValueError(\"`features` should only be provided when parameter == 'beta'\")\n\n    # Get summary statistics from ArviZ\n    summary_df = az.summary(\n        idata,\n        var_names=[parameter],\n        hdi_prob=hdi_prob,\n        kind=\"stats\",\n        fmt=\"wide\",\n    ).reset_index(names=\"feature\")\n\n    # Assign meaningful feature names for beta coefficients\n    if parameter == \"beta\":\n        if features is None:\n            raise ValueError(\"`features` must be provided when analyzing 'beta' parameter\")\n        if len(features) != len(summary_df):\n            raise ValueError(\n                f\"Length of features ({len(features)}) must equal number of beta coefficients ({len(summary_df)})\"\n            )\n        summary_df[\"feature\"] = features\n\n    conditions = [\n        summary_df[\"hdi_2.5%\"] &gt; 0,  # Entire interval is positive\n        summary_df[\"hdi_97.5%\"] &lt; 0,  # Entire interval is negative\n    ]\n    choices = [\"Positive\", \"Negative\"]\n    summary_df[\"certainty\"] = np.select(conditions, choices, default=\"Uncertain\")\n\n    # Set default subtitle for beta coefficients\n    if subtitle is None and parameter == \"beta\":\n        subtitle = \"Beta coefficient analysis\"\n\n    gt_table = (\n        GT(summary_df)\n        .tab_header(\n            title=md(f\"**{title}**\"),\n            subtitle=md(subtitle) if subtitle else None,\n        )\n        .fmt_number(\n            columns=[\"mean\", \"sd\", \"hdi_2.5%\", \"hdi_97.5%\"],\n            decimals=3,\n        )\n        .data_color(\n            columns=[\"certainty\"],\n            palette=[\"#E1DFDD\", \"#F18F01\", \"#F18F01\"],\n            domain=[\"Uncertain\", \"Negative\", \"Positive\"],\n        )\n        .cols_label(\n            feature=md(\"**Feature**\"),\n            mean=md(\"**Mean**\"),\n            sd=md(\"**SD**\"),\n            **{\"hdi_2.5%\": md(\"**HDI 2.5%**\")},\n            **{\"hdi_97.5%\": md(\"**HDI 97.5%**\")},\n        )\n        .cols_align(align=\"center\", columns=[\"mean\", \"sd\", \"hdi_2.5%\", \"hdi_97.5%\", \"Certainty\"])\n        .tab_options(\n            table_font_size=\"14px\",\n            heading_title_font_size=\"20px\",\n            heading_subtitle_font_size=\"16px\",\n            row_group_font_weight=\"bold\",\n        )\n    )\n\n    return gt_table\n\n\n\n\nIdentifying Reliable Degradation Drivers (\\boldsymbol{\\beta} Coefficients)\nThe regression coefficients \\boldsymbol{\\beta} quantify the relationship between the engineered operational features (e.g., current and voltage metrics) and battery State of Health (\\text{SoH}) through the logit link function, \\log\\left(\\frac{\\mu}{1-\\mu}\\right). The credibility of each predictor is assessed by examining whether its 95% Highest Density Interval (HDI) includes zero.\n\n\nShow the code\ntable=analyze_parameter(idata, \"beta\", features=features)\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nParameter Summary\n\n\nBeta coefficient analysis\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\ncharge_current_auc\n‚àí0.159\n0.145\n‚àí0.437\n0.132\nUncertain\n\n\ncharge_current_mean\n‚àí0.147\n0.145\n‚àí0.420\n0.145\nUncertain\n\n\ndischarge_voltage_crest\n‚àí0.446\n0.041\n‚àí0.528\n‚àí0.369\nNegative\n\n\ndischarge_voltage_auc\n0.020\n0.040\n‚àí0.063\n0.092\nUncertain\n\n\n\n\n\n\n\n\nThe posterior summary indicates that only one feature emerges as a statistically reliable degradation driver at the 95% credibility level: the discharge_voltage_crest factor. Its 95% HDI lies entirely below zero (from -0.528 to -0.369), indicating strong evidence of a negative association with capacity retention. This result implies, with high certainty, that increases in this factor accelerate capacity fade. The posterior mean coefficient for the discharge_voltage_crest factor is \\beta = -0.446. Interpreted on the odds scale as \\text{Odds Ratio} = \\exp(-0.446) \\approx 0.64.\nThus, a one-unit increase in the crest factor is associated with an approximately 36% reduction in the odds of maintaining high battery capacity (1 - 0.64).\nIn contrast, the 95% HDIs for the remaining three features include zero (e.g., for charge_current_auc, HDI [-0.437,,0.132]). As a result, the model cannot rule out the possibility that their true effects are negligible or even slightly positive. These features therefore do not constitute statistically reliable degradation drivers under the current model specification. Consequently, maintenance and monitoring efforts can be focused on the discharge_voltage_crest factor as the dominant operational indicator of degradation.\n\nüõ†Ô∏è Action: Refit the Beta regression model using only the discharge_voltage_crest factor as an operational covariate. Evaluate whether predictive performance on the test set remains comparable and whether the posterior mean and HDI for this coefficient remain stable. Such consistency would further support the conclusion that the remaining features primarily contributed noise rather than explanatory signal.\n\n\n\nQuantifying the degradation rate (\\lambda_{\\text{rate}})\nThe parameter \\lambda_{\\text{rate}} governs the speed of capacity fade induced by cycling. By adopting a weakly informative Lognormal prior with increased dispersion (\\sigma = 1.0), the data is allowed to dominate the posterior estimation of this parameter.\n\n\nShow the code\ntable=analyze_parameter(idata, \"lambda_rate\", title=\"Lambda Rate Summary\", subtitle=\"Degradation rate parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nLambda Rate Summary\n\n\nDegradation rate parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\nlambda_rate\n0.003\n0.001\n0.002\n0.004\nPositive\n\n\n\n\n\n\n\n\nThe posterior summary yields a highly precise estimate of the degradation rate, with a posterior mean of \\mathbf{0.003} per unit of cycle data. This value is lower than the prior expectation (centered around 0.005), indicating that although degradation is inevitable, it progresses more gradually than initially assumed.\nThe remaining uncertainty is minimal, as evidenced by a small posterior standard deviation (\\text{SD} = 0.001) and a narrow 95% HDI of [0.002,,0.004]. These results confirm that the MCMC sampler has effectively leveraged the data to tightly constrain the degradation speed.\n\n\nModel precision (\\phi)\nThe precision parameter \\phi controls the dispersion of the Beta likelihood, quantifying how tightly the observed \\text{SoH} measurements cluster around the model-predicted mean after accounting for all modeled effects.\n\n\nShow the code\ntable=analyze_parameter(idata, \"phi\", title=\"Phi Summary\", subtitle=\"Phi  parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nPhi Summary\n\n\nPhi parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\nphi\n233.461\n10.140\n213.389\n252.803\nPositive\n\n\n\n\n\n\n\n\nThe posterior distribution of \\phi exhibits a high degree of concentration, with a posterior mean of 233.461 and a narrow 95% HDI. This large mean precision implies very low residual variance in \\text{SoH}, indicating that the combined model structure‚Äîincorporating the exponential degradation term, the cycling rate \\lambda, and the operational features \\boldsymbol{\\beta} explains the majority of observed variability across the battery fleet. The narrow HDI further indicates that this high precision is estimated with substantial certainty.\n\nDegr amp parameter (A)\nThe degradation amplitude parameter, \\text{degr\\_amp}, quantifies the maximum potential capacity fade attributable solely to the cycling process and operates on the log-odds scale. The posterior mean of 0.367 represents the maximum reduction in \\text{logit}(\\mu) induced by cycling over the battery‚Äôs lifetime, thereby determining the vertical extent of the degradation curve.\n\n\nShow the code\n\ntable=analyze_parameter(idata, \"degr_amp\", title=\"Degr amp\", subtitle=\"A  parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nDegr amp\n\n\nA parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\ndegr_amp\n0.367\n0.053\n0.265\n0.468\nPositive\n\n\n\n\n\n\n\n\nThe 95% HDI for \\text{degr\\_amp} is narrow and entirely positive, spanning [0.265,,0.468]. This provides strong statistical evidence that degradation due to cycling is both certain and quantitatively well-defined, rather than an artifact of noise. The strictly positive support of this parameter confirms that capacity loss is an unavoidable consequence of repeated cycling.\n\n\n\nPosterior predictive check\nFollowing parameter interpretation, a Posterior Predictive Check (PPC) is performed to assess model adequacy. This step evaluates whether the model, using posterior parameter samples, can generate synthetic data that closely resembles the observed measurements.\nUsing PyMC‚Äôs sample_posterior_predictive function, samples are drawn from the likelihood conditioned on the converged posterior chains. The resulting PPC compares three distributions: the observed data, the prior predictive distribution, and the posterior predictive distribution.\n\n\nShow the code\nwith model:\n    post_pred = pm.sample_posterior_predictive(idata, var_names=[\"y_obs\"], random_seed=42)\n\ny_post = post_pred.posterior_predictive[\"y_obs\"].stack(sample=[\"chain\", \"draw\"]).values\n# Mean of the posterior predictive\npost_mean = y_post.mean()\nn_draws = 500\nrng = np.random.default_rng(42)\ndraw_idx = rng.choice(y_post.shape[0], size=n_draws, replace=False)\ny_post_subset = y_post[:, draw_idx].flatten()\ny_post_subset = y_post_subset * (upper_bound - lower_bound) + lower_bound\n\n\ndf_posterior = pd.DataFrame(\n    {\n        \"SoH\": np.concatenate([y_obs, y_post_subset]),\n        \"type\": [\"observed\"] * len(y_obs) + [\"posterior\"] * len(y_post_subset),\n    }\n)\n\n\n\n\n\n\n\n\nThe figure below shows a Posterior Predictive Check (PPC), which is the gold standard for evaluating model fit in Bayesian statistics. It compares three key distributions for the capacity: the data we observed, our initial beliefs (Prior), and the model‚Äôs final predictions (Posterior).\n\n\nShow the code\ndf = pd.concat([df_prior, df_posterior])\nplot=plot_density(\n    df,\n    x_col=\"SoH\",\n    color_col=\"type\",\n    x_label=\"Capacity\",\n    fig_size=(700, 350),\n    title=\"Prior and Posterior Comparison\",\n    subtitle=\"Beta-regression\",\n)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.3\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.7\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                0.9\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.2\n              \n            \n          \n          \n            \n            \n            \n              \n                1.3\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Prior and Posterior Comparison\n      \n    \n    \n      \n        Beta-regression\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Capacity\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                observed\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                prior\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                posterior\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThe posterior predictive distribution aligns closely with the observed capacity distribution, indicating that the model successfully captures the underlying data-generating process. In contrast, the prior predictive distribution is smoother and exhibits broader structure, reflecting weaker and less targeted assumptions before observing data.\nThe shift and sharpening from prior to posterior predictive distributions demonstrate that the observed data provided substantial information and that the model effectively updated its initial beliefs.\nThe posterior predictive distribution is strongly skewed toward high capacity values near 1.0, consistent with the predominance of early- and mid-life measurements. A smaller secondary mode around 0.3‚Äì0.4 corresponds to a limited number of end-of-life observations. This agreement between synthetic and observed data provides strong evidence of model adequacy and predictive reliability.\n\n\n\nPredict capacity for a new battery\nThe final objective of this modeling effort is to transition from parameter estimation to practical prognosis by generating a full Posterior Predictive Distribution (PPD) for the capacity of a new or future battery state. This process converts uncertainty-aware parameter estimates into actionable prognostic predictions.\nThe PPD explicitly incorporates two fundamental sources of uncertainty:\n\nEpistemic uncertainty, arising from uncertainty in the estimated model parameters (e.g., the width of the HDI for \\lambda_{\\text{rate}}).\nAleatoric uncertainty, representing irreducible noise in the measurement process, as captured by the precision parameter \\phi.\n\nAs a result, the model produces not a single point estimate but a credible interval (HDI) that probabilistically bounds the true capacity value at each cycle.\nTo assess generalization performance and demonstrate practical utility for risk management, four cells from the CALCE dataset that were excluded during training are selected for evaluation. For each battery, the same four operational features used in model training are extracted and supplied to the fitted model.\n\n\nShow the code\nfrom bayes.regression.beta_degradation import get_posterior_predictions\n\n\n\n\nShow the code\nnew_df = test_df[test_df[\"BatteryID\"].isin([\"CALCE_CS2_34\", \"CALCE_CS2_36\", \"CALCE_CS2_37\", \"CALCE_CS2_33\"])].copy()\n\n\nPosterior predictions for unseen batteries are generated using the `get_posterior_predictions procedure, which applies the fitted Bayesian model to new input data. The process consists of the following steps:\n\nFeature Transformation: The operational features of the new battery are transformed using the same scaling object fitted during training, ensuring consistency between training and inference domains. The corresponding cycle counts are extracted separately, as they directly enter the exponential degradation component of the model.\n\n\n    x_new = scaler.transform(data[features])\n\nDummy Target Initialization: A placeholder target array is supplied to satisfy the dimensional requirements of the PyMC model‚Äôs observed variable. These values are ignored during posterior prediction..\n\n    y_dummy_scaled = np.full(shape=(X_new_scaled.shape[0],), fill_value=0.5)\n\nModel update: The new feature matrix, cycle data, and dummy target are injected into the model using pm.set_data, reconfiguring the model for prediction without retraining.\n\n    with battery_model:\n        pm.set_data({\n            \"x_data\": x_new,\n            \"cycle_data\": cycle_new,\n            \"y_obs\": y_dummy_scaled\n        })\n\nPosterior Predictive Sampling: Samples are drawn from the Posterior Predictive Distribution using pm.sample_posterior_predictive, incorporating both posterior parameter uncertainty and observation noise..\n\n    with battery_model:\n        post_pred = pm.sample_posterior_predictive(\n            idata,\n            var_names=[\"y_obs\"],\n            random_seed=42,\n            predictions=True,\n        )\n\n\nShow the code\npred_df = get_posterior_predictions(\n    idata, model, scaler, new_df, features, alpha=0.1, upper_bound=upper_bound, lower_bound=lower_bound\n)\nclear_output()\n\n\nPredictions for each test battery are visualized using the plot_hdi_regression function. The plots display the observed capacity measurements (points) overlaid with the model‚Äôs posterior predictive mean and the corresponding 90% HDI. The blue line represents the posterior predictive mean where the shaded region represents the 90% HDI, quantifying predictive uncertainty.\n\n\nShow the code\nfrom bayes.plot.regres_plot import plot_hdi_regression\n\n\n\n\nShow the code\nplot=plot_hdi_regression(\n    pred_df,\n    x_column=\"cycle\",\n    y_column=\"capacity\",\n    group_column=\"BatteryID\",\n    pred_column=\"pred_median\",\n    x_label=\"Cycle Number\",\n    y_label=\"Capacity (Ah)\",\n    title_prefix=\"Battery Capacity vs. Cycle with Posterior Predictions\",\n    subtitle=\"90% HDI accounts for  uncertainty.\",\n    alpha=0.1,\n) + ggsize(800, 500)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_33\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_34\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                500\n              \n            \n          \n          \n            \n            \n            \n              \n                1,000\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_36\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                500\n              \n            \n          \n          \n            \n            \n            \n              \n                1,000\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_37\n        \n      \n    \n    \n      \n        Battery Capacity vs. Cycle with Posterior Predictions\n      \n    \n    \n      \n        90% HDI accounts for  uncertainty.\n      \n    \n    \n      \n        Capacity (Ah)\n      \n    \n    \n      \n        Cycle Number\n      \n    \n    \n      \n        \n          \n            Legend\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                90% HDI\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n                \n                  \n                    \n                  \n                \n              \n            \n            \n              \n                Observed Data\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n                \n                  \n                    \n                  \n                \n              \n            \n            \n              \n                Posterior Mean\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayesian Modelling| Anthony Faustine @ 2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nKey Observations\n\nThe posterior predictive mean closely tracks the observed capacity trajectory across the full battery lifetime, including the non-linear degradation phase near end-of-life.\nApproximately 90\\% (or more) of observed data points fall within the predictive HDI, indicating well-calibrated uncertainty estimates.\nFor batteries CS2_33, CS2_36, and CS2_37, the HDI remains narrow even during late-life degradation, reflecting high model confidence and low residual noise.\nFor battery CS2_34, the HDI widens toward the final cycles, appropriately reflecting increased predictive uncertainty in the late-life regime.\n\n\nEvaluating Predictive Performance\nPredictive performance on the held-out batteries is quantified using both accuracy and uncertainty-based metrics.\n\n\nShow the code\nfrom bayes.metrics.interval import get_interval_metrics\nfrom bayes.metrics.regression import regression_report\n\nmetrics_list = []\nfor battery_id, df in pred_df.groupby(\"BatteryID\"):\n    reg_report = regression_report(df[\"capacity\"], df[\"pred_median\"])\n    interval_report = get_interval_metrics(\n        df[\"pred_median\"].values,\n        df[\"capacity\"].values,\n        df[\"hdi_low\"].values,\n        df[\"hdi_high\"].values,\n        alpha=0.1,\n    )\n    full_report = pd.concat([reg_report, interval_report], ignore_index=True)\n    full_report[\"BatteryID\"] = battery_id\n    metrics_list.append(full_report)\nmetrics_df = pd.concat(metrics_list, ignore_index=True)\n\nmetrics_df = metrics_df.pivot_table(\n    index=[\"BatteryID\"],\n    columns=\"Metric\",\n    values=\"Value\",\n).reset_index()\n\n\n\ndef make_metrics_table(metrics_df, title=\"Model Evaluation Results\"):\n    \"\"\"Generate a formatted GT table from cross-validation metrics.\"\"\"\n    # Sort to present best models first\n    df = metrics_df.sort_values([\"BatteryID\", \"MAE\"]).reset_index(drop=True)\n\n    # Build base table\n    gt = (\n        GT(df[[\"BatteryID\", \"MAE\", \"RMSE\", \"R2\", \"NMPI\", \"PICP\"]])\n        .tab_header(title=title, subtitle=\"Per-battery performance\")\n        .cols_label(BatteryID=\"Test Cell\", MAE=\"MAE\", RMSE=\"RMSE\", R2=md(\"R&lt;sup&gt;2&lt;/sup&gt;\"))\n        .fmt_number(columns=[\"MAE\", \"RMSE\", \"NMPI\", \"PICP\"], decimals=3)\n        .fmt_number(columns=\"R2\", decimals=3)\n        .tab_spanner(label=\"Error Metrics\", columns=[\"MAE\", \"RMSE\", \"NMPI\", \"PICP\"])\n        .tab_style(\n            style=style.text(weight=\"bold\"),\n            locations=loc.body(columns=\"Model\"),\n        )\n        .tab_options(\n            table_font_size=\"small\",\n            # row_strip_color=\"#fafafa\"\n        )\n    )\n    for col in [\"MAE\", \"RMSE\", \"R2\", \"NMPI\", \"PICP\"]:\n        best_idx = df[col].idxmax() if col in [\"R2\", \"PICP\"] else df[col].idxmin()\n        gt = gt.tab_style(style=style.fill(color=\"#E1DFDD\"), locations=loc.body(rows=best_idx, columns=col))\n\n    return gt\n\n\n\n\nShow the code\nmake_metrics_table(metrics_df, title=\"Model Evaluation Results\")\n\n\n\n\n\n\n\n\nModel Evaluation Results\n\n\nPer-battery performance\n\n\nTest Cell\nError Metrics\nR2\n\n\nMAE\nRMSE\nNMPI\nPICP\n\n\n\n\nCALCE_CS2_33\n0.010\n0.010\n0.100\n1.000\n0.990\n\n\nCALCE_CS2_34\n0.040\n0.050\n0.100\n0.750\n0.910\n\n\nCALCE_CS2_36\n0.020\n0.020\n0.100\n0.990\n0.990\n\n\nCALCE_CS2_37\n0.010\n0.010\n0.100\n1.000\n1.000\n\n\n\n\n\n\n\n\nAccuracy Metrics\nPoint-prediction accuracy is evaluated using the coefficient of determination (R^2), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). Together, these metrics assess how well the model‚Äôs posterior predictive mean captures the observed capacity degradation trajectory.\n\nR^2 quantifies the proportion of variance in the observed capacity explained by the model. Values close to 1 indicate that the predicted degradation curve accurately follows the overall trend and slope of capacity fade.\nMAE represents the average absolute deviation between predicted and observed capacity values, expressed in State-of-Health (SoH) units. MAE provides a physically interpretable measure of typical prediction error.\nRMSE penalizes larger deviations more strongly than MAE and is therefore particularly sensitive to localized mismatches, especially during the highly nonlinear end-of-life degradation phase.\n\nFrom the results summarized above, the model achieves R^2 values between 0.910 and 1.0, with MAE in the range of 0.010‚Äì0.040 SoH units, indicating strong point-prediction accuracy across all evaluated cells. RMSE values range from 0.01 to 0.05 SoH units, confirming that large deviations are generally rare.\nHowever, Cell 34 exhibits the highest RMSE (0.05), along with comparatively lower R^2 and higher MAE than the remaining cells. This combination indicates that, while the model captures the overall degradation trend for Cell 34, it experiences larger localized errors‚Äîparticularly near late-life degradation relative to other cells. These deviations suggest the presence of sharper nonlinear behavior or cell-specific degradation mechanisms not fully represented by the global model parameters.\nUncertainty Quality Metrics\nBeyond point accuracy, a central goal of Bayesian modeling is to provide reliable and interpretable uncertainty estimates. This is assessed using Prediction Interval Coverage Probability (PICP) and Normalized Mean Prediction Interval (NMPI).\n\nPICP measures the fraction of observed capacity values that fall within the model‚Äôs 90% Highest Density Interval (HDI). A well-calibrated model should achieve PICP close to the nominal level (0.95), indicating that the predicted uncertainty accurately reflects real variability.\nNMPI quantifies the average width of the predictive interval, normalized by the observed capacity range. NMPI reflects the sharpness of predictions: lower values indicate tighter uncertainty bounds, which are essential for actionable maintenance and risk-based decision-making.\n\nResults show consistently low NMPI values (approximately \\mathbf{0.1}) across all test cells. This confirms that the predictive intervals are narrow relative to the capacity range, indicating high confidence in the model‚Äôs predictions. At the same time, PICP exceeds 0.90 for most cells, demonstrating that this confidence is not over-stated and that the uncertainty bounds are well calibrated.\nCell 34 again deviates from this pattern, exhibiting reduced PICP (75\\%). This indicates that a larger fraction of its observed capacity measurements fall outside the predicted 90% HDI, suggesting either increased intrinsic variability or degradation dynamics that differ from those captured by the global model.\n\nüõ†Ô∏è Action: To diagnose the degraded predictive performance observed for Cell 34, compare all test cells by plotting R^2 versus PICP to identify accuracy‚Äìreliability trade-offs. Examine whether Cell 34‚Äôs operational features fall outside the training-feature distribution, indicating extrapolation beyond the model‚Äôs learned domain. Finally, compare the capacity degradation distribution of Cell 34 with the training cell to assess whether it follows a distinct degradation regime.\n\n\n\n\nConclusion\nThis post demonstrates that a single-level Bayesian Beta regression model, informed by physics-aware priors and carefully engineered features, can deliver highly accurate capacity predictions together with trustworthy uncertainty bounds. The model achieves near-perfect predictive accuracy while maintaining well-calibrated 95% credible intervals, validating the use of Beta likelihoods and informed priors for battery degradation modeling.\nHowever, the current formulation assumes that the degradation rate (\\lambda_{\\text{rate}}) and operational sensitivities (\\boldsymbol{\\beta}) are shared across the entire battery fleet. In practice, manufacturing variability and latent defects introduce unit-specific degradation behavior. As a result, even a highly accurate global model may underpredict risk for batteries from unfavorable batches or overestimate degradation for higher-quality units.\nComing Next: In Part 3, this limitation will be addressed by introducing Hierarchical Bayesian Models. These models learn a robust global degradation trend while allowing each individual battery to exhibit informed local deviations in parameters such as \\lambda_{\\text{rate}} and \\boldsymbol{\\beta}.\n\nThe data and full code in pymc5 is available on my GITHUB page.\n\n\n\nReferences\n\nZhang, H., Li, Y., Zheng, S. et al.¬†Battery lifetime prediction across diverse ageing conditions with inter-cell deep learning. Nat Mach Intell 7, 270‚Äì277 (2025). https://doi.org/10.1038/s42256-024-00972-x\nFerrari, S. L. P., & Cribari-Neto, F. (2004). Beta regression for modelling rates and proportions. Journal of Applied Statistics, 31(7), 799‚Äì815.\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.\nMcElreath, R. (2020). Statistical Rethinking (2nd ed.). CRC Press.\n\n\n\n\n\nCitationBibTeX citation:@online{faustine2025,\n  author = {Faustine, Anthony},\n  title = {Bayesian {Regression:} {A} {Real-World} {Battery}\n    {Degradation} {Case} {Study}},\n  date = {2025-12-17},\n  url = {https://sambaiga.github.io/pages/blog/posts/2025/12/2025-12-1-bayesian-regression..html},\n  langid = {en}\n}"
  },
  {
    "objectID": "pages/about/index.html",
    "href": "pages/about/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "I am a Lead Data Scientist at the Centre for Intelligent Power (CIP) in Dublin, Ireland. My expertise sits at the intersection of applied machine learning and strategic innovation management. Beyond technical specialization, I function as a strategic AI leader and builder. With proven experience across industry and academia, I specialize in architecting intelligent solutions while fostering growth through mentorship and cross-functional leadership.\n\nMy mission: to lead strategic AI innovation that bridges research with real-world impact, building toward a more sustainable and intelligent future.\n\n\nReady to collaborate?\nI‚Äôm always open to discussing new opportunities, research collaborations, or speaking engagements.\n\nSchedule a Chat  Download CV"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "  \n          Leading Applied AI\n          for Strategic Impact\n        ",
    "section": "",
    "text": "I architect intelligent systems that translate advanced machine learning\n          into measurable business value and sustainable transformation.\n        \n        \n          \n            \n            Let's Connect\n          \n        \n      \n\n      \n        \n          10+\n          yearsexperience\n        \n        \n          10+\n          projectsdelivered\n        \n        \n          100%\n          satisfiedimpact"
  },
  {
    "objectID": "pages/publication/articles/mlpf-2025/index.html",
    "href": "pages/publication/articles/mlpf-2025/index.html",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "",
    "text": "Presented at the 44th International Symposium on Forecasting in Dijon, France June 2024"
  },
  {
    "objectID": "pages/publication/index.html",
    "href": "pages/publication/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "2025 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Submitted\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine, Ethna O Connor,  Dr Tupokigwe Isagah, and  Dr Kevin Byrne\n                \n              \n\n              \n              \n                \n                  Strategic Management of AI Investments: A Dynamic Capabilities and Business Model Innovation Perspective\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      Journal of Strategic Information Systems\n                      \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      Artificial Intelligence (AI) promises substantial returns in organisations, motivating firms to cons\n                      \n                        ...\n                        \n                          ider AI investments to improve their businesses. The investments offer new opportunities for transforming business processes, which can increase the company‚Äôs value. Despite the promise of AI to drive competitive advantage, organisations struggle to realise its value, often because of fragmented initiatives and strategic misalignment rather than technological failures. Current research often overlooks the strategic initiatives required to convert dynamic technological potential into sustained performance. This study addresses this gap by integrating Dynamic Capabilities (DC) and Business Model Innovation (BMI) theories to deconstruct the realisation of AI value. Employing a mixed-methods design, we triangulated a systematic literature review with exploratory practitioner interviews to contrast theoretical ideals with operational realities, aiming to benefit from AI investments. The findings reveal that the primary barriers to AI value realisation, strategic misalignment, organisational inertia, and technical/data limitations, are interconnected and mutually reinforcing, creating a negative feedback loop that prevents scaled impact. This systemic failure is driven by critical operational frictions, including data governance silos, leadership literacy gaps, and the inability to quantify AI value, which collectively disrupt the link between strategic intent and execution. Also, findings showed the potential of strategically interlinking Dynamic Capabilities (DC) and Business Model Innovation (BMI) for business value, where DC senses and mobilizes AI opportunities and BMI turns over adaptive capacities into value creation and capture, thus contributing to AI value realization in organisations.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      strategy\n                    \n                  \n                    \n                      innovation\n                    \n                  \n                    \n                      technology\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2025 \n                  \n                    \n                      \n                        \n                      \n                      Conference\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine and Pereira, Lucas\n                \n              \n\n              \n              \n                \n                  Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power\n                \n              \n\n              \n              \n                \n                  \n                    \n                    CIRED Chicago Workshop 2024: Resilience of Electric Distribution Systems\n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      This paper presents a probabilistic forecasting approach tailored for low voltage (LV)\nsubstations, \n                      \n                        ...\n                        \n                          offering short-term predictions for three crucial variables: voltage,\nreactive power, and active power. These parameters play a vital role in the resilience\nof distribution systems, especially in the presence of Distributed Energy Resources (DERs).\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      forecasting\n                    \n                  \n                    \n                      probabilistic\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      low voltage network\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n            \n               Featured\n            \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2025 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine and Nuno Jardim Nunes and Lucas Pereira\n                \n              \n\n              \n              \n                \n                  Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      IEEE Transactions on Power Systems\n                      \n                        , Vol. 40, No. 1\n                      \n                      \n                        , pp. 46-5\n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      Power demand forecasting is becoming a crucial tool for the planning and operation of Low Voltage (L\n                      \n                        ...\n                        \n                          V) distribution systems. Most importantly, the high penetration of Photovoltaics (PV) power generation as part of Distributed Energy Resource (DER)s has transformed the power demand forecasting problem at the distribution level into net-load forecasting. This paper introduces a novel and scalable approach to probabilistic forecasting at LV substations with PV generation. It presents a multi-variates probabilistic forecasting approach, leveraging Quantile Regression (QR). The proposed architecture uses a computationally efficient feed-forward neural net to capture the complex interaction between the historical load demands and covariate variables such as solar irradiance. It is empirically demonstrated that the proposed method can efficiently produce well-calibrated forecasts, both auto-regressively or in a single forward pass. Furthermore, a benchmark against four state-of-the-art forecasting approaches show that the proposed approach offers a desirable trade-off between forecasting accuracies, calibrated uncertainty, and computation complexity.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      forecasting\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      probabilistic\n                    \n                  \n                    \n                      low voltage networks\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Slide\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2024 \n                  \n                    \n                      \n                        \n                      \n                      Conference\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine and Pereira, Lucas\n                \n              \n\n              \n              \n                \n                  Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation\n                \n              \n\n              \n              \n                \n                  \n                    \n                    2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm), Oslo, Norway\n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      This paper presents a probabilistic forecasting approach tailored for low voltage (LV)\nsubstations, \n                      \n                        ...\n                        \n                          offering short-term predictions for three crucial variables: voltage,\nreactive power, and active power. These parameters play a vital role in the resilience\nof distribution systems, especially in the presence of Distributed Energy Resources (DERs).\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      forecasting\n                    \n                  \n                    \n                      probabilistic\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      low voltage network\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Slide\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2023 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Hafsa Bousbiat, Anthony Faustine, Christoph Klemenjak, Lucas Pereira and Wilfried Elmenreich\n                \n              \n\n              \n              \n                \n                  Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters and  Modular Pipelines\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      IEEE Transactions on Industrial Informatics\n                      \n                        , Vol. 19, No. 5\n                      \n                      \n                        , pp. 7002-7010\n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      Nonintrusive load monitoring (NILM) techniques are increasingly becoming a key instrument for identi\n                      \n                        ...\n                        \n                          fying the power consumption of individual appliances based on a single metering point. Particularly, deep learning (DL) models are gaining interest in this regard. However, the challenges brought by the NILM datasets and the nonavailability of common experimental guidelines tend to compromise comparison, research transparency, and replicability. The limited adoption of efficient research instruments and lack of best practices guidelines contribute in huge part to this problem, where no features, encouraging standardized formats for benchmarking, and results sharing are offered. To address these issues, we first present a brief overview of recent best practices for DL and highlight how deep NILM research can benefit from these practices. Furthermore, we suggest a novel open-source toolkit leveraging these practices, i.e., Deep-NILMTK. The proposed toolkit offers a common testing bed for NILM algorithms independently of the underlying deep learning framework with a modular NILM pipeline that can easily be customized. Furthermore, Deep-NILMTK introduces the concept of experiment templating to offer predesigned experiments allowing to enhancing research efficiency. Leveraging this concept and DL best practices, we present a case study of creating an online NILM benchmark repository1 considering eight of the most popular deep NILM algorithms. All sources relative to the tool are made publicly available on Github2 along with the corresponding documentation.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      nilm\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      mlops\n                    \n                  \n                    \n                      disaggregation\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Code\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2022 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine and Pereira, Lucas\n                \n              \n\n              \n              \n                \n                  FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      IEEE Transactions on Smart Grid\n                      \n                        , Vol. 13, No. 3\n                      \n                      \n                        , pp. 2440-2451\n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      The increased penetration of Renewable Energy Sources (RES) as part of a decentralized and distribut\n                      \n                        ...\n                        \n                          ed power system makes net-load forecasting a critical component in the planning and operation of power systems. However, compared to the transmission level, producing accurate short-term net-load forecasts at the distribution level is complex due to the small number of consumers. Moreover, owing to the stochastic nature of RES, it is necessary to quantify the uncertainty of the forecasted net-load at any given time, which is critical for the real-world decision process. This work presents parameterized deep quantile regression for short-term probabilistic net-load forecasting at the distribution level. To be precise, we use a Deep Neural Network (DNN) to learn both the quantile fractions and quantile values of the quantile function. Furthermore, we propose a scoring metric that reflects the trade-off between predictive uncertainty performance and forecast accuracy. We evaluate the proposed techniques on historical real-world data from a low-voltage distribution substation and further assess its robustness when applied in real-time. The experiment‚Äôs outcomes show that the resulting forecasts from our approach are well-calibrated and provide a desirable trade-off between forecasting accuracies and predictive uncertainty performance that are very robust even when applied in real-time.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      forecasting\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      probabilistic\n                    \n                  \n                    \n                      low voltage network\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Code\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2021 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine and Pereira, Lucas and Klemenjak, Christoph\n                \n              \n\n              \n              \n                \n                  Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      IEEE Transactions on Smart Grid\n                      \n                        , Vol. 12, No. 1\n                      \n                      \n                        , pp. 398-406\n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      To this day, hyperparameter tuning remains a cumbersome task in Non-Intrusive Load Monitoring (NILM)\n                      \n                        ...\n                        \n                           research, as researchers and practitioners are forced to invest a considerable amount of time in this task. This paper proposes adaptive weighted recurrence graph blocks (AWRG) for appliance feature representation in event-based NILM. An AWRG block can be combined with traditional deep neural network architectures such as Convolutional Neural Networks for appliance recognition. Our approach transforms one cycle per activation current into an weighted recurrence graph and treats the associated hyper-parameters as learn-able parameters. We evaluate our technique on two energy datasets, the industrial dataset LILACD and the residential PLAID dataset. The outcome of our experiments shows that transforming current waveforms into weighted recurrence graphs provides a better feature representation and thus, improved classification results. It is concluded that our approach can guarantee uniqueness of appliance features, leading to enhanced generalisation abilities when compared to the widely researched V-I image features. Furthermore, we show that the initialisation parameters of the AWRG's have a significant impact on the performance and training convergence.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      nilm\n                    \n                  \n                    \n                      load recognition\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      disaggregation\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Code\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2021 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  V√∂lker, Benjamin,  Reinhardt, Andreas,  Anthony Faustine and Pereira, Lucas\n                \n              \n\n              \n              \n                \n                  Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      Energies\n                      \n                        , Vol. 14, No. 3\n                      \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      The key advantage of smart meters over traditional metering devices is their ability to transfer con\n                      \n                        ...\n                        \n                          sumption information to remote data processing systems. Besides enabling the automated collection of a customer‚Äôs electricity consumption for billing purposes, the data collected by these devices makes the realization of many novel use cases possible. However, the large majority of such services are tailored to improve the power grid‚Äôs operation as a whole. For example, forecasts of household energy consumption or photovoltaic production allow for improved power plant generation scheduling. Similarly, the detection of anomalous consumption patterns can indicate electricity theft and serve as a trigger for corresponding investigations. Even though customers can directly influence their electrical energy consumption, the range of use cases to the users‚Äô benefit remains much smaller than those that benefit the grid in general. In this work, we thus review the range of services tailored to the needs of end-customers. By briefly discussing their technological foundations and their potential impact on future developments, we highlight the great potentials of utilizing smart meter data from a user-centric perspective. Several open research challenges in this domain, arising from the shortcomings of state-of-the-art data communication and processing methods, are furthermore given. We expect their investigation to lead to significant advancements in data processing services and ultimately raise the customer experience of operating smart meters.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      nilm\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      disaggregation\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2020 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine and Pereira, Lucas\n                \n              \n\n              \n              \n                \n                  Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      Energies\n                      \n                        , Vol. 13, No. 13\n                      \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      Appliance recognition is one of the vital sub-tasks of NILM in which a machine learning classier is \n                      \n                        ...\n                        \n                          used to detect and recognize active appliances from power measurements. The performance of the appliance classifier highly depends on the signal features used to characterize the loads. Recently, different appliance features derived from the voltage‚Äìcurrent (V‚ÄìI) waveforms have been extensively used to describe appliances. However, the performance of V‚ÄìI-based approaches is still unsatisfactory as it is still not distinctive enough to recognize devices that fall into the same category. Instead, we propose an appliance recognition method utilizing the recurrence graph (RG) technique and convolutional neural networks (CNNs). We introduce the weighted recurrent graph (WRG) generation that, given one-cycle current and voltage, produces an image-like representation with more values than the binary output created by RG. Experimental results on three different sub-metered datasets show that the proposed WRG-based image representation provides superior feature representation and, therefore, improves classification performance compared to V‚ÄìI-based features.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      nilm\n                    \n                  \n                    \n                      load recognition\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      dissagregation\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Poster\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Code\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2020 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine and Pereira, Lucas\n                \n              \n\n              \n              \n                \n                  Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      Energies\n                      \n                        , Vol. 13, No. 16\n                      \n                      \n                        , pp. 1996-1073\n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      The advance in energy-sensing and smart-meter technologies have motivated the use of a Non-Intrusive\n                      \n                        ...\n                        \n                           Load Monitoring (NILM), a data-driven technique that recognizes active end-use appliances by analyzing the data streams coming from these devices. NILM offers an electricity consumption pattern of individual loads at consumer premises, which is crucial in the design of energy efficiency and energy demand management strategies in buildings. Appliance classification, also known as load identification is an essential sub-task for identifying the type and status of an unknown load from appliance features extracted from the aggregate power signal. Most of the existing work for appliance recognition in NILM uses a single-label learning strategy which, assumes only one appliance is active at a time. This assumption ignores the fact that multiple devices can be active simultaneously and requires a perfect event detector to recognize the appliance. In this paper proposes the Convolutional Neural Network (CNN)-based multi-label learning approach, which links multiple loads to an observed aggregate current signal. Our approach applies the Fryze power theory to decompose the current features into active and non-active components and use the Euclidean distance similarity function to transform the decomposed current into an image-like representation which, is used as input to the CNN. Experimental results suggest that the proposed approach is sufficient for recognizing multiple appliances from aggregated measurements.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      nilm\n                    \n                  \n                    \n                      load recognition\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      dissagregation\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          PDF\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Code\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2017 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine, Nerey Henry Mvungi,  Shubi Kaijage and Kisangiri Michael\n                \n              \n\n              \n              \n                \n                  A Survey on Non-Intrusive Load Monitoring Methodies and Techniques for Energy Disaggregation Problem\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      CoRR\n                      \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      The rapid urbanization of developing countries coupled with explosion in construction of high rising\n                      \n                        ...\n                        \n                           buildings and the high power usage in them calls for conservation and efficient energy program. Such a program require monitoring of end-use appliances energy consumption in real-time. The worldwide recent adoption of smart-meter in smart-grid, has led to the rise of Non-Intrusive Load Monitoring (NILM); which enables estimation of appliance-specific power consumption from building's aggregate power consumption reading. NILM provides households with cost-effective real-time monitoring of end-use appliances to help them understand their consumption pattern and become part and parcel of energy conservation strategy. This paper presents an up to date overview of NILM system and its associated methods and techniques for energy disaggregation problem. This is followed by the review of the state-of-the art NILM algorithms. Furthermore, we review several performance metrics used by NILM researcher to evaluate NILM algorithms and discuss existing benchmarking framework for direct comparison of the state of the art NILM algorithms. Finally, the paper discuss potential NILM use-cases, presents an overview of the public available dataset and highlight challenges and future research directions.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      nilm\n                    \n                  \n                    \n                      machine learning\n                    \n                  \n                    \n                      disaggregation\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2014 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine and Aloys N. Mvuma\n                \n              \n\n              \n              \n                \n                  Ubiquitous Mobile Sensing for Water Quality Monitoring and Reporting within Lake Victoria Basin\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      Wireless Sensor Network\n                      \n                        , Vol. 6, No. 12\n                      \n                      \n                        , pp. 257-264\n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      As the human population growth and industry pressure in most developing countries continue to increa\n                      \n                        ...\n                        \n                          se, effective water quality monitoring and evaluation has become critical for water resources management programs. This paper presents the ubiquitous mobile sensing system for water quality data collection and monitoring applications in developing countries. The system was designed based on the analysis of the existing solution. Open source hardware and software was used to develop the prototype of the system. Field testing of the system conducted in Nkokonjero, Uganda and Mwanza, Tanzania verified the functionalities of the system and its practical application in actual environment. Results show that proposed solution is able to collect and present data in a mobile environment.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      wsn\n                    \n                  \n                    \n                      iot\n                    \n                  \n                    \n                      water quality\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n            \n              \n              \n                \n                  \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  2014 \n                  \n                    \n                      \n                        \n                      \n                      Journal\n                    \n                  \n                \n              \n              \n              \n              \n                \n                   Published\n                \n              \n            \n\n            \n            \n              \n              \n                \n                  \n                  Anthony Faustine, Aloys N. Mvuma,  Hector J. Mongi,  Maria C. Gabriel, Albino J. Tenge and Samuel B. Kucel\n                \n              \n\n              \n              \n                \n                  Wireless Sensor Networks for Water Quality Monitoring and Control within Lake Victoria Basin: Prototype Development\n                \n              \n\n              \n              \n                \n                  \n                    \n                    \n                      Wireless Sensor Network\n                      \n                        , Vol. 6, No. 12\n                      \n                      \n                        , pp. 281-90\n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                  \n                    \n                      The need for effective and efficient monitoring, evaluation and control of water quality in Lake Vic\n                      \n                        ...\n                        \n                          toria Basin (LVB) has become more demanding in this era of urbanization, population growth and climate change and variability. Traditional methods that rely on collecting water samples, testing and analyses in water laboratories are not only costly but also lack capability for real-time data capture, analyses and fast dissemination of information to relevant stakeholders for making timely and informed decisions. In this paper, a Water Sensor Network (WSN) system prototype developed for water quality monitoring in LVB is presented. The development was preceded by evaluation of prevailing environment including availability of cellular network coverage at the site of operation. The system consists of an Arduino microcontroller, water quality sensors, and a wireless network connection module. It detects water temperature, dissolved oxygen, pH, and electrical conductivity in real-time and disseminates the information in graphical and tabular formats to relevant stakeholders through a web-based portal and mobile phone platforms. The experimental results show that the system has great prospect and can be used to operate in real world environment for optimum control and protection of water resources by providing key actors with relevant and timely information to facilitate quick action taking.\n\n                        \n                        \n                          Read more\n                          \n                            \n                          \n                        \n                      \n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                    \n                      wsn\n                    \n                  \n                    \n                      iot\n                    \n                  \n                    \n                      water quality\n                    \n                  \n                \n              \n\n              \n              \n                \n                  \n                  \n                     Full details\n                  \n                  \n                  \n                  \n                    \n                      \n                        \n                          \n                          Slide\n                        \n                      \n                    \n                      \n                        \n                          \n                            \n                          \n                          Link\n                        \n                      \n                    \n                  \n                  \n                  \n                  \n                    \n                       DOI\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n  \n\n  \n  \n    \n    \n    \n      \n        \n        Prev\n      \n      \n      \n      \n      \n        Next\n        \n      \n    \n    \n    \n    \n    \n  \n\nNo matching items"
  },
  {
    "objectID": "pages/publication/conferences/smartgrid-2024/index.html",
    "href": "pages/publication/conferences/smartgrid-2024/index.html",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "",
    "text": "Presented at the 2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm), Oslo, Norway"
  },
  {
    "objectID": "pages/blog/posts/2025/10/2025-10-10-bayesion-foundation.html",
    "href": "pages/blog/posts/2025/10/2025-10-10-bayesion-foundation.html",
    "title": "Understanding Bayesian Thinking for Industrial Applications",
    "section": "",
    "text": "Introduction\nA company has recently installed a new, expensive machine. A critical question arises: How long will it last before failure?. The lead engineer, drawing on experience with previous models, estimates a lifespan of approximately 10 years. However, only 3 months of real-world test data are available for this specific unit, and a major warranty and service contract decision must be made immediately.\nThis scenario exemplifies a common challenge in industrial applications: making informed decisions with limited data. Bayesian thinking offers a powerful framework to address such problems by combining prior knowledge with observed data to update our beliefs about uncertain parameters.\nIn this article, we will explore the fundamentals of Bayesian thinking and how it can be applied to industrial scenarios like the one described above. We will cover key concepts such as prior distributions, likelihood functions, and posterior distributions, and demonstrate how to implement Bayesian models using Python‚Äôs PyMC library.\nTo immediately dive into the code and reproduce the models discussed in this article, you can use our accompanying resources:\n\nRepository: Fork bayesian-modelling repository and follow the setup instructions in the README.md file.\nNotebook: Launch the bayesian-modelling-01.ipynb notebook located in the notebook folder to follow along step-by-step.\n\n\n\nThe Building Blocks of Bayesian modeling\nTraditional (Frequentist) statistics relies on large datasets the ‚Äúlong run‚Äù to produce confident conclusions, a limitation in industrial contexts where data are often sparse. New products, machines, or processes typically generate only small samples, while valuable expert knowledge such as an engineer‚Äôs lifespan estimate is excluded from conventional models.\nBayesian inference overcomes these issues by combining prior knowledge with new data, enabling faster and more informed decisions when information is limited. This integration of expertise and evidence defines Bayesian thinking.\nThe process of updating our beliefs is formalized by Bayes‚Äô Theorem. \nP(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}\n\nWhile the formula looks mathematical, its components represent a beautifully intuitive learning cycle.\n\n\\mathrm{Posterior} \\propto \\mathrm{Likelihood} \\times \\mathrm{Prior}\n\nLet us break down how this relates to our machine failure problem; where \\theta is the machine‚Äôs expected failure rate, and D is the 3 months of test data.\n\nPrior, Likelihood, and Posterior\n\nPrior P(\\theta) represents the initial belief before observing any data. It incorporates domain expertise and historical knowledge. For instance, historical records may indicate that similar machines have an average lifespan of approximately ten years, implying a low failure rate. This prior belief defines the starting point for ùúÉ \\theta.\nLikelihood P(D \\mid \\theta) quantifies the compatibility between the observed data and a given parameter value. It expresses the probability of observing the test outcomes for different possible failure rates. In this context, the likelihood measures how probable it is to observe zero failures within three months if the true average lifespan were, for example, five or fifteen years.\nPosterior P(\\theta \\mid D) represents the updated belief after incorporating the observed data. It integrates prior knowledge with the evidence provided by the likelihood. In the machine-failure example, the posterior distribution expresses the updated estimate of expected lifespan after combining historical information (e.g., the ten-year prior) with the three months of failure-free operational data.\n\n\n\nPyMC: The Probabilistic Programming Engine\nUnderstanding the relationship \\mathrm{Posterior} \\propto \\mathrm{Likelihood} \\times \\mathrm{Prior} is the conceptual heart of Bayesian analysis. However, calculating the actual posterior distribution, P(\\theta \\mid D), often involves complex, multi-dimensional integration that is impossible to solve analytically for real-world industrial problems. This challenge is addressed through Probabilistic Programming Languages (PPLs) such as PyMC.\nPyMC is an open-source Python library for constructing and fitting Bayesian statistical models using advanced computational algorithms, including Markov Chain Monte Carlo (MCMC) and variational inference. It is one of several modern PPLs available in Python, alongside Pyro and TensorFlow Probability (TFP). This tutorial focuses on PyMC due to its clarity, community support, and extensive documentation.\n\n\n\nCase study: A/B Testing with Small Samples\nTo shift from theory to practical implementation, we will apply the Bayesian building blocks Prior, Likelihood, and Posterior to a concrete industrial problem common in tech and e-commerce: A/B Testing\n\nSuppose you are a data scientist at an e-commerce company. The marketing team just launched a new website feature and wants to know:\n\n\nWhat‚Äôs the true conversion rate?\nIs it better than the old version (which historically has an 8% conversion rate)?\nHow much should we trust this estimate with limited data?\n\n\nDuring the first few days of the feature launch, the company has observed 200 visitors with only 15 conversion.\n\n\nLibrary Imports\nImport the required Python libraries for Bayesian modeling:\n\nNumPy for numerical computations\nPandas for data manipulation\nPyMC for Bayesian statistical modeling\nMatplotlib, arviz and altair for visualization\n\n\n\nShow the code\n#import libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nimport pymc as pm\nimport arviz as az\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import clear_output\nfrom great_tables import GT\nimport matplotlib.pyplot as plt\naz.style.use(\"arviz-white\")\nfrom cycler import cycler\ncolors=['#107591','#f69a48','#00c0bf', '#fdcd49',\"#cf166e\", \n        \"#7035b7\", \"#212121\",\"#757575\", \"#E0E0E0\",\"#FAFAFA\"]\nplt.rcParams.update({\n    \"figure.dpi\": 100,\n    \"axes.labelsize\": 12,\n    \"axes.titlesize\": 12,\n    \"figure.titlesize\": 12,\n        \"font.size\": 12,\n        \"legend.fontsize\": 12,\n        \"xtick.labelsize\": 12,\n        \"ytick.labelsize\": 12,\n        \"axes.linewidth\" : 0.5,\n        \"lines.linewidth\" : 1.,\n        \"legend.frameon\" :False,\n        'axes.prop_cycle': cycler(color=colors)\n        \n})\nimport altair \naltair.themes.enable('carbonwhite')\nimport altair as alt\nalt.data_transformers.enable('default', max_rows=None)\nclear_output()\n\n\n\nDefine key variables and parameters\n\n\nShow the code\n# For reproducibility\nnp.random.seed(42)\n\n# A/B Test Parameters\nvisitors = 200\nconversions = 15\nobserved_conversion_rate = conversions / visitors\nhistorical_baseline = 0.08\n\n\n\n\n\nDefine Prior, likelihood, and Posterior in PyMC\nTo model this problem in PyMC, one must first define the Prior and Likelihood distributions\nPrior: Since the conversion rate \\theta, can only range between 0 and 1. The Beta distribution is ideal for modeling parameters that are bounded between 0 and 1, such as probabilities or rates.\nThe Beta distribution is controlled by two parameters, \\alpha and \\beta. hese parameters are set to formally encode the prior knowledge: the historical 8% conversion rate. The mean of a \\mathrm{Beta}(\\alpha,\\beta) distribution is \\frac{\\alpha}{ \\alpha + \\beta}. Since the historical rate is 8% (or 0.08), we need to choose \\alpha and \\beta such that: \n\\frac{\\alpha}{ \\alpha + \\beta} = 0.08\n\nTo determine the strength of this belief, a number that represents the effective sample size (ESS) of the historical knowledge is chosen. Choosing a hypothetical ESS of 100 trials, we can solve for \\alpha and \\beta: - ESS = \\alpha + \\beta = 100 - \\alpha (hypothetical successes) =100√ó0.08=8 - \\beta (hypothetical failures) =100‚àí8=92\nLikelihood the Likelihood is determined by the process that generated the data. Since there is a fixed number of trials (N=200 visitors) and the number of successes (k=15 conversions) is counted, this is a Binomial distribution.\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n    \n    # Sample from posterior distribution\n    trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True)\n\nclear_output()\n\n\nThis small block of code above defines and runs our entire Bayesian analysis. For those seeing PyMC for the first time, here is what each section is doing:\n\nwith pm.Model() as model: This block acts as a container for all the random variables and data in our model. Everything inside this context belongs to the conversion_model\nconversion_rate = pm.Beta(...): We are telling PyMC that the true conversion_rate is a random variable, and our initial belief is described by the Beta(8,92) distribution.\nlikelihood = pm.Binomial(...): This defines the process that generated our observed data. We link the conversion_rate parameter to the actual observed data (n=visitors, observed=conversions) using the appropriate Binomial distribution.\npm.sample(...): This is where the magic happens! The pm.sample function runs the MCMC sampler (the computational engine) to combine the Prior and the Likelihood, effectively calculating the Posterior distribution. We ask the sampler to draw 2000 samples after a 1000-sample tuning period, running 4 independent chains to ensure reliable results.\n\n\n\n\nModel diagnostics\nRunning pm.sample() generates the raw output, but the job isn‚Äôt done yet. Before we trust the results, we must perform Model Diagnostics to ensure our computational engine (the MCMC sampler) has worked correctly. The single most important diagnostic check is confirming Convergence.\n\nModel Convergencevergence\nIn Bayesian inference, Markov Chain Monte Carlo (MCMC) methods are employed to sample from the complex posterior distribution. These samples are relied upon to accurately estimate quantities like the mean conversion rate or its credible interval.\nConvergence is the guarantee that the MCMC chains have explored the entire distribution and are now producing samples that truly represent the target Posterior distribution, and are not just stuck in a starting location.\n\nAnalogy: Imagine trying to understand the shape of a deep, misty valley (the posterior). If your chains haven‚Äôt converged, they might be stuck high up on a ridge, missing the true, deep center. Diagnostics are the tools we use to confirm the chains have found and are walking across the bottom of the true valley.\n\nPyMC uses the supporting library ArviZ for standardizing and analyzing the results, which provides the following diagnostics and plots\n\nTrace Plots: Visual inspection of parameter samples across iterations.\n\nGood trace plots look like fuzzy caterpillars with no trends or jumps.\n\nR-hat (Gelman-Rubin Statistic): Measures how well multiple chains agree.\n\nR-hat ‚âà 1 means convergence.\nR-hat &gt; 1.01 suggests problems.\n\nEffective Sample Size (ESS): Indicates how many independent samples you effectively have.\n\nLow ESS means poor mixing or autocorrelation. Good ESS is typically &gt; 200 per parameter.\n\n\nThe most efficient way to check convergence numerically is using the ArviZ summary function, specifically asking for the diagnostics az.summary(trace, kind=\"diagnostics\"). Alternatively, az.plot_trace(trace) can be used to get a visual sense of convergence.\n\nExample Diagnostic Output\n\n\nShow the code\ndiag_table=az.summary(trace, kind=\"diagnostics\")[['ess_bulk', 'ess_tail', 'r_hat']]\nGT(diag_table).tab_header(\n    title=\"\",\n    subtitle=\"Conversion Rate Model\"\n).cols_label({\n        'ess_bulk': 'ESS Bulk',\n        'ess_tail': 'ESS Tail.',\n        'r_hat': 'R-hat',\n    })\n\n\n\n\n\n\n\n\n\n\n\nConversion Rate Model\n\n\nESS Bulk\nESS Tail.\nR-hat\n\n\n\n\n3966.0\n5559.0\n1.0\n\n\n\n\n\n\n\n\nThis table immediately indicates that the model is reliable and ready for analysis\n\n(R_hat = 1.0,Goal Achieved): Since R_hat is exactly 1.0, this confirms that the four independent MCMC chains have fully converged and agree on the shape of the posterior distribution. The model is reliable.\nESS bulk ‚Äãand ESS tail (3966.0 and 5559.0, Goal Achieved): Both effective sample sizes are significantly greater than the ‚â•400 minimum threshold. This means there are plenty of high-quality, effectively independent samples to accurately estimate the mean, mode, and credible intervals of the true conversion rate.\n\n\nVisual Check: Trace Plots\nWhile the numbers in the summary table are essential, visually inspecting the MCMC chains confirms the story.\n\n\nShow the code\naz.plot_trace(trace);\n\n\n\n\n\n\n\n\n\nThe trace plot above shows excellent convergence for our conversion rate (\\theta) model, supporting the conclusions from our quantitative diagnostics. The plot is split into two panels 1. Right Panel: MCMC Sampling Behavior\n\nThis panel shows the raw sampled values across iterations for each of our four chains. The sampled values oscillate stably around ‚àº0.075 (7.5%) without any noticeable trends, sudden jumps, or long-term drifts.\nThe different lines (chains) are thoroughly intertwined and overlap completely. This ‚Äúfuzzy caterpillar‚Äù appearance is the visual proof that the sampler is efficiently exploring the parameter space and that all chains have converged to the same distribution.\nThe stable behavior confirms the chain has reached stationarity, meaning it is now sampling from the true, converged Posterior distribution.\n\n\nLeft Panel: Posterior Distribution\n\nThis panel shows the estimated Posterior probability density function (PDF) based on the samples. The shape is smooth and unimodal (single peak), indicating a well-behaved posterior without ambiguity.\nThe peak is clearly centered at a value close to our observed rate (7.5%), which is what we expect when combining a prior (8%) and data (7.5%).\nThe spread of the distribution clearly visualizes our remaining uncertainty about the true conversion rate.\n\n\n\n\n\n\n\nPrior Predictive Checks: Validating Model Assumptions\nWhile we have demonstrated that the convergence of the fitted model a complete Bayesian analysis requires us to first validate the assumptions we made before seeing any data (priors). This validation is accomplished through the Prior Predictive Check.\n\nPrior Predictive Checks helps validate the prior assumptions before fitting the model to data. It show what kind of data the model expects to generate based solely on the prior beliefs.\n\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n\n\nGiven our prior belief of an 8% conversion rate, we can simulate what kind of data we would expect to see if this belief were true. This is done by generating synthetic datasets from the prior distribution and comparing them to the actual observed data. This is acomplished in PyMC by running pm.sample_prior_predictive() function.\n\n\nShow the code\nwith conversion_model:\n    prior_pred = pm.sample_prior_predictive()\n\n\n\n\nShow the code\nfig, ax=plt.subplots(figsize=(4, 2.8))\naz.plot_ppc(prior_pred, group=\"prior\", ax=ax)\nplt.xlabel('conversions')\nplt.ylabel('Density');\n\n\n\n\n\n\n\n\n\nThe generated plot from the prior predictive sampling shows the distributions of simulated data (the number of conversions) created by sampling from the \\text{Beta}(8,92) prior distribution. - The distribution of the simulated data (prior predictive line) appear broadly spread out and relatively flat across a wide range (0 to 40+ conversions). This indicates that the prior allows for many different outcomes, and thus it is not overly restrictive. - The dashed line represents the average predicted number of conversions It is also quite flat and non-committal. - It evident that the prior predictive distribution does not overly concentrate around any specific number of conversions, which is desirable when we want to remain open to various possible outcomes. - So the prior predictive check confirms that our chosen Beta(8,92) prior is reasonable though it also quite weakly informative how?\nRed flags to watch for:\n\nImpossible values: Predictions outside the feasible range (e.g., negative conversion, &gt;200 observations)\nUnrealistic concentrations: If priors are too informative, you might see all predictions clustered in a narrow range\nPoor scaling: Predictions that don‚Äôt match the scale of your problem\n\n\nPosterior Predictive Checks\nPosterior Predictive Checks addresses the important test: Does the model actually make sense given the data observed?\nThis moves the process from confirming the samplers or priors to validating the model itself.\n\nPPCs evaluate model fit by comparing the observed data to simulated data from the posterior distribution. If a model accurately represents the data-generating process, the simulated data should resemble the actual observations.\n\nTo achive this it is important to generate new, simulated data from the posterior distribution and compare it to the actual observed data. This is done using the pm.sample_posterior_predictive function in PyMC.\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n    \n    # Sample from posterior distribution\n    trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True)\n\n\nwith conversion_model:\n    posterior_pred = pm.sample_posterior_predictive(trace, random_seed=42,)\n    \nclear_output()\n\n\n\n\nShow the code\nppc_summary = az.summary(posterior_pred, kind='stats', hdi_prob=0.95)\nppc_table = (\n    GT(ppc_summary)\n    .cols_label({\n        'mean': 'Mean',\n        'sd': 'Std-Dev.',\n        'hdi_2.5%': 'HDI 3%',\n        'hdi_97.5%': 'HDI 97%'\n    }).tab_header(\n        title=\"Posterior Predictive Summary Statistics\",\n        subtitle=\"\"\n    )\n)\nppc_table \n\n\n\n\n\n\n\n\nPosterior Predictive Summary Statistics\n\n\n\n\n\nMean\nStd-Dev.\nHDI 3%\nHDI 97%\n\n\n\n\n15.314\n4.806\n6.0\n24.0\n\n\n\n\n\n\n\n\nIt clear that the predicted mean (15.314) is extremely close to the actual observed count (15). This is a strong indication that the conversion_rate model fits the data very well. And thus the choice of the Beta Prior and Binomial Likelihood is appropriate for this data. The model predicts that 95% of the time, the number of conversions will fall between 6 and 24. Since the company observed value of 15 falls well within this 95% HDI, the actual observation is considered highly plausible according to your model.\n\n\nShow the code\nppc_samples = posterior_pred.posterior_predictive['observations'].values.flatten()\npercentile = (ppc_samples &lt;= conversions).mean() * 100\nprint(f\"Observed convervation ({conversions}) is at the {percentile:.1f}th percentile of predictions\")\n\n\nObserved convervation (15) is at the 53.7th percentile of predictions\n\n\nThe value of 53.7 is very close to the ideal 50, which provides further strong evidence (in addition to the mean of 15.314 already seen) that:\n\nThe model is not biased (it is not systematically over- or under-estimating the data).\nThe choice of the Beta-Binomial model is highly appropriate for this data set.\n\n\n\n\nModel Utility and Business Decision\nFollowing the confirmation of the reliability of the Bayesian model through the Posterior Predictive Check (PPC), the focus now shifts from ‚ÄúDoes the model fit the data?‚Äù to the commercially critical question: ‚ÄúIs the model useful for making business decisions?‚Äù\nSpecifically, the derived posterior distribution is utilized to quantify the evidence that the new feature‚Äôs conversion rate (Feature B) is superior to the existing, established conversion rate (Feature A, which has a known baseline rate of 0.08).\nTo achieve this, we need to extract the posterior distribution for the conversion_rate parameter that your model estimated.\n\n\nShow the code\nposterior_samples = trace.posterior.stack(sample=(\"chain\", \"draw\"))\nconversion_samples=posterior_samples['conversion_rate'].values\n\n\nNext, calculate the ‚Äúprobability of superiority‚Äù‚Äîthe probability that the new feature (B) is better than the old (A).\nFinally, the expected uplift in conversion rate can be computed and translated into business value. This involves calculating the difference between the posterior mean conversion rate and the historical rate, then multiplying by the number of visitors and average revenue per conversion.\n\n0.95: Strong Evidence. The new feature is very likely superior. Launching it should be considered.\n0.80: Moderate Evidence. The new feature is likely better, but there is still a 20% chance it is worse. The decision depends on company risk tolerance.\n0.50: No Evidence. The new feature is a toss-up; there is no statistical reason to prefer it over the old feature.\n\n\n\nShow the code\nprob_superiority = (conversion_samples &gt; historical_baseline).mean()\n# Display the result\nprint(f\"The probability that the new feature is better than the old rate (0.08) is: {prob_superiority:.1%}\")\n\n\nThe probability that the new feature is better than the old rate (0.08) is: 39.7%\n\n\nThe calculated Probability of Superiority is approximately 0.40, indicating that there is a 40% chance the new feature‚Äôs conversion rate is better than the old feature‚Äôs with 8% rate. This imply that there is a 60.3% chance the new feature is worse than the baseline.\nSince the probability that the new feature is superior is well below the neutral benchmark of 50%, the data does not support replacing the existing Feature A with the new Feature B based on conversion rate alone. The new feature is highly likely to perform worse.\n\nProbability of Meaningful Improvement.\nDepending on the business context, one might also want to calculate the probability that the new feature is better by a meaningful margin (e.g., &gt;1% improvement). This calculates the chance that the new feature is better than the old feature by a practically significant margin of at least 1 percentage point. This is a crucial business metric. Sometimes a tiny statistical ‚Äúwin‚Äù is not worth the cost of development and deployment. If this probability is low, it confirms the feature is not a major improvement.\n\n\nShow the code\nprob_1pct_improvement = (conversion_samples &gt; historical_baseline+0.01).mean()\nprint(f\" Probability of &gt;1% improvement: {prob_1pct_improvement:.1%}\")\n\n\n Probability of &gt;1% improvement: 18.4%\n\n\n\n\nProbability of Hitting a Target Rate\nThis calculates the chance that the true conversion rate of the new feature is 10% or higher. This is useful if 10% is a specific, ambitious KPI (Key Performance Indicator) or goal set by the marketing or product team. It tells how likely the team is to meet its goal.\n\n\nShow the code\nprob_10pct = (conversion_samples &gt; 0.10).mean()\nprint(f\" Probability of &gt;10% improvement: {prob_10pct:.1%}\")\n\n\n Probability of &gt;10% improvement: 6.3%\n\n\n\nExpected Value of the Change.\nThis is the single-number best estimate of the average change (positive or negative) to expect upon deploying the new feature. This is the foundation for the ‚ÄúExpected Business Impact‚Äù section. If this value is negative, it represents an Expected Loss.\nIf one calculates: expected_uplift√óTotal¬†Visitors√óARPC, the estimated dollar value of the change is obtained.\nThe goal is to translate the statistically determined average change in conversion rate into a clear financial outcome for the business. This calculation provides the most actionable insight for the go/no-go decision on the new feature.\n\n\nShow the code\n# Calculate key business probabilities\nexpected_uplift = conversion_samples.mean() - historical_baseline\n\n# Calculate expected business impact\nmonthly_visitors = 10000\nexpected_additional_conversions = monthly_visitors * expected_uplift\nconversion_value = 50  # Average value per conversion\nexpected_monthly_value = expected_additional_conversions * conversion_value\n\nprint(f\"Expected uplift: {expected_uplift:.3f} ({expected_uplift:.1%} points)\")\nprint(f\"Expected monthly value: ${expected_monthly_value:,.0f}\")\n\n\nExpected uplift: -0.003 (-0.3% points)\nExpected monthly value: $-1,717\n\n\nWe see that the expected uplift is -0.030 (-3.0% points), indicating that, on average, the new feature is expected to decrease the conversion rate by 3 percentage points. The expected financial consequence of deploying the new feature is a loss of $1,717 per month\nThe data suggests that deploying the new feature would likely result in an Expected Monthly Loss of $1,717. This financial quantification is the most compelling reason to reject the new feature based on conversion rate performance.\nThe decision to proceed should only be considered if the feature provides other, unquantified benefits (e.g., improved customer retention, compliance, or brand value) that are estimated to be worth more than $1,717 per month.\n\n\n\nPrior sensitivity analysis\nAssessing how the choice of prior influences the final decision is important. This transparency is a key strength of Bayesian modeling, as it allows testing every assumption. Thus, one must determine how much the choice of prior affects the outcome by comparing several Beta priors. These comparisons will include priors from very optimistic (expecting a 15\\% conversion rate) to skeptical (expecting only 4\\%), plus an uninformative prior that lets the data speak entirely for itself.\n\n\nShow the code\n\npriors_to_test = [\n    (\"Very Optimistic\", 15, 85),    # Believes 15% conversion rate\n    (\"Optimistic\", 12, 88),         # Believes 12% conversion rate  \n    (\"Historical Based\", 8, 92),    # Our original prior (8%)\n    ('Strong Historical', 80, 920),      # Mean = 0.08, ESS = 1000\n    (\"Skeptical\", 4, 96),           # Believes 4% conversion rate\n    (\"Uninformative\", 1, 1),        # Let data dominate completely\n]\n\nsensitivity_results = []\ntraces_to_comprare = []\n\nfor idx, (prior_name, alpha, beta) in enumerate(priors_to_test):\n    with pm.Model() as model:\n        conversion_rate = pm.Beta(\"conversion_rate\", alpha=alpha, beta=beta)\n        obs = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n        trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True, progressbar=False)\n        \n \n    posterior_samples = trace.posterior.stack(sample=(\"chain\", \"draw\"))\n    conversion_samples=posterior_samples['conversion_rate'].values\n    expected_uplift = conversion_samples.mean() - historical_baseline\n    expected_additional_conversions = monthly_visitors * expected_uplift\n    expected_monthly_value = expected_additional_conversions * conversion_value\n\n\n    posterior_mean = conversion_samples.mean()\n    posterior_std = conversion_samples.std()\n    prior_std = np.sqrt((alpha * beta) / ((alpha + beta)**2 * (alpha + beta + 1)))\n    prob_better = (conversion_samples &gt; historical_baseline).mean()\n    uncertainty_reduction = max(0, (prior_std - posterior_std) / prior_std)\n    \n    df = pd.DataFrame({\"Prior\": prior_name,\n                       \"Posterior\": conversion_samples}\n                       )\n    \n    sensitivity_results.append({\n        'Prior_Belief': prior_name,\n        'Prior_Parameters': f\"Beta({alpha}, {beta})\",\n        'Prior_Mean': f\"{alpha/(alpha+beta):.3f}\",\n        'Posterior_Mean': f\"{posterior_mean:.3f}\",\n        'Prob_Better': f\"{prob_better:.1%}\",\n        \"Expected_uplift\": f\"${expected_uplift:.1%}\",\n        \"Expected_monthly_value\": f\"${expected_monthly_value:,.0f}\",\n        'Uncertainty_Reduction': f\"{uncertainty_reduction:.1%}\",\n        'Data_Influence': 'Strong' if uncertainty_reduction &gt; 0.8 else 'Moderate'\n    })\n    traces_to_comprare.append(df) \nsensitivity_df = pd.DataFrame(sensitivity_results)\ntraces_to_comprare=pd.concat(traces_to_comprare)\nclear_output()\n\n\n\nLet first visualise the different priors\n\n\nShow the code\ndomain = [\"Very Optimistic\", \"Optimistic\", \"Historical Based\", \"Strong Historical\", \"Skeptical\", \"Uninformative\"]\nreference_data = pd.DataFrame([\n    {'x': 0.08, 'label': 'Old Feature CR (8%)', 'color': 'red', 'label_x': 0.08},\n    {'x': 0.075, 'label': 'New Feature CR (7.5%)', 'color': 'green', 'label_x': 0.075}\n])\n\n\nchart = alt.Chart(traces_to_comprare).transform_density(density='Posterior', groupby=['Prior'], as_=['Posterior', 'density'])\\\n    .mark_line(opacity=0.5)\\\n    .encode(x='Posterior:Q', \n            y='density:Q', \n            color=alt.Color(\"Prior:N\").scale(domain=domain, range=colors[:len(domain)])\n            )\n\nreference_lines = alt.Chart(reference_data).mark_rule(\n    strokeDash=[5, 5], # Dashed line\n    size=1, \n).encode(\n    x='x:Q',\n    color=alt.Color('label:N', \n                    scale=alt.Scale(domain=reference_data['label'].tolist(), \n                                    range=reference_data['color'].tolist()),\n                    legend=alt.Legend(title=\"Conversion Rate\"))\n)\n\n\nfinal=chart + reference_lines \nfinal=final.properties(title='Posterior Distributions by Prior Belief', width=700, height=200).configure_axis(\n    # Set grid to False to remove all grid lines\n    grid=False\n).configure_view(\n    # Optional: Remove the surrounding border of the plot area\n    strokeWidth=0 \n)\nfinal.save('posterior_sensitivity.pdf')\nfinal\n\n\n\n\n\n\n\n\nThe figure above show Posterior belief distributions under different prior assumptions. Each curve in the figure represents a posterior distribution for the conversion rate under a different prior assumption. The horizontal axis shows possible conversion rates, and the vertical axis shows how plausible each value is after combining the prior belief with the observed data.\nDespite very different starting assumptions, the posterior estimates converge around 7‚Äì10%, showing that the data provide a stable, consistent signal largely independent of the chosen prior. All reasonable priors converge to the same conclusion: the new feature likely isn‚Äôt better than the old one.\nTo support decision-making, we compile a summary table comparing key metrics across these prior choices.\n\n\nShow the code\n# Create sensitivity analysis table\nsensitivity_table = (\n    GT(sensitivity_df)\n    .tab_header(\n        title=\"Sensitivity Analysis: Impact of Prior Beliefs\",\n        subtitle=\"How Different Starting Assumptions Affect Final Conclusions\"\n    )\n    .cols_label(\n        Prior_Belief=\"Prior Belief\",\n        Prior_Parameters=\"Prior Distribution\", \n        Prior_Mean=\"Prior Mean\",\n        Posterior_Mean=\"Posterior Mean\",\n        Prob_Better=\"Prob. Better\",\n        Data_Influence=\"Data Influence\",\n        Uncertainty_Reduction=\"Uncertainty Reduction\",\n        Expected_uplift=\"Expected Uplift\",\n        Expected_monthly_value=\"Expected Monthly Value\"\n    )\n    .data_color(\n        columns=[\"Prob_Better\"],\n        palette=[\"#C73E1D\", \"#F18F01\", \"#2E8B57\"],\n        domain=[0.0, 0.5, 1.0]\n    )\n    .data_color(\n        columns=[\"Data_Influence\"],\n        palette=[\"#2E8B57\", \"#F18F01\", \"#C73E1D\"],\n        domain=[\"Strong\", \"Moderate\", \"Weak\"]\n    ).data_color(\n        columns=[\"Uncertainty Reduction\"],\n        palette=[\"#C73E1D\", \"#F18F01\", \"#2E8B57\"],\n        domain=[0.0, 80, 100]\n    ).tab_source_note(\n        \"Analysis shows robustness of conclusions to different prior assumptions ‚Ä¢ \"\n        \"Even skeptical priors converge toward data-driven truth\"\n    )\n    .tab_options(\n        table_width=\"100%\",\n    )\n)\nclear_output()\nsensitivity_table\n\n\n\n\n\n\n\n\nSensitivity Analysis: Impact of Prior Beliefs\n\n\nHow Different Starting Assumptions Affect Final Conclusions\n\n\nPrior Belief\nPrior Distribution\nPrior Mean\nPosterior Mean\nProb. Better\nExpected Uplift\nExpected Monthly Value\nUncertainty Reduction\nData Influence\n\n\n\n\nVery Optimistic\nBeta(15, 85)\n0.150\n0.100\n88.2%\n$2.0%\n$9,930\n52.2%\nModerate\n\n\nOptimistic\nBeta(12, 88)\n0.120\n0.090\n71.0%\n$1.0%\n$4,992\n49.1%\nModerate\n\n\nHistorical Based\nBeta(8, 92)\n0.080\n0.077\n39.7%\n$-0.3%\n$-1,717\n44.1%\nModerate\n\n\nStrong Historical\nBeta(80, 920)\n0.080\n0.079\n47.3%\n$-0.1%\n$-263\n10.4%\nModerate\n\n\nSkeptical\nBeta(4, 96)\n0.040\n0.064\n11.8%\n$-1.6%\n$-8,231\n29.0%\nModerate\n\n\nUninformative\nBeta(1, 1)\n0.500\n0.079\n44.8%\n$-0.1%\n$-456\n93.5%\nStrong\n\n\n\nAnalysis shows robustness of conclusions to different prior assumptions ‚Ä¢ Even skeptical priors converge toward data-driven truth\n\n\n\n\n\n\n\n\n\nKey observations:\n\nPrior Pull: The data moderate extreme beliefs. The very optimistic prior (0.150) is pulled down to 0.100, while the skeptical prior (0.040) rises to 0.064‚Äîdemonstrating how new evidence shifts expectations toward a central value.\nPrior Strength vs.¬†Data Influence: The Strong Historical prior (Beta(80, 920)) and Uninformative prior (Beta(1, 1)) yield similar posteriors (~0.079, ~45% Prob_Better), but for opposite reasons. The former is dominated by prior belief; the latter, by data‚Äîshowing that strong evidence can overcome weak or missing priors.\nImpact on Decision Metrics:: The Very Optimistic prior suggests an 88% Prob_Better and +$9.9k expected value, favoring a launch. The Skeptical prior predicts only 12% Prob_Better and ‚Äì$8.2k, suggesting the opposite. Even moderate priors (e.g., 40‚Äì47%) could tip a go/no-go decision depending on the threshold.\n\n\n\n\nCommon bayesian pitfalls and best practices\nWhile Bayesian modeling provides a powerful framework for decision-making under uncertainty, several common mistakes can reduce its effectiveness.\n\nThe first is overconfident priors, where excessively strong prior assumptions prevent new data from influencing the results. This issue is best mitigated by starting with weak or moderately informative priors and refining them as more evidence becomes available.\n\n\nA second pitfall is ignoring prior sensitivity. Conclusions may shift significantly depending on the chosen prior distribution, so it is essential to conduct sensitivity analyses using multiple plausible priors to ensure that insights are robust.\n\n\nThe third major issue involves misinterpreting probability. A statement such as ‚Äú95% probability‚Äù does not imply absolute certainty; rather, it reflects the degree of belief given the available data and assumptions. Probabilities in Bayesian analysis are best interpreted in terms of relative confidence, risk, and decision trade-offs.\n\n\n\nNext steps in the bayesian journey\nWe have laid the groundwork by exploring the core components of the Bayesian approach. The subsequent session will focus on advanced practical implementation. We will apply this framework to critical survival analysis problems, modeling the probability and timing of events such as equipment failure.\n\n\nResources to continue learning\n\nBayesian Modeling and Computation in Python\nBayesian Methods for Hackers\nStatistical Rethinking\nDoing Bayesian Data Analysis\nBayesian Data Analysis\nPyMC Documentation\n\n\n\n\n\nCitationBibTeX citation:@online{faustine2025,\n  author = {Faustine, Anthony},\n  title = {Understanding {Bayesian} {Thinking} for {Industrial}\n    {Applications}},\n  date = {2025-10-10},\n  url = {https://sambaiga.github.io/pages/blog/posts/2025/10/2025-10-10-bayesion-foundation.html},\n  langid = {en}\n}"
  },
  {
    "objectID": "pages/blog/posts/2024/07/index.html",
    "href": "pages/blog/posts/2024/07/index.html",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "",
    "text": "Introduction\nEver been on a team where everything just clicks? communication is effortless, everyone knows their role, and the team anticipates each other‚Äôs moves, achieving goals with remarkable efficiency. This seamless collaboration is not a magic. It is often the result of something called a shared mental model, the invisible architecture that supports high-performance teamwork.\nThis post explores what shared mental models are, why they are a critical predictor of team effectiveness, and how leaders can consciously cultivate them, especially in the complex environments of virtual and hybrid teams.\n\n\nWhat is a Shared Mental Model?\nTeam effectiveness and coordination are critical aspects of a high-performance team. Team members who share a similar and organized understanding of team tasks and goals ‚Äî and who understand each other‚Äôs working environments ‚Äî are more likely to perform well.\nThis shared understanding and knowledge about the mission, goals, and other relevant environments among team members are called mental models. The shared mental model is one of the most frequently used concepts in team cognition (Schelble et al. 2022). (2022) further identify the shared mental model as a critical predictor of team effectiveness.\nLungeanu, DeChurch, and Contractor (2022) define shared mental models as team properties reflecting how team members organize knowledge and understanding about the team‚Äôs purpose, the nature of the work, and how they work together. Thus, team mental models are a collective mental representation among team members of how they interact in performing task-work (Larson and DeChurch 2020).\nThey represent the organized mental representations of the various component pieces relevant to a team‚Äôs overall task (Schelble et al. 2022). As (Schelble et al. 2022) point out, shared mental models measure whether or not team members share a common understanding of their shared tasks, roles, interdependencies, and strategies.\nSchelble et al. (2022) break shared mental models into two types:\n\nTask mental model ‚Äî covers aspects specific to understanding and completing a shared task.\n\nTeam mental model ‚Äî focuses on factors related to cooperation and communication within a team.\n\n\n\nThe Impact on Team Success\nShared mental models can continually develop over time, becoming more effective and influencing various team outcomes, such as objective performance, team viability, member well-being, and strategic alignment (Lungeanu, DeChurch, and Contractor 2022).\nTeams with shared mental models can recognize one another‚Äôs needs and information requirements (Lungeanu, DeChurch, and Contractor 2022; Schelble et al. 2022), which enhances coordination and mutual support.\nWhile this may be more intuitive in physical teams, virtual teams ‚Äî now an integral part of modern work ‚Äî require special attention in developing and maintaining shared mental models among members.\nUnlike face-to-face teams, creating and sustaining mental models is harder in virtual environments. Leaders must therefore compensate for challenges such as communication barriers and cultural differences. These issues can impact relationship building, which is essential for developing and sustaining shared mental models.\nAs underlined in (Larson and DeChurch 2020), face-to-face teams tend to have stronger shared mental models than virtual ones.\nTo improve team effectiveness and performance in virtual settings, leaders should aim to create a conducive environment for shared mental models. This can be achieved by:\n\nCultivating high-quality, interpersonal communication.\n\nCreating psychological safety.\n\nAdopting a leadership style that aligns well with virtual collaboration (Larson and DeChurch 2020).\n\n\n\nLeadership‚Äôs Role in Building a Shared Mind\nLungeanu, DeChurch, and Contractor (2022) highlight that leadership particularly shared leadership plays a crucial role in creating and shaping shared mental models in teams. This applies to both face-to-face and virtual teams.\nFor instance, (Lungeanu, DeChurch, and Contractor 2022) note that when leadership responsibilities are shared among members, the team tends to show greater commitment and information sharing. This dynamic fosters trust and enhances performance.\nTeams that embrace shared leadership and have diverse skills, experiences, and perspectives are more likely to develop and maintain strong shared mental models.\nFurthermore, connected leadership as opposed to fragmented leadership offers several advantages for improving similarity in team mental models. It promotes accuracy, synchronization of effort, and cohesion or trust.\nFor example, (Lungeanu, DeChurch, and Contractor 2022) argue that hierarchical and coordinated leadership are better at promoting shared mental models than factional or isolated forms of leadership. They also emphasize that boundaries among members of shared leadership groups are permeable, allowing reciprocal leadership processes that reduce conflict and tension.\n\n\nConclusion\nUltimately, a shared mental model is not just a nice-to-have; it‚Äôs the cognitive foundation upon which effective teams are built. It serves as the shared ‚Äúmap‚Äù that enables a group of individuals to navigate complex tasks together with clarity and confidence.\nWhile the rise of virtual work presents new challenges, the core principle remains: effective leadership is the catalyst. By fostering open communication, psychological safety, and a connected leadership structure, leaders can intentionally design the conditions for these powerful shared understandings to emerge and thrive.\n\n\n\n\n\nReferences\n\nLarson, Lindsay, and Leslie A. DeChurch. 2020. ‚ÄúLeading Teams in the Digital Age: Four Perspectives on Technology and What They Mean for Leading Teams.‚Äù The Leadership Quarterly 31 (1): 101377. https://doi.org/10.1016/j.leaqua.2019.101377.\n\n\nLungeanu, Alina, Leslie A. DeChurch, and Noshir S. Contractor. 2022. ‚ÄúLeading Teams over Time Through Space: Computational Experiments on Leadership Network Archetypes.‚Äù The Leadership Quarterly, January, 101595. https://doi.org/10.1016/j.leaqua.2021.101595.\n\n\nSchelble, Beau G., Christopher Flathmann, Nathan J. McNeese, Guo Freeman, and Rohit Mallick. 2022. ‚ÄúLets Think Together! Assessing Shared Mental Models, Performance, and Trust in Human-Agent Teams.‚Äù Proceedings of the ACM on Human-Computer Interaction 6 (GROUP): 1‚Äì29. https://doi.org/10.1145/3492832.\n\nCitationBibTeX citation:@online{faustine2024,\n  author = {Faustine, Anthony},\n  title = {Shared {Mental} {Models:} {The} {Invisible} {Architecture} of\n    {High-Performance} {Teams}},\n  date = {2024-07-12},\n  url = {https://sambaiga.github.io/pages/blog/posts/2024/07/},\n  langid = {en}\n}"
  },
  {
    "objectID": "pages/blog/index.html",
    "href": "pages/blog/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Featured\n            \n          \n\n          \n          \n            \n              \n              \n              \n            \n\n            \n            \n              \n                \n                  python\n                \n                  bayesian\n                \n                \n              \n            \n          \n\n          \n          \n            \n            \n              \n                \n                  \n                  Dec 17, 2025\n                \n              \n\n              \n\n              \n                \n                  \n                  Anthony Faustine\n                \n              \n            \n\n            \n            \n              Bayesian Regression: A Real-World Battery Degradation Case Study\n            \n\n            \n            \n              \n                Learn how Bayesian regression works in practice using PyMC, through a real-world battery‚Ä¶ \n                Read More \n              \n              \n            \n\n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n              \n              \n              \n            \n\n            \n            \n              \n                \n                  python\n                \n                  bayesian\n                \n                \n              \n            \n          \n\n          \n          \n            \n            \n              \n                \n                  \n                  Oct 10, 2025\n                \n              \n\n              \n\n              \n                \n                  \n                  Anthony Faustine\n                \n              \n            \n\n            \n            \n              Understanding Bayesian Thinking for Industrial Applications\n            \n\n            \n            \n              \n                Learn how Bayesian thinking can enhance decision-making in industrial applications. This article‚Ä¶ \n                Read More \n              \n              \n            \n\n            \n          \n        \n      \n        \n          \n          \n          \n\n          \n          \n            \n              \n              \n              \n            \n\n            \n            \n              \n                \n                  Team leadership\n                \n                  Leadership\n                \n                \n              \n            \n          \n\n          \n          \n            \n            \n              \n                \n                  \n                  Jul 12, 2024\n                \n              \n\n              \n\n              \n                \n                  \n                  Anthony Faustine\n                \n              \n            \n\n            \n            \n              Shared Mental Models: The Invisible Architecture of High-Performance Teams\n            \n\n            \n            \n              \n                Learn how shared mental models common understandings of goals, roles, and teamwork‚Äîform the‚Ä¶ \n                Read More \n              \n              \n            \n\n            \n          \n        \n      \n    \n  \n\n  \n  \n    \n  \n\nNo matching items"
  }
]