[
  {
    "objectID": "blog/2025/12/2025-12-1-bayesian-regression..html",
    "href": "blog/2025/12/2025-12-1-bayesian-regression..html",
    "title": "Bayesian Regression: A Real-World Battery Degradation Case Study",
    "section": "",
    "text": "Predictive modeling in industrial settings is rarely just about accuracy. Decisions informed by models often carry financial, safety, and operational risks. In such environments, understanding uncertainty can be just as important as generating a good point prediction.\nThis article offers a practical introduction to Bayesian regression through a real-world case study: predicting lithium-ion battery degradation. Instead of treating model parameters as fixed, unknown values, the Bayesian approach frames them as probability distributions‚Äîexplicitly modeling uncertainty to give engineers and data scientists a richer, more actionable understanding of system behavior.\n\nüõ†Ô∏è Action: If you prefer to learn by doing, you can reproduce this article using the accompanying resources:\n\n\nRepository: Fork the bayesian-modelling repository and follow the setup instructions in the README.md\nNotebook: Launch 2025-12-1-bayesian-regression.ipynb in the notebook folder to follow along step-by-step.\n\nThis post is part of the Bayesian Modelling for Industrial Applications series ‚Äì Part 2 . If you are new to Bayesian methods, you may want to start with the first post Part 1, which introduces the foundational concepts of Bayesian inference.\n\n\nIntroduction\nWelcome back to our series on Bayesian Modelling for Industrial Applications. In Part 1, we explored how Bayesian thinking provides a principled framework for decision-making under uncertainty when evidence is limited.\nIn this post, we extend that foundation to continuous prediction problems, showing how Bayesian regression transforms noisy industrial data into actionable insights with uncertainty explicitly quantified rather than ignored.\n\nWhy Bayesian regression?\nImagine managing a fleet of electric vehicles. One of your biggest challenges is predicting battery State of Health (SoH) over time. Early predictions inform warranty decisions, maintenance planning, and safety margins.\nIndustrial systems, like vehicle battery packs, are inherently complex: they are characterized by noisy, non-repeatable measurements due to sensor noise, unit variability, and stochastic physical processes.\nTraditional, purely deterministic regression methods (which produce a single ‚Äúbest-fit‚Äù curve) fail to capture this complexity. A deterministic model might predict one degradation curve, but in practice, real batteries age differently even under similar conditions. This reliance on a single point estimate overlooks critical, high-risk aspects of the industrial data:\n\nMeasurement uncertainty inherent in sensors and data acquisition systems\n\nUnit-to-unit variability in components (e.g., subtle differences between nominally identical batteries)\n\nRandom and nonlinear degradation behaviour\n\nLimited early-life observations, a common constraint in industrial testing\n\nBayesian regression addresses this reality by treating uncertainty as a first-class citizen. Instead of delivering a single prediction, it provides ranges of plausible outcomes, allowing decisions to be made with risk explicitly accounted for.\n\n\nModeling distributions with bayes‚Äô theorem\nBayesian regression models uncertainty by treating parameters as probability distributions rather than fixed values. Each coefficient represents a range of plausible effects, informed by both prior knowledge and observed data.\nThis framework is built on three core components:\n\nPriors, which encode existing engineering knowledge or physical constraints\n\nLikelihood, which links the model to noisy real-world measurements\n\nPosterior, which combines prior information and data into an updated belief\n\nThe posterior distribution enables credible intervals, allowing us to quantify how confident we are in both parameter estimates and future predictions‚Äîan essential capability in industrial decision-making.\n\n\n\nCase Study: Predicting battery degradation\nLithium-ion batteries are critical components in electric vehicles and stationary energy storage systems. Unexpected capacity loss can lead to service interruptions, safety risks, and costly premature replacements.\nThe challenge is predicting future battery health when: - Direct capacity measurements are infrequent and expensive - Early-life data is sparse - Degradation accelerates nonlinearly near end of life\nThe goal is not only to predict degradation, but to quantify uncertainty well enough to support maintenance and replacement decisions.\nWe use battery degradation data from the CALCE Battery Research Group at the University of Maryland. Battery State of Health (\\(\\text{SoH}\\)) is defined as current capacity relative to initial capacity (\\(\\text{SoH} = C/C_{max}\\)). This continuous value degrades non-linearly over the battery‚Äôs life, driven primarily by cycle count and operational conditions, with significant measurement noise. The CALCE dataset provides over 1,200 capacity measurements taken at discrete cycle intervals, alongside features like charging and discharge current and voltage. This comprehensive dataset has become a benchmark in battery research, allowing rigorous comparison of degradation models\n\n\nShow the code\nimport arviz as az\nfrom great_tables import GT, loc, md, style\nfrom IPython.display import clear_output\nfrom lets_plot import (\n    LetsPlot,\n    aes,\n    coord_cartesian,\n    facet_wrap,\n    flavor_high_contrast_dark,\n    geom_area,\n    geom_band,\n    geom_density,\n    geom_histogram,\n    geom_line,\n    geom_point,\n    geom_ribbon,\n    geom_vline,\n    gggrid,\n    ggplot,\n    ggsize,\n    guide_legend,\n    guides,\n    labs,\n    layer_tooltips,\n    scale_color_brewer,\n    scale_color_manual,\n    scale_fill_manual,\n    scale_y_continuous,\n)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nfrom sklearn.preprocessing import  StandardScaler\n\naz.style.use(\"arviz-doc\")\n\nfrom bayes.plot.basic_plots import line_plot, modern_theme, pro_colors, scatter_plot\n\nLetsPlot.setup_html(isolated_frame=False, offline=True, no_js=True, show_status=False)\nnp.random.seed(42)\n\n\n\nChoosing the Beta Likelihood\nThe capacity data being modeled, \\(C\\), represents the battery‚Äôs health and is strictly bounded between zero and its initial (maximum) capacity, \\(C_{\\text{max}}\\). The goal of this analysis is to model the evolution of \\(C\\). In many standard regression approaches, the model‚Äôs likelihood function (which defines the distribution of the noise) is assumed to be Gaussian (Normal). This assumption is fundamentally incompatible with the physical reality of capacity degradation for two key reasons.\n\nGaussian models assume the target variable can take any real value (\\(-\\infty\\) to \\(+\\infty\\)), ignoring the fact that the underlying capacity \\(C\\) cannot fall outside their physical limits (i.e., \\([C_{\\text{min}}, C_{\\text{max}}]\\) )\nWith enough extrapolation, Gaussian models produce impossible values (e.g.,negative capacity). In safety-critical systems, such predictions are dangerous.\n\nThe Beta distribution is consequently chosen as the likelihood function for this regression problem. The Beta distribution is flexible, capable of modeling various shapes (uniform, U-shaped, skewed) depending on its shape parameters (\\(\\alpha\\) and \\(\\beta\\)).Furthermore, since battery capacity degradation is continuous and strictly bounded by \\([C_{\\text{min}}, C_{\\text{max}}]\\), a Beta likelihood provides a natural modeling choice that avoids the ad-hoc truncation required by Gaussian assumptions.\nTo fit the Beta‚Äôs intrinsic \\([0, 1]\\) domain, capacity (\\(C_i\\)) is first scaled into a normalized State of Health (\\(\\tilde{C}_i\\)) using the following transformation:\\[\\tilde{C}_i = \\frac{C_i - C_{\\text{min}}}{C_{\\text{max}} - C_{\\text{min}}}\\]The model then utilizes the Beta distribution for the scaled capacity:\\[\\tilde{C}_i \\sim \\text{Beta}(\\alpha_i, \\beta_i)\\]where \\(\\alpha_i, \\beta_i &gt; 0\\) and \\(\\tilde{C}_i \\in [0, 1]\\). This modeling choice ensures all predicted \\(\\tilde{C}_i\\) values remain physically plausible while allowing for flexible modeling of degradation patterns through the shape parameters.\nwith pm.Model() as battery_model:\n    pm.Beta(\"y_obs\", alpha=alpha, beta=beta_shape, observed=y_data)\n\nParameterization: Mean (\\(\\mu\\)) and Precision (\\(\\phi\\))\nTo make the parameters intuitive, the Beta distribution is typically reparameterized using the mean (\\(\\mu\\)) and the precision (\\(\\phi\\)).The shape parameters, \\(\\alpha_i\\) and \\(\\beta_i\\), which define the exact shape of the distribution for a given observation, are calculated directly from the mean \\(\\mu_{i} \\in (0, 1)\\) and the global precision \\(\\phi &gt; 0\\) such that:\\[\\alpha_i = \\mu_{i} \\cdot \\phi \\quad \\text{and} \\quad \\beta_i = (1 - \\mu_{i}) \\cdot \\phi\\] The precision parameter, \\(\\phi\\), controls the variance: a large \\(\\phi\\) means the predictions are tightly clustered around the mean \\(\\mu_{i}\\), indicating low uncertainty (low variance).\nwith pm.Model() as battery_model:\n    alpha = mu_scaled * phi\n    beta_shape = (1 - mu_scaled) * phi\nThe mean parameter \\(\\mu_{i} \\in (0, 1)\\) must be linked to our predictors. Since the mean is bounded by \\((0, 1)\\), we use the Logit Link Function to map the linear combination of predictors (\\(\\eta_i\\)) to this interval: \\[\\text{logit}(\\mu_{i}) = \\eta_i\\] as such , \\[\\mu_{i} = \\text{logit}^{-1}(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}\\]\nwith pm.Model() as battery_model:\n    mu_scaled = pm.Deterministic(\"mu_scaled\", pm.math.sigmoid(logit_mu))\n\nüí° Key Takeaway: The Logit Link function is the mathematical bridge that ensures our mean prediction, \\(\\mu_i\\), respects the physical boundary of \\((0, 1)\\) imposed by the Beta distribution\n\n\n\nThe Linear Predictor: Capturing Degradation\nThe core of our predictive power lies in the linear predictor, \\(\\eta_i\\). It is structured to incorporate both the fundamental, non-linear degradation due to cycling and the linear operational effects from features \\(\\mathbf{x}\\) like voltage and current: \\[\\eta_i = \\underbrace{\\beta_0}_{\\text{Intercept}} + \\underbrace{f(k_i)}_{\\text{Non-linear Decay}} + \\underbrace{\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}}_{\\text{Operational Effects}}\\]\nIn this work, the non-linear decay component is modeled as an exponential function: \\[f(k_i) = -A\\cdot(1-e^{-\\lambda \\cdot k_i})\\] where \\(k_i\\) is the cycle count, \\(\\lambda\\) is the degradation rate, and \\(A&gt;0\\) is a learnable amplitude parameter controlling the strength of decay. This captures the physical reality that battery capacity degrades rapidly at first and then more slowly over time. where:\nwith pm.Model() as battery_model:\n    degradation = pm.math.exp(-lambda_rate * cycle_data)\n    degradation_term = -degr_amp * (1 - degradation)\n    logit_mu = intercept + degradation_term + pm.math.dot(x_data, beta)\nThe components of \\(\\eta_i\\) are as follows:\n\nIntercept (\\(\\beta_0\\)): The baseline capacity on the logit scale when operational effects are zero and the cycle count (\\(k_i\\)) is zero.\nDegradation Term (\\(e^{-\\lambda \\cdot k_i}\\)): This is the non-linear exponential decay over the cycle count \\(k_i\\), controlled by the rate \\(\\lambda\\). This term ensures the capacity prediction naturally trends downward toward zero capacity over time.\nOperational Effects (\\(\\mathbf{x}_{i}^{\\top} \\boldsymbol{\\beta}\\)): This is a standard linear combination, where \\(\\boldsymbol{\\beta}\\) is the vector of coefficients for the standardized operational features \\(\\mathbf{x}_{i}\\).\n\nThis models how factors like maximum temperature accelerate or slow down the degradation.\n\nüß† Self-Test: You are modeling \\(\\text{SoH}\\), which must stay in \\([0, 1]\\). Your linear predictor, \\(\\eta_i = \\beta_0 + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\), can produce values ranging from \\(-\\infty\\) to \\(+\\infty\\).What would happen if you skipped the Logit Link Function and simply set \\(\\mu_i = \\eta_i\\)? Why is the Logit Link function mandatory for the Beta regression model?\n\n\n\n\nEncoding Knowledge with Priors\nIn Bayesian modeling, defining priors is a critical step. This step allows domain knowledge accumulated from battery engineering to be embedded directly into the model, ensuring that predictions remain physically plausible even when data is sparse. A prior distribution is assigned to every unknown parameter (\\(\\beta_0, \\boldsymbol{\\beta}, \\lambda, \\phi\\)). These priors act as soft constraints, preventing the model from learning extreme or non-physical relationships.\nThe Intercept (\\(\\beta_0\\))\nThe Intercept \\(\\beta_0\\) represents the initial capacity of the battery fleet on the logit scale. The orange curve in the figure below represents the selected informative prior, \\(\\text{Normal}(\\mu_{\\text{logit\\_start}}, 0.5^2)\\). A standard deviation of \\(\\sigma = 0.5\\) is chosen to balance prior knowledge (centering at \\(\\mu_{\\text{logit\\_start}}\\)) with sufficient uncertainty to allow the observed data to meaningfully influence the final estimate.\nwith pm.Model() as battery_model:\n    eps=1e-8\n    initial_logit_capacity_mean = -np.log(1-eps)\n    intercept = pm.Normal(\"intercept\", mu=initial_logit_capacity_mean, sigma=0.5)\n\n\nShow the code\nfrom bayes.plot.distribution import plot_density\n\n\n\n\nShow the code\nn = 1000\ns1 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.1), n)\ns2 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.2), n)\ns3 = pm.draw(pm.Normal.dist(mu=0.28, sigma=0.5), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=0.28, œÉ=0.1)\", \"(Œº=0.28, œÉ=0.2)\", \"(Œº=0.28, œÉ=0.5)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"Normal Distributions Intercept Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -2\n              \n            \n          \n          \n            \n            \n            \n              \n                -1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                -1\n              \n            \n          \n          \n            \n            \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Normal Distributions Intercept Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.2)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0.28, œÉ=0.5)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThe narrower blue (\\(\\sigma = 0.1\\)) and green (\\(\\sigma = 0.2\\)) curves represent highly concentrated priors that would strongly restrict the posterior estimates. The wider \\(\\sigma = 0.5\\) (orange) distribution corresponds to a more conservative informative prior, granting the initial capacity estimate \\(\\beta_0\\) a reasonable degree of uncertainty.\nOperational Effects (\\(\\boldsymbol{\\beta}\\))\nThe vector of coefficients \\(\\boldsymbol{\\beta}\\) controls the influence of operational features on capacity fade. Engineering knowledge suggests that, unless a feature is extreme, its immediate effect on capacity should be subtle, as the overall degradation process is primarily driven by cycle count.\n    with pm.Model() as battery_model:\n    beta = pm.Normal(\"beta\", mu=0, sigma=0.2, shape=n_features)\n\n\nShow the code\nfrom bayes.plot.distribution import plot_density\n\n\n\n\nShow the code\n\ns1 = pm.draw(pm.Normal.dist(mu=0, sigma=0.1), n)\ns2 = pm.draw(pm.Normal.dist(mu=0, sigma=0.2), n)\ns3 = pm.draw(pm.Normal.dist(mu=0, sigma=1.0), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=0, œÉ=0.1)\", \"(Œº=0, œÉ=0.2)\", \"(Œº=0, œÉ=1.0)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"Normal Distributions Beta Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -4\n              \n            \n          \n          \n            \n            \n            \n              \n                -3\n              \n            \n          \n          \n            \n            \n            \n              \n                -2\n              \n            \n          \n          \n            \n            \n            \n              \n                -1\n              \n            \n          \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Normal Distributions Beta Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=0.2)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=0, œÉ=1.0)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nAs shown in the figure above, a tight informative prior, \\(\\text{Normal}(0, 0.2^2)\\), is used for \\(\\boldsymbol{\\beta}\\). Centering this prior at zero reflects the assumption that, on average, operational features have no effect, while the small standard deviation (\\(0.2\\)) requires strong evidence from the data before attributing a large effect to any single feature. This constraint prevents non-physical, abrupt changes in capacity predictions. In contrast, a broader prior such as \\(\\text{Normal}(0, 1.0^2)\\) (orange curve) allows extreme effects that are considered non-physical.\nDegradation rate \\(\\lambda\\)\nThe degradation rate \\(\\lambda\\) governs the exponential decay term \\(e^{-\\lambda k_i}\\). Since degradation must always occur and capacity cannot increase indefinitely, it is necessary to enforce \\(\\lambda &gt; 0\\). Accordingly, a Log-Normal prior, \\(\\text{LogNormal}(\\ln(0.005), 0.5^2)\\), is used for \\(\\lambda\\).\nwith pm.Model() as battery_model:\n    lambda_rate = pm.Lognormal(\"lambda_rate\", mu=np.log(0.01), sigma=0.5)\n\nüß† Self-Test: Recall that we set the prior for the fade rate \\(\\lambda\\) as \\(\\text{LogNormal}(\\ln(0.01), 0.5^2)\\) (where \\(\\sigma = 0.5\\)). What practical problem would arise if an engineer, overly confident in their historical knowledge, reset the prior to \\(\\text{LogNormal}(\\ln(0.01), 0.1^2)\\) (where \\(\\sigma = 0.1\\))?\n\n\n\nShow the code\n\ns1 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=0.1), n)\ns2 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=0.5), n)\ns3 = pm.draw(pm.LogNormal.dist(np.log(0.005), sigma=1.0), n)\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"(Œº=In(0.005), œÉ=0.1)\", \"(Œº=In(0.005), œÉ=0.5)\", \"(Œº=In(0.005), œÉ=1.0)\"], n),\n    }\n)\n\nplot=plot_density(df, title=\"LogNormal Distributions Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.05\n              \n            \n          \n          \n            \n            \n            \n              \n                0.1\n              \n            \n          \n          \n            \n            \n            \n              \n                0.15\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.25\n              \n            \n          \n          \n            \n            \n            \n              \n                0.3\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                200\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n          \n            \n              \n                600\n              \n            \n          \n          \n            \n              \n                800\n              \n            \n          \n        \n      \n    \n    \n      \n        LogNormal Distributions Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=0.1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=0.5)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                (Œº=In(0.005), œÉ=1.0)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThis weakly informative prior centers the expected degradation rate around \\(\\mathbf{0.5\\%}\\), while the spread \\(\\sigma = 0.5\\) (green/teal curve) is sufficiently wide to accommodate realistic fleet-level variability. At the same time, it remains substantially tighter than \\(\\sigma = 1.0\\) (orange curve), thereby avoiding non-physical probability mass assigned to unrealistically large degradation rates.\nThis distribution reflects a conservative estimate of uncertainty, allowing greater variation in degradation behavior than a tighter prior (e.g., \\(\\sigma = 0.1\\)) would permit, while still preventing implausible rates.\nDegradation Amplitude (\\(A\\))\nThe parameter degr_amp (\\(A\\)) controls the overall amplitude of the degradation component. Since this amplitude must be non-negative, a Half-Normal distribution is used, which has support only on positive values. The scale parameter \\(\\sigma\\) determines the strength of regularization.\n   with pm.Model() as battery_model:\n    degr_amp = pm.HalfNormal(\"degr_amp\", sigma=0.1)\n\n\nShow the code\n\ns1 = pm.draw(pm.HalfNormal.dist(sigma=0.1), n)\ns2 = pm.draw(pm.HalfNormal.dist(sigma=0.2), n)\ns3 = pm.draw(pm.HalfNormal.dist(sigma=0.5), n)\n\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"œÉ=0.1\", \"œÉ=0.2\", \"œÉ=0.5\"], n),\n    }\n)\nplot=plot_density(df, title=\"Gamma Distributions Phi Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.5\n              \n            \n          \n          \n            \n            \n            \n              \n                2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n          \n            \n              \n                6\n              \n            \n          \n        \n      \n    \n    \n      \n        Gamma Distributions Phi Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.1\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.2\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                œÉ=0.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nAs shown in the figure above, \\(\\text{HalfNormal}(\\sigma = 0.1)\\) strongly concentrates probability mass near zero, requiring substantial evidence before attributing a large degradation amplitude. In contrast, broader priors such as \\(\\text{HalfNormal}(\\sigma = 0.5)\\) place non-negligible probability on large, non-subtle amplitudes (up to approximately \\(1.0\\)), increasing the risk of overfitting by allowing the model to explain noise through the amplitude term.\nPrecision Parameter (\\(\\phi\\))\nThe precision parameter \\(\\phi\\) controls the variance of the Beta likelihood and represents the expected level of noise in the \\(\\text{SoH}\\) measurements. Accordingly, a highly informative Gamma prior, \\(\\text{Gamma}(100, 2)\\), is assigned to \\(\\phi\\).\nwith pm.Model() as battery_model:\n    phi = pm.Gamma(\"phi\", alpha=100, beta=2.0)\nThis prior is centered at \\(\\mathbb{E}[\\phi] = \\alpha / \\beta = 50\\) with a relatively small standard deviation (\\(\\sigma_{\\phi} = 5.0\\)), indicating high confidence in this expectation. This choice encodes the belief that sensor noise is low (\\(\\sigma_{\\text{noise}} \\approx 0.14\\)), reflecting the physical reality of precise laboratory-grade measurements.\n\n\nShow the code\n\ns1 = pm.draw(pm.Gamma.dist(alpha=10, beta=1), n)\ns2 = pm.draw(pm.Gamma.dist(alpha=50, beta=5), n)\ns3 = pm.draw(pm.Gamma.dist(alpha=100, beta=2), n)\n\n\ndf = pd.DataFrame(\n    {\n        \"value\": np.concatenate([s1, s2, s3]),\n        \"distribution\": np.repeat([\"Gamma(Œ±=10, Œ≤=1)\", \"Gamma(Œ±=50, Œ≤=5)\", \"Gamma(Œ±=100, Œ≤=2)\"], n),\n    }\n)\nplot=plot_density(df, title=\"Gamma Distributions Phi Priors\", fig_size=(500, 400))\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                10\n              \n            \n          \n          \n            \n            \n            \n              \n                20\n              \n            \n          \n          \n            \n            \n            \n              \n                30\n              \n            \n          \n          \n            \n            \n            \n              \n                40\n              \n            \n          \n          \n            \n            \n            \n              \n                50\n              \n            \n          \n          \n            \n            \n            \n              \n                60\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.05\n              \n            \n          \n          \n            \n              \n                0.1\n              \n            \n          \n          \n            \n              \n                0.15\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.25\n              \n            \n          \n        \n      \n    \n    \n      \n        Gamma Distributions Phi Priors\n      \n    \n    \n      \n        Empirical Density Estimates\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Value\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=10, Œ≤=1)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=50, Œ≤=5)\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                Gamma(Œ±=100, Œ≤=2)\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nFrom the figure above, it is evident that \\(\\text{Gamma}(\\alpha = 100, \\beta = 2.0)\\) (orange curve) provides a strong belief in high precision. In contrast, \\(\\text{Gamma}(\\alpha = 10, \\beta = 1.0)\\) yields a lower expected precision with greater spread, allowing excessive uncertainty and risking a flat, unphysical prior predictive distribution. Alternative Gamma priors with the same expected precision but larger variance similarly underestimate the precision of modern sensors.\nThe complete model now combines all these components:\n\n\nShow the code\ndef beta_regression_model(\n    data: pd.DataFrame,\n    features: list[str],\n    target: str = \"capacity\",\n    scaler: StandardScaler | None = None,\n    lower_bound: float = 0.2,\n    upper_bound: float = 1.3,\n    eps: float = 1e-8,\n) -&gt; tuple[pm.Model, StandardScaler]:\n    \"\"\"Beta regression model for bounded battery capacity data using PyMC.\n\n    Capacity (SoH) is scaled to the (0, 1) interval for the Beta distribution.\n\n    Args:\n        data: DataFrame containing 'capacity', 'cycle', and feature columns.\n        features: List of column names used as predictors (X variables).\n        target: Name of the capacity column.\n        scaler: Pre-fitted StandardScaler object, or None to fit a new one.\n        lower_bound: Physical lower bound for capacity (for scaling).\n        upper_bound: Physical upper bound for capacity (for scaling).\n        eps: Small value to avoid boundary issues in Beta distribution.\n\n    Returns:\n        A tuple containing the PyMC model and the fitted/provided StandardScaler.\n    \"\"\"\n    # 1. Prepare Features (X)\n    if scaler is None:\n        scaler = StandardScaler()\n        x_scaled = scaler.fit_transform(data[features])\n    else:\n        x_scaled = scaler.transform(data[features])\n\n    # 2. Prepare Targets (Y)\n    y = data[target].values.astype(np.float64)\n    cycles = data[\"cycle\"].values.astype(np.float64)\n    n_features = len(features)\n\n    # Transform y to (0,1) interval and clip to avoid boundaries (0 or 1)\n    y_scaled = (y - lower_bound) / (upper_bound - lower_bound)\n    y_scaled = np.clip(y_scaled, eps, 1 - eps)\n\n    with pm.Model() as model:\n        # Data Containers\n        x_data = pm.Data(\"x_data\", x_scaled)\n        cycle_data = pm.Data(\"cycle_data\", cycles)\n        y_data = pm.Data(\"y_data\", y_scaled)\n\n        # Priors\n        initial_logit_capacity_mean = -np.log(1 - 1e-6)\n        intercept = pm.Normal(\"intercept\", mu=initial_logit_capacity_mean, sigma=0.5)\n        lambda_rate = pm.Lognormal(\"lambda_rate\", mu=np.log(0.005), sigma=0.5)\n        beta = pm.Normal(\"beta\", mu=0, sigma=0.2, shape=n_features)\n        phi = pm.Gamma(\"phi\", alpha=100, beta=2.0)\n        degr_amp = pm.HalfNormal(\"degr_amp\", sigma=0.1)\n\n        # Linear predictor (eta) on logit scale\n        degradation = pm.math.exp(-lambda_rate * cycle_data)\n        degradation_term = -degr_amp * (1 - degradation)\n        logit_mu = intercept + degradation_term + pm.math.dot(x_data, beta)\n\n        # Convert to probability scale (0,1)\n        mu_scaled = pm.Deterministic(\"mu_scaled\", pm.math.invlogit(logit_mu))\n\n        # Beta likelihood\n        alpha = mu_scaled * phi\n        beta_shape = (1 - mu_scaled) * phi\n        pm.Beta(\"y_obs\", alpha=alpha, beta=beta_shape, observed=y_data)\n\n        # Transform mu back to original scale\n        mu_original = pm.Deterministic(\"mu_original\", mu_scaled * (upper_bound - lower_bound) + lower_bound)\n\n        pm.Deterministic(\"capacity_pred\", mu_original)\n        pm.Deterministic(\"feature_effects\", beta)\n\n    return model, scaler\n\n\n\n\n\nTranslating raw battery data to diagnostics features\nIn the preceding sections, the output side of the Bayesian model was rigorously defined, including the Beta likelihood, the Logit link function, and physics-informed priors for the parameters (\\(\\beta_0, \\boldsymbol{\\beta}, \\lambda, \\phi\\)). However, the quality of the resulting predictions depends critically on the quality of the input features (\\(\\mathbf{x}\\)) that drive the degradation term (\\(\\eta_i = \\dots + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\)).\nRaw capacity measurement curves are noisy and variable. Therefore, before proceeding to Bayesian sampling, it is necessary to dedicate a structured process to translating real-world operational data into robust, physically meaningful diagnostic features.\nThis motivates the crucial step of feature engineering.\n\nData alignment and cleaning\nBefore extracting diagnostic features, a uniform time base must be established, as the formulas used for feature extraction require comparable voltage and current values across cycles.\n\nCycle Alignment (Standardization): Linear interpolation is used to resample all voltage and current time-series arrays to a uniform length (e.g., 500 points). This standardization enables direct cycle-to-cycle comparison, as illustrated by the transition from the raw data (Figures 1 and 2) to the interpolated curves (Figures 3 and 4).\n\n\n\n\n\nFigure 1: Charging Voltage curve at the beginning of life\n\n\n\n\n\nFigure 2: Discharge Voltage curve at the beginning of life\n\n\n\nAs shown in Figures 1 and 2, voltage curves differ in length due to variations in charge and discharge durations. Linear interpolation standardizes these curves to a fixed length (e.g., 500 points), enabling direct comparison across cycles, as illustrated in Figures 3 and 4.\n\n\n\n\nFigure 3: Inteporated Charging Voltage curve at the beginning of life\n\n\n\n\n\nFigure 4: Inteporated Discharge Voltage curve at the beginning of life\n\n\n\n\nData Filtering: Cycles exhibiting non-meaningful behavior (e.g., flat voltage profiles, excessive noise, or unrealistic starting or peak voltages) are removed to ensure that all inputs correspond to valid charging or discharging events.\n\n\n\nDiagnostic Feature Extraction\nWith aligned and cleaned curves, it is now possible to reliably extract cycle-specific diagnostic features that quantify the battery‚Äôs underlying physical degradation processes. These features are sensitive to aging mechanisms such as active material loss and internal resistance growth\n\nüß† Reflection: We chose to derive these physically meaningful features instead of feeding the entire, aligned time-series data (Figures 3 & 4). Why are these manually engineered features often preferred in industrial applications? Consider the trade-offs in model complexity, training speed, and the crucial interpretability of the final Bayesian coefficients (\\(\\beta\\)).\n\n\n\nShow the code\n\n\n# Fix the typo and create data\ndata = pd.DataFrame({\n    \"Diagnostic Feature\": [\n        \"Voltage Gap\",\n        \"Voltage Hysteresis\", \n        \"IC Peak Metrics\",\n        \"Hysteresis Proxy Resistance\"\n    ],\n    \"Formula\": [\n        \"ŒîVÃÑ = VÃÑ_c - VÃÑ_d\",\n        \"ŒîV(x) = V_c(x) - V_d(x)\",\n        \"IC = dQ/dV\",\n        \"R_proxy ‚àù ŒîV(x)/I_diff\"\n    ],\n    \"Physical Meaning\": [\n        \"Average polarization; quantifies internal losses\",\n        \"Loss mechanisms at specific state-of-charge\",\n        \"Phase transitions; indicates active material loss\",\n        \"Proxy for internal resistance growth\"\n    ]\n})\n\n\n# Create publication-quality table\ntable = (\n    GT(data)\n    .tab_header(\n        title=md(\"**Table 1: Battery Degradation Diagnostic Features**\"),\n        subtitle=\"Mathematical definitions and physical interpretations of key battery health indicators\"\n    )\n    .cols_label(\n        **{\n            \"Diagnostic Feature\": md(\"**Diagnostic Feature**\"),\n            \"Formula\": md(\"**Formula**\"),\n            \"Physical Meaning\": md(\"**Physical Meaning**\")\n        }\n    )\n    .tab_options(\n        table_width=\"100%\",\n        container_width=\"100%\",\n        table_font_size=\"14px\",\n        heading_title_font_size=\"18px\",\n        heading_subtitle_font_size=\"14px\",\n        column_labels_border_bottom_style=\"solid\",\n        column_labels_border_bottom_width=\"3px\",\n        column_labels_border_bottom_color=\"#3498db\",\n        table_body_border_bottom_style=\"solid\",\n        table_body_border_bottom_width=\"1px\",\n        table_body_border_bottom_color=\"#dee2e6\"\n    )\n    .tab_source_note(\n        source_note=\"Formulas assume constant temperature and current rates unless otherwise specified.\"\n    )\n)\n\n# Display\ntable.show()\n\n\n\n\n\n\n\n\nTable 1: Battery Degradation Diagnostic Features\n\n\nMathematical definitions and physical interpretations of key battery health indicators\n\n\nDiagnostic Feature\nFormula\nPhysical Meaning\n\n\n\n\nVoltage Gap\nŒîVÃÑ = VÃÑ_c - VÃÑ_d\nAverage polarization; quantifies internal losses\n\n\nVoltage Hysteresis\nŒîV(x) = V_c(x) - V_d(x)\nLoss mechanisms at specific state-of-charge\n\n\nIC Peak Metrics\nIC = dQ/dV\nPhase transitions; indicates active material loss\n\n\nHysteresis Proxy Resistance\nR_proxy ‚àù ŒîV(x)/I_diff\nProxy for internal resistance growth\n\n\n\nFormulas assume constant temperature and current rates unless otherwise specified.\n\n\n\n\n\n\n\n\n\n\n\nStatistical feature aggregation\nThe diagnostic signals derived above (e.g., incremental capacity curves) remain high-resolution time- or cycle-series data. To produce robust, concise, and comparable inputs for the Bayesian regression model, a final aggregation step is performed by extracting statistical moments from each diagnostic signal \\(s(x)\\). This aggregation reduces hundreds of data points per cycle into a small number of highly informative scalar features.\n\n\nShow the code\ndata = pd.DataFrame({\n    \"Statistical Feature\": [\n        \"Mean\",\n        \"Standard Deviation\", \n        \"Skewness\",\n        \"Kurtosis\",\n        \"RMS (Root-Mean-Square)\",\n        \"Entropy\",\n        \"Crest Factor\",\n        \"AUC (Area Under the Curve)\"\n    ],\n    \"Role in Degradation Modeling\": [\n        \"Captures the overall trend or shift of the diagnostic signal.\",\n        \"Measures variability and cycle-to-cycle noise.\",\n        \"Indicates asymmetry or bias in the signal distribution.\",\n        \"Quantifies the presence of extreme values or anomalies.\",\n        \"Represents the overall magnitude and stress level of the signal.\",\n        \"Measures irregularity or disorder, often increasing with non-uniform degradation.\",\n        \"Compares peak magnitude to average signal level, highlighting abnormal peaks.\",\n        \"Captures cumulative effects such as total energy loss or degradation trends.\"\n    ]\n})\n\n\n\n\nShow the code\ntable = (\n    GT(data)\n    .tab_header(\n        title=md(\"Statistical Features for Battery Degradation Modeling\"),\n        subtitle=\"Key signal processing metrics used to quantify degradation patterns\"\n    )\n    .cols_label(\n        **{\n            \"Statistical Feature\": md(\"**Statistical Feature**\"),\n            \"Role in Degradation Modeling\": md(\"**Role in Degradation Modeling**\")\n        }\n    )\n    .tab_options(\n        table_width=\"100%\",\n        container_width=\"100%\",\n        table_font_size=\"14px\",\n        heading_title_font_size=\"18px\",\n        heading_subtitle_font_size=\"14px\",\n        column_labels_border_bottom_style=\"solid\",\n        column_labels_border_bottom_width=\"3px\",\n        column_labels_border_bottom_color=\"#3498db\",\n        table_body_border_bottom_style=\"solid\",\n        table_body_border_bottom_width=\"1px\",\n        table_body_border_bottom_color=\"#dee2e6\"\n    )\n    .tab_source_note(\n        source_note=\"Bayesian Modelling|Anthony Faustine@ 2025\"\n    )\n)\n\ntable.show()\n\n\n\n\n\n\n\n\nStatistical Features for Battery Degradation Modeling\n\n\nKey signal processing metrics used to quantify degradation patterns\n\n\nStatistical Feature\nRole in Degradation Modeling\n\n\n\n\nMean\nCaptures the overall trend or shift of the diagnostic signal.\n\n\nStandard Deviation\nMeasures variability and cycle-to-cycle noise.\n\n\nSkewness\nIndicates asymmetry or bias in the signal distribution.\n\n\nKurtosis\nQuantifies the presence of extreme values or anomalies.\n\n\nRMS (Root-Mean-Square)\nRepresents the overall magnitude and stress level of the signal.\n\n\nEntropy\nMeasures irregularity or disorder, often increasing with non-uniform degradation.\n\n\nCrest Factor\nCompares peak magnitude to average signal level, highlighting abnormal peaks.\n\n\nAUC (Area Under the Curve)\nCaptures cumulative effects such as total energy loss or degradation trends.\n\n\n\nBayesian Modelling|Anthony Faustine@ 2025\n\n\n\n\n\n\n\n\n\nThese statistical summaries (e.g., \\(\\text{Mean}(\\text{IC})\\), \\(\\text{Std}(\\Delta V)\\)) form the input vector \\(\\mathbf{x}\\) in the linear predictor \\(\\eta_i = \\dots + \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\). Such summaries of early-cycle behavior often preserve key degradation signatures while significantly reducing model complexity.\n\n\nFeature selection\nAfter extracting a broad set of diagnostic and statistical features, feature selection is required. Using all available features can lead to overfitting, increased model complexity, and multicollinearity, which compromises interpretability of the Bayesian coefficients (\\(\\boldsymbol{\\beta}\\)). The final four features selected for regression (\\(\\mathbf{x}\\)) are:\n\ncharge_current_auc\ncharge_current_mean\ndischarge_voltage_auc\ndischarge_voltage_crest\n\n\n\nLoad pre-processed CALCE dataset\nThe raw CALCE dataset is a widely used public resource in battery prognostics and can be downloaded via the CALCE dataset link.\nFor this notebook, however, we use pre-processed data that has been cleaned and formatted. This pre-processing reuses the techniques and codes originally published in this paper Ref. Using the cleaned data allows us to focus immediately on the Bayesian modeling aspects without the overhead of complex data preparation\n\n\nShow the code\nFIGSHARE_DOWNLOAD_URL = \"https://ndownloader.figshare.com/files/59415941\"\nfeatures = [\"charge_current_auc\", \"charge_current_mean\", \"discharge_voltage_crest\", \"discharge_voltage_auc\"]\ntarget = \"capacity\"\ndata = pd.read_parquet(FIGSHARE_DOWNLOAD_URL, engine=\"pyarrow\")\nupper_bound, lower_bound = data[target].max(), data[target].min()\ndf = data[data.CellType == \"CS2\"].copy()\ntest_df = df[df.BatteryID != \"CALCE_CS2_38\"]\ntrain_df = df[df.BatteryID == \"CALCE_CS2_38\"]\n\n\n\n\nShow the code\nplots = []\nfor feature_name in features:\n    plot = scatter_plot(train_df, y_col=feature_name)\n    plots.append(plot + labs(title=feature_name) + modern_theme(font_size=9))\n\nplot=gggrid(plots, ncol=2) + ggsize(650, 450)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  -0.1\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          charge_current_auc\n        \n      \n      \n        \n          charge_current_auc\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  -0.1\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          charge_current_mean\n        \n      \n      \n        \n          charge_current_mean\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  1.1\n                \n              \n            \n            \n              \n                \n                  1.15\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          discharge_voltage_crest\n        \n      \n      \n        \n          discharge_voltage_crest\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n            \n          \n          \n            \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                    \n                  \n                \n              \n              \n                \n                  \n                  \n                \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  0\n                \n              \n            \n            \n              \n              \n              \n                \n                  200\n                \n              \n            \n            \n              \n              \n              \n                \n                  400\n                \n              \n            \n            \n              \n              \n              \n                \n                  600\n                \n              \n            \n            \n              \n              \n              \n                \n                  800\n                \n              \n            \n            \n              \n              \n              \n                \n                  1,000\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  0.54\n                \n              \n            \n            \n              \n                \n                  0.56\n                \n              \n            \n          \n        \n        \n          \n            CALCE_CS2_38\n          \n        \n      \n      \n        \n          discharge_voltage_auc\n        \n      \n      \n        \n          discharge_voltage_auc\n        \n      \n      \n        \n          cycle\n        \n      \n      \n        \n          \n            \n              Status\n            \n          \n          \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Censored (Still Functioning)\n                \n              \n            \n            \n              \n                \n                  \n                    \n                      \n                    \n                  \n                \n              \n              \n                \n                  Failed (SOH &lt;= 80%)\n                \n              \n            \n          \n        \n      \n      \n        \n          Bayes Modelling | Anthony Faustine @2025\n        \n      \n      \n      \n    \n    \n    \n  \n\n\n\nThe figure below plots the four selected features against cycle number for a representative battery. These plots empirically validate the selection process, as all four features exhibit clear, monotonic changes with cycling and, critically, show a distinct shift or acceleration in slope as the battery enters the failure state (\\(\\text{SoH} \\le 80\\%\\), shown in green). This strong visual correlation provides high confidence that these inputs will effectively drive the degradation component of our Bayesian model.\n\n\n\nBayesian model building\nBefore constructing the Bayesian Beta regression model, it is necessary to define a rigorous evaluation strategy. The central question addressed here is whether a model trained on data from a single battery can successfully generalize to other batteries whose degradation trajectories were not observed during training.\nThis setting reflects a common real-world scenario in which detailed historical data may be available for only a limited number of prototype units, while the deployed model must operate reliably across an entire manufacturing batch.\nTo ensure a fair and controlled evaluation, all batteries considered in this study are restricted to a single cell chemistry type (‚ÄúCS2‚Äù). By holding the underlying electrochemical properties constant, the analysis isolates unit-to-unit variability rather than confounding the results with chemistry-dependent effects.\nA one-shot generalization split is employed. A single representative battery (CALCE_CS2_38) is designated as the training set (train_df). The model learns the degradation rate (\\(\\lambda\\)) and operational sensitivities (\\(\\boldsymbol{\\beta}\\)) exclusively from this unit‚Äôs historical data. All remaining batteries of the same chemistry are assigned to the test set (test_df). Model performance is therefore evaluated based on its ability to predict capacity fade for previously unseen batteries using only the generalizable parameters inferred from the training unit.\nWith the input data rigorously cleaned, aligned, scaled, and reduced to the four most informative operational features (\\(\\mathbf{x}\\)), the Bayesian regression model can now be implemented and fitted. As defined in the Beta Likelihood and Priors subsection, the model is specified as a Bayesian Beta regression with a logit link function, enabling the modeling of bounded battery capacity (\\(\\tilde{C}\\)).\n\n\nShow the code\n# from bayes.regression.beta_degradation import beta_regression_model\nmodel, scaler = beta_regression_model(\n    train_df, features, target=target, upper_bound=upper_bound, lower_bound=lower_bound\n)\nmodel\n\n\n\\[\n            \\begin{array}{rcl}\n            \\text{intercept} &\\sim & \\operatorname{Normal}(1e-06,~0.5)\\\\\\text{lambda\\_rate} &\\sim & \\operatorname{LogNormal}(-5.3,~0.5)\\\\\\text{beta} &\\sim & \\operatorname{Normal}(0,~0.2)\\\\\\text{phi} &\\sim & \\operatorname{Gamma}(100,~f())\\\\\\text{degr\\_amp} &\\sim & \\operatorname{HalfNormal}(0,~0.1)\\\\\\text{mu\\_scaled} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{mu\\_original} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{capacity\\_pred} &\\sim & \\operatorname{Deterministic}(f(\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\\\\\\text{feature\\_effects} &\\sim & \\operatorname{Deterministic}(f(\\text{beta}))\\\\\\text{y\\_obs} &\\sim & \\operatorname{Beta}(f(\\text{phi},~\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}),~f(\\text{phi},~\\text{beta},~\\text{intercept},~\\text{degr\\_amp},~\\text{lambda\\_rate}))\n            \\end{array}\n            \\]\n\n\n\nPrior Predictive Check\nFollowing standard Bayesian practice, model validation begins with a Prior Predictive Check (PPC). The PPC involves simulating data from the model using only the prior distributions, without conditioning on any observed measurements. This procedure serves as a critical sanity check, verifying that the encoded engineering knowledge produces physically plausible behavior.\n\n\nShow the code\nwith model:\n    prior_pred = pm.sample_prior_predictive(samples=1000)\n\nfig, ax = plt.subplots(figsize=(4, 1.8))\naz.plot_ppc(prior_pred, group=\"prior\", ax=ax)\nplt.xlabel(\"Capacity\")\nplt.ylabel(\"Density\");\n\n\nSampling: [beta, degr_amp, intercept, lambda_rate, phi, y_obs]\n\n\n\n\n\n\n\n\n\nThe figure below, compare the predicted prior distribution (green line) against the observed data (blue line). This plot is essential for validating that our model‚Äôs structural assumptions align with physical reality\n\n\nShow the code\ny_prior = prior_pred.prior[\"capacity_pred\"].stack(sample=[\"chain\", \"draw\"]).values\ny_obs = train_df[target].values\npost_mean = y_prior.mean()\nn_draws = 500\nrng = np.random.default_rng(42)\ndraw_idx = rng.choice(y_prior.shape[0], size=n_draws, replace=False)\ny_prior_subset = y_prior[:, draw_idx].flatten()\ndf_prior = pd.DataFrame(\n    {\n        \"SoH\": np.concatenate([y_obs, y_prior_subset]),\n        \"type\": [\"observed\"] * len(y_obs) + [\"prior\"] * len(y_prior_subset),\n    }\n)\nplot=plot_density(\n    df_prior,\n    x_col=\"SoH\",\n    color_col=\"type\",\n    x_label=\"Capacity\",\n    fig_size=(500, 400),\n    title=\"Prior comparison\",\n    subtitle=\"Prior Predictive Check\",\n)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.2\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Prior comparison\n      \n    \n    \n      \n        Prior Predictive Check\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Capacity\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                observed\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                prior\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nFrom the two figures above, the PPC confirms the structural validity of the model:\n\nHigh-Confidence Initial Capacity: The predicted capacity distribution exhibits a dominant peak near \\(\\mu \\approx 1.0\\), reflecting the highly informative precision prior \\(\\phi \\sim \\text{Gamma}(100, 2.0)\\) (mean \\(\\phi = 50\\)). This enforces a strong prior belief in low sensor noise and high initial measurement confidence.\nRealistic Degradation Envelope: The prior predictive distribution remains tightly constrained across the capacity range. This behavior is driven by the informative degradation-rate prior on \\(\\lambda\\), which minimizes the probability of immediate or catastrophic capacity loss and enforces physically plausible degradation trajectories.\nAcknowledgment of Failure Modes: While constrained, the prior allocates non-negligible probability mass to lower capacity regions (e.g., \\(\\tilde{C} \\approx 0.4\\)‚Äì\\(0.7\\)). This reflects uncertainty in the degradation amplitude and rate parameters, allowing for degradation and failure scenarios without overstating their likelihood.\n\nThe PPC demonstrates that the model respects physical bounds, reflects realistic degradation behavior, and balances strong prior knowledge with controlled uncertainty. The model is therefore suitable for posterior inference.\n\n\n\nRunning inference\nWith the model fully specified, priors validated through the PPC, and input features prepared, posterior inference is performed using Markov Chain Monte Carlo (MCMC) sampling. This step approximates the posterior distribution by updating prior beliefs using the observed data, forming the core of Bayesian inference.\n\n\nShow the code\nwith model:\n    idata = pm.sample(2000, tune=2000, target_accept=0.95, random_seed=42)\nclear_output()\n\n\nAs discussed in Part 1, the MCMC process uses the No-U-Turn Sampler (NUTS) to explore the parameter space. The primary arguments guide this process:\n\ntune=2000: Specifies 2000 initial samples that are used solely to adapt the sampler‚Äôs step size and are then discarded. A high tuning value is crucial for complex, highly curved posteriors (like those involving Beta distributions) to ensure stable exploration.\ndraws=2000: Specifies 2000 final samples kept from the chain. These collected samples form the final Posterior Distribution for every model parameter (\\(\\lambda\\), \\(\\beta\\), \\(\\phi\\)).\ntarget_accept=0.95: Forces the sampler to take smaller, more cautious steps. This high acceptance rate is necessary to avoid divergences in challenging models, ensuring a high-quality, accurate representation of the posterior distribution, though it increases computation time.\n\nThe resulting idata object now contains thousands of samples for every single model parameter, representing our comprehensive, uncertainty-quantified solution. The next step is to ensure these samples are reliable\n\nModel diagnostics\nAfter sampling, convergence diagnostics are evaluated to ensure the reliability of posterior estimates. The validity of all subsequent inferences depends on whether the Markov chains have adequately explored the parameter space.\n\n\nShow the code\nvars = [\"beta\", \"intercept\", \"lambda_rate\", \"phi\", \"degr_amp\"]\ndata_summary = az.summary(idata, var_names=vars, kind=\"diagnostics\")[[\"ess_bulk\", \"ess_tail\", \"r_hat\"]]\nGT(data_summary.reset_index()).tab_header(title=\"\", subtitle=\"Diagnostics Summary\").cols_label(\n    {\n        \"ess_bulk\": \"ESS Bulk\",\n        \"ess_tail\": \"ESS Tail.\",\n        \"r_hat\": \"R-hat\",\n    }\n)\n\n\n\n\n\n\n\n\n\n\n\nDiagnostics Summary\n\n\nindex\nESS Bulk\nESS Tail.\nR-hat\n\n\n\n\nbeta[0]\n4381.0\n4866.0\n1.0\n\n\nbeta[1]\n4268.0\n4571.0\n1.0\n\n\nbeta[2]\n4218.0\n4234.0\n1.0\n\n\nbeta[3]\n4394.0\n4732.0\n1.0\n\n\nintercept\n4426.0\n4803.0\n1.0\n\n\nlambda_rate\n5309.0\n4684.0\n1.0\n\n\nphi\n7368.0\n5104.0\n1.0\n\n\ndegr_amp\n3758.0\n4646.0\n1.0\n\n\n\n\n\n\n\n\nTwo primary diagnostics are considered:\n\n\\(\\hat{R}\\) (Gelman‚ÄìRubin statistic): All parameters exhibit \\(\\hat{R} = 1.0\\), indicating excellent chain mixing and agreement across chains.\nEffective Sample Size (ESS): ESS values exceed 400 for all parameters (ranging from approximately 4,200 to 7,300), confirming that a sufficient number of independent samples were obtained for stable estimation of posterior means and credible intervals.\n\nThese diagnostics collectively indicate successful convergence and robust posterior sampling.\n\n\nAnalysing the posterior distribution\nWith convergence confirmed, the sampled chains provide a reliable approximation of the posterior distribution. The marginal posterior densities and corresponding trace plots for the core model parameters are examined to quantify degradation dynamics and assess the influence of operational features.\nThis analysis enables principled uncertainty quantification of degradation rates and feature effects, supporting interpretable and decision-relevant predictions for battery health forecasting.\n\n\nShow the code\naz.plot_trace(idata, var_names=vars, compact=True);\n\n\n\n\n\n\n\n\n\nThe analyze_parameter function below acts as a post-processing utility dedicated to generating publication-ready summary tables from the output of the MCMC sampling.\n\n\nShow the code\ndef analyze_parameter(\n    idata,\n    parameter: str,\n    features: list[str] | None = None,\n    hdi_prob: float = 0.95,\n    title: str = \"Parameter Summary\",\n    subtitle: str | None = None,\n) -&gt; GT:\n    \"\"\"Generates a formatted summary table for a single parameter using Great Tables.\n\n    This function extracts posterior summary statistics from ArviZ InferenceData and\n    returns a beautifully styled table suitable for reports, notebooks, or publications.\n\n    Args:\n        idata: ArviZ InferenceData object containing posterior samples.\n        parameter: Name of the parameter to summarize (e.g., \"beta\", \"alpha\", \"sigma\").\n        features: List of feature names to label rows. Required and used only when\n            ``parameter == \"beta\"``. Length must match the number of coefficients.\n        hdi_prob: Highest density interval probability (default: 0.95).\n        title: Main title for the table.\n        subtitle: Optional subtitle. If None and parameter is \"beta\", defaults to\n            \"Beta coefficient analysis\".\n\n    Returns:\n        A Great Tables (GT) object ready for display or further customization.\n\n    Raises:\n        ValueError: If ``features`` is provided for non-beta parameters or has wrong length.\n\n    Example:\n        &gt;&gt;&gt; gt = analyze_parameter(idata, \"beta\", features=X.columns.tolist())\n        &gt;&gt;&gt; gt  # displays nicely in Jupyter\n    \"\"\"\n    if features is not None and parameter != \"beta\":\n        raise ValueError(\"`features` should only be provided when parameter == 'beta'\")\n\n    # Get summary statistics from ArviZ\n    summary_df = az.summary(\n        idata,\n        var_names=[parameter],\n        hdi_prob=hdi_prob,\n        kind=\"stats\",\n        fmt=\"wide\",\n    ).reset_index(names=\"feature\")\n\n    # Assign meaningful feature names for beta coefficients\n    if parameter == \"beta\":\n        if features is None:\n            raise ValueError(\"`features` must be provided when analyzing 'beta' parameter\")\n        if len(features) != len(summary_df):\n            raise ValueError(\n                f\"Length of features ({len(features)}) must equal number of beta coefficients ({len(summary_df)})\"\n            )\n        summary_df[\"feature\"] = features\n\n    conditions = [\n        summary_df[\"hdi_2.5%\"] &gt; 0,  # Entire interval is positive\n        summary_df[\"hdi_97.5%\"] &lt; 0,  # Entire interval is negative\n    ]\n    choices = [\"Positive\", \"Negative\"]\n    summary_df[\"certainty\"] = np.select(conditions, choices, default=\"Uncertain\")\n\n    # Set default subtitle for beta coefficients\n    if subtitle is None and parameter == \"beta\":\n        subtitle = \"Beta coefficient analysis\"\n\n    gt_table = (\n        GT(summary_df)\n        .tab_header(\n            title=md(f\"**{title}**\"),\n            subtitle=md(subtitle) if subtitle else None,\n        )\n        .fmt_number(\n            columns=[\"mean\", \"sd\", \"hdi_2.5%\", \"hdi_97.5%\"],\n            decimals=3,\n        )\n        .data_color(\n            columns=[\"certainty\"],\n            palette=[\"#E1DFDD\", \"#F18F01\", \"#F18F01\"],\n            domain=[\"Uncertain\", \"Negative\", \"Positive\"],\n        )\n        .cols_label(\n            feature=md(\"**Feature**\"),\n            mean=md(\"**Mean**\"),\n            sd=md(\"**SD**\"),\n            **{\"hdi_2.5%\": md(\"**HDI 2.5%**\")},\n            **{\"hdi_97.5%\": md(\"**HDI 97.5%**\")},\n        )\n        .cols_align(align=\"center\", columns=[\"mean\", \"sd\", \"hdi_2.5%\", \"hdi_97.5%\", \"Certainty\"])\n        .tab_options(\n            table_font_size=\"14px\",\n            heading_title_font_size=\"20px\",\n            heading_subtitle_font_size=\"16px\",\n            row_group_font_weight=\"bold\",\n        )\n    )\n\n    return gt_table\n\n\n\n\nIdentifying Reliable Degradation Drivers (\\(\\boldsymbol{\\beta}\\) Coefficients)\nThe regression coefficients \\(\\boldsymbol{\\beta}\\) quantify the relationship between the engineered operational features (e.g., current and voltage metrics) and battery State of Health (\\(\\text{SoH}\\)) through the logit link function, \\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\). The credibility of each predictor is assessed by examining whether its \\(95%\\) Highest Density Interval (HDI) includes zero.\n\n\nShow the code\ntable=analyze_parameter(idata, \"beta\", features=features)\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nParameter Summary\n\n\nBeta coefficient analysis\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\ncharge_current_auc\n‚àí0.159\n0.145\n‚àí0.437\n0.132\nUncertain\n\n\ncharge_current_mean\n‚àí0.147\n0.145\n‚àí0.420\n0.145\nUncertain\n\n\ndischarge_voltage_crest\n‚àí0.446\n0.041\n‚àí0.528\n‚àí0.369\nNegative\n\n\ndischarge_voltage_auc\n0.020\n0.040\n‚àí0.063\n0.092\nUncertain\n\n\n\n\n\n\n\n\nThe posterior summary indicates that only one feature emerges as a statistically reliable degradation driver at the \\(95%\\) credibility level: the discharge_voltage_crest factor. Its \\(95%\\) HDI lies entirely below zero (from \\(-0.528\\) to \\(-0.369\\)), indicating strong evidence of a negative association with capacity retention. This result implies, with high certainty, that increases in this factor accelerate capacity fade. The posterior mean coefficient for the discharge_voltage_crest factor is \\(\\beta = -0.446\\). Interpreted on the odds scale as \\[\\text{Odds Ratio} = \\exp(-0.446) \\approx 0.64\\].\nThus, a one-unit increase in the crest factor is associated with an approximately \\(36%\\) reduction in the odds of maintaining high battery capacity (\\(1 - 0.64\\)).\nIn contrast, the \\(95%\\) HDIs for the remaining three features include zero (e.g., for charge_current_auc, HDI \\([-0.437,,0.132]\\)). As a result, the model cannot rule out the possibility that their true effects are negligible or even slightly positive. These features therefore do not constitute statistically reliable degradation drivers under the current model specification. Consequently, maintenance and monitoring efforts can be focused on the discharge_voltage_crest factor as the dominant operational indicator of degradation.\n\nüõ†Ô∏è Action: Refit the Beta regression model using only the discharge_voltage_crest factor as an operational covariate. Evaluate whether predictive performance on the test set remains comparable and whether the posterior mean and HDI for this coefficient remain stable. Such consistency would further support the conclusion that the remaining features primarily contributed noise rather than explanatory signal.\n\n\n\nQuantifying the degradation rate (\\(\\lambda_{\\text{rate}}\\))\nThe parameter \\(\\lambda_{\\text{rate}}\\) governs the speed of capacity fade induced by cycling. By adopting a weakly informative Lognormal prior with increased dispersion (\\(\\sigma = 1.0\\)), the data is allowed to dominate the posterior estimation of this parameter.\n\n\nShow the code\ntable=analyze_parameter(idata, \"lambda_rate\", title=\"Lambda Rate Summary\", subtitle=\"Degradation rate parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nLambda Rate Summary\n\n\nDegradation rate parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\nlambda_rate\n0.003\n0.001\n0.002\n0.004\nPositive\n\n\n\n\n\n\n\n\nThe posterior summary yields a highly precise estimate of the degradation rate, with a posterior mean of \\(\\mathbf{0.003}\\) per unit of cycle data. This value is lower than the prior expectation (centered around \\(0.005\\)), indicating that although degradation is inevitable, it progresses more gradually than initially assumed.\nThe remaining uncertainty is minimal, as evidenced by a small posterior standard deviation (\\(\\text{SD} = 0.001\\)) and a narrow \\(95%\\) HDI of \\([0.002,,0.004]\\). These results confirm that the MCMC sampler has effectively leveraged the data to tightly constrain the degradation speed.\n\n\nModel precision (\\(\\phi\\))\nThe precision parameter \\(\\phi\\) controls the dispersion of the Beta likelihood, quantifying how tightly the observed \\(\\text{SoH}\\) measurements cluster around the model-predicted mean after accounting for all modeled effects.\n\n\nShow the code\ntable=analyze_parameter(idata, \"phi\", title=\"Phi Summary\", subtitle=\"Phi  parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nPhi Summary\n\n\nPhi parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\nphi\n233.461\n10.140\n213.389\n252.803\nPositive\n\n\n\n\n\n\n\n\nThe posterior distribution of \\(\\phi\\) exhibits a high degree of concentration, with a posterior mean of \\(233.461\\) and a narrow \\(95%\\) HDI. This large mean precision implies very low residual variance in \\(\\text{SoH}\\), indicating that the combined model structure‚Äîincorporating the exponential degradation term, the cycling rate \\(\\lambda\\), and the operational features \\(\\boldsymbol{\\beta}\\) explains the majority of observed variability across the battery fleet. The narrow HDI further indicates that this high precision is estimated with substantial certainty.\n\nDegr amp parameter (\\(A\\))\nThe degradation amplitude parameter, \\(\\text{degr\\_amp}\\), quantifies the maximum potential capacity fade attributable solely to the cycling process and operates on the log-odds scale. The posterior mean of \\(0.367\\) represents the maximum reduction in \\(\\text{logit}(\\mu)\\) induced by cycling over the battery‚Äôs lifetime, thereby determining the vertical extent of the degradation curve.\n\n\nShow the code\n\ntable=analyze_parameter(idata, \"degr_amp\", title=\"Degr amp\", subtitle=\"A  parameter\")\n\n\n\n\nShow the code\ntable\n\n\n\n\n\n\n\n\nDegr amp\n\n\nA parameter\n\n\nFeature\nMean\nSD\nHDI 2.5%\nHDI 97.5%\ncertainty\n\n\n\n\ndegr_amp\n0.367\n0.053\n0.265\n0.468\nPositive\n\n\n\n\n\n\n\n\nThe \\(95%\\) HDI for \\(\\text{degr\\_amp}\\) is narrow and entirely positive, spanning \\([0.265,,0.468]\\). This provides strong statistical evidence that degradation due to cycling is both certain and quantitatively well-defined, rather than an artifact of noise. The strictly positive support of this parameter confirms that capacity loss is an unavoidable consequence of repeated cycling.\n\n\n\nPosterior predictive check\nFollowing parameter interpretation, a Posterior Predictive Check (PPC) is performed to assess model adequacy. This step evaluates whether the model, using posterior parameter samples, can generate synthetic data that closely resembles the observed measurements.\nUsing PyMC‚Äôs sample_posterior_predictive function, samples are drawn from the likelihood conditioned on the converged posterior chains. The resulting PPC compares three distributions: the observed data, the prior predictive distribution, and the posterior predictive distribution.\n\n\nShow the code\nwith model:\n    post_pred = pm.sample_posterior_predictive(idata, var_names=[\"y_obs\"], random_seed=42)\n\ny_post = post_pred.posterior_predictive[\"y_obs\"].stack(sample=[\"chain\", \"draw\"]).values\n# Mean of the posterior predictive\npost_mean = y_post.mean()\nn_draws = 500\nrng = np.random.default_rng(42)\ndraw_idx = rng.choice(y_post.shape[0], size=n_draws, replace=False)\ny_post_subset = y_post[:, draw_idx].flatten()\ny_post_subset = y_post_subset * (upper_bound - lower_bound) + lower_bound\n\n\ndf_posterior = pd.DataFrame(\n    {\n        \"SoH\": np.concatenate([y_obs, y_post_subset]),\n        \"type\": [\"observed\"] * len(y_obs) + [\"posterior\"] * len(y_post_subset),\n    }\n)\n\n\nSampling: [y_obs]\n\n\n\n\n\n\n\n\nThe figure below shows a Posterior Predictive Check (PPC), which is the gold standard for evaluating model fit in Bayesian statistics. It compares three key distributions for the capacity: the data we observed, our initial beliefs (Prior), and the model‚Äôs final predictions (Posterior).\n\n\nShow the code\ndf = pd.concat([df_prior, df_posterior])\nplot=plot_density(\n    df,\n    x_col=\"SoH\",\n    color_col=\"type\",\n    x_label=\"Capacity\",\n    fig_size=(700, 350),\n    title=\"Prior and Posterior Comparison\",\n    subtitle=\"Beta-regression\",\n)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.3\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.5\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.7\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                0.9\n              \n            \n          \n          \n            \n            \n            \n              \n                1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.1\n              \n            \n          \n          \n            \n            \n            \n              \n                1.2\n              \n            \n          \n          \n            \n            \n            \n              \n                1.3\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                1\n              \n            \n          \n          \n            \n              \n                2\n              \n            \n          \n          \n            \n              \n                3\n              \n            \n          \n          \n            \n              \n                4\n              \n            \n          \n        \n      \n    \n    \n      \n        Prior and Posterior Comparison\n      \n    \n    \n      \n        Beta-regression\n      \n    \n    \n      \n        Density\n      \n    \n    \n      \n        Capacity\n      \n    \n    \n      \n        \n          \n            Distribution\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                observed\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                prior\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                posterior\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayes Modelling | Anthony Faustine @2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nThe posterior predictive distribution aligns closely with the observed capacity distribution, indicating that the model successfully captures the underlying data-generating process. In contrast, the prior predictive distribution is smoother and exhibits broader structure, reflecting weaker and less targeted assumptions before observing data.\nThe shift and sharpening from prior to posterior predictive distributions demonstrate that the observed data provided substantial information and that the model effectively updated its initial beliefs.\nThe posterior predictive distribution is strongly skewed toward high capacity values near \\(1.0\\), consistent with the predominance of early- and mid-life measurements. A smaller secondary mode around \\(0.3\\)‚Äì\\(0.4\\) corresponds to a limited number of end-of-life observations. This agreement between synthetic and observed data provides strong evidence of model adequacy and predictive reliability.\n\n\n\nPredict capacity for a new battery\nThe final objective of this modeling effort is to transition from parameter estimation to practical prognosis by generating a full Posterior Predictive Distribution (PPD) for the capacity of a new or future battery state. This process converts uncertainty-aware parameter estimates into actionable prognostic predictions.\nThe PPD explicitly incorporates two fundamental sources of uncertainty:\n\nEpistemic uncertainty, arising from uncertainty in the estimated model parameters (e.g., the width of the HDI for \\(\\lambda_{\\text{rate}}\\)).\nAleatoric uncertainty, representing irreducible noise in the measurement process, as captured by the precision parameter \\(\\phi\\).\n\nAs a result, the model produces not a single point estimate but a credible interval (HDI) that probabilistically bounds the true capacity value at each cycle.\nTo assess generalization performance and demonstrate practical utility for risk management, four cells from the CALCE dataset that were excluded during training are selected for evaluation. For each battery, the same four operational features used in model training are extracted and supplied to the fitted model.\n\n\nShow the code\nfrom bayes.regression.beta_degradation import get_posterior_predictions\n\n\n\n\nShow the code\nnew_df = test_df[test_df[\"BatteryID\"].isin([\"CALCE_CS2_34\", \"CALCE_CS2_36\", \"CALCE_CS2_37\", \"CALCE_CS2_33\"])].copy()\n\n\nPosterior predictions for unseen batteries are generated using the `get_posterior_predictions procedure, which applies the fitted Bayesian model to new input data. The process consists of the following steps:\n\nFeature Transformation: The operational features of the new battery are transformed using the same scaling object fitted during training, ensuring consistency between training and inference domains. The corresponding cycle counts are extracted separately, as they directly enter the exponential degradation component of the model.\n\n\n    x_new = scaler.transform(data[features])\n\nDummy Target Initialization: A placeholder target array is supplied to satisfy the dimensional requirements of the PyMC model‚Äôs observed variable. These values are ignored during posterior prediction..\n\n    y_dummy_scaled = np.full(shape=(X_new_scaled.shape[0],), fill_value=0.5)\n\nModel update: The new feature matrix, cycle data, and dummy target are injected into the model using pm.set_data, reconfiguring the model for prediction without retraining.\n\n    with battery_model:\n        pm.set_data({\n            \"x_data\": x_new,\n            \"cycle_data\": cycle_new,\n            \"y_obs\": y_dummy_scaled\n        })\n\nPosterior Predictive Sampling: Samples are drawn from the Posterior Predictive Distribution using pm.sample_posterior_predictive, incorporating both posterior parameter uncertainty and observation noise..\n\n    with battery_model:\n        post_pred = pm.sample_posterior_predictive(\n            idata,\n            var_names=[\"y_obs\"],\n            random_seed=42,\n            predictions=True,\n        )\n\n\nShow the code\npred_df = get_posterior_predictions(\n    idata, model, scaler, new_df, features, alpha=0.1, upper_bound=upper_bound, lower_bound=lower_bound\n)\nclear_output()\n\n\nPredictions for each test battery are visualized using the plot_hdi_regression function. The plots display the observed capacity measurements (points) overlaid with the model‚Äôs posterior predictive mean and the corresponding \\(90%\\) HDI. The blue line represents the posterior predictive mean where the shaded region represents the \\(90%\\) HDI, quantifying predictive uncertainty.\n\n\nShow the code\nfrom bayes.plot.regres_plot import plot_hdi_regression\n\n\n\n\nShow the code\nplot=plot_hdi_regression(\n    pred_df,\n    x_column=\"cycle\",\n    y_column=\"capacity\",\n    group_column=\"BatteryID\",\n    pred_column=\"pred_median\",\n    x_label=\"Cycle Number\",\n    y_label=\"Capacity (Ah)\",\n    title_prefix=\"Battery Capacity vs. Cycle with Posterior Predictions\",\n    subtitle=\"90% HDI accounts for  uncertainty.\",\n    alpha=0.1,\n) + ggsize(800, 500)\n\n\n\n\nShow the code\nplot\n\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_33\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_34\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                500\n              \n            \n          \n          \n            \n            \n            \n              \n                1,000\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_36\n        \n      \n    \n    \n      \n        \n          \n          \n        \n        \n          \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                0\n              \n            \n          \n          \n            \n            \n            \n              \n                500\n              \n            \n          \n          \n            \n            \n            \n              \n                1,000\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                0\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.2\n              \n            \n          \n        \n      \n      \n        \n          CALCE_CS2_37\n        \n      \n    \n    \n      \n        Battery Capacity vs. Cycle with Posterior Predictions\n      \n    \n    \n      \n        90% HDI accounts for  uncertainty.\n      \n    \n    \n      \n        Capacity (Ah)\n      \n    \n    \n      \n        Cycle Number\n      \n    \n    \n      \n        \n          \n            Legend\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                90% HDI\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n                \n                  \n                    \n                  \n                \n              \n            \n            \n              \n                Observed Data\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n                \n                  \n                    \n                  \n                \n              \n            \n            \n              \n                Posterior Mean\n              \n            \n          \n        \n      \n    \n    \n      \n        Bayesian Modelling| Anthony Faustine @ 2025\n      \n    \n    \n    \n  \n  \n  \n\n\n\nKey Observations\n\nThe posterior predictive mean closely tracks the observed capacity trajectory across the full battery lifetime, including the non-linear degradation phase near end-of-life.\nApproximately \\(90\\%\\) (or more) of observed data points fall within the predictive HDI, indicating well-calibrated uncertainty estimates.\nFor batteries CS2_33, CS2_36, and CS2_37, the HDI remains narrow even during late-life degradation, reflecting high model confidence and low residual noise.\nFor battery CS2_34, the HDI widens toward the final cycles, appropriately reflecting increased predictive uncertainty in the late-life regime.\n\n\nEvaluating Predictive Performance\nPredictive performance on the held-out batteries is quantified using both accuracy and uncertainty-based metrics.\n\n\nShow the code\nfrom bayes.metrics.interval import get_interval_metrics\nfrom bayes.metrics.regression import regression_report\n\nmetrics_list = []\nfor battery_id, df in pred_df.groupby(\"BatteryID\"):\n    reg_report = regression_report(df[\"capacity\"], df[\"pred_median\"])\n    interval_report = get_interval_metrics(\n        df[\"pred_median\"].values,\n        df[\"capacity\"].values,\n        df[\"hdi_low\"].values,\n        df[\"hdi_high\"].values,\n        alpha=0.1,\n    )\n    full_report = pd.concat([reg_report, interval_report], ignore_index=True)\n    full_report[\"BatteryID\"] = battery_id\n    metrics_list.append(full_report)\nmetrics_df = pd.concat(metrics_list, ignore_index=True)\n\nmetrics_df = metrics_df.pivot_table(\n    index=[\"BatteryID\"],\n    columns=\"Metric\",\n    values=\"Value\",\n).reset_index()\n\n\n\ndef make_metrics_table(metrics_df, title=\"Model Evaluation Results\"):\n    \"\"\"Generate a formatted GT table from cross-validation metrics.\"\"\"\n    # Sort to present best models first\n    df = metrics_df.sort_values([\"BatteryID\", \"MAE\"]).reset_index(drop=True)\n\n    # Build base table\n    gt = (\n        GT(df[[\"BatteryID\", \"MAE\", \"RMSE\", \"R2\", \"NMPI\", \"PICP\"]])\n        .tab_header(title=title, subtitle=\"Per-battery performance\")\n        .cols_label(BatteryID=\"Test Cell\", MAE=\"MAE\", RMSE=\"RMSE\", R2=md(\"R&lt;sup&gt;2&lt;/sup&gt;\"))\n        .fmt_number(columns=[\"MAE\", \"RMSE\", \"NMPI\", \"PICP\"], decimals=3)\n        .fmt_number(columns=\"R2\", decimals=3)\n        .tab_spanner(label=\"Error Metrics\", columns=[\"MAE\", \"RMSE\", \"NMPI\", \"PICP\"])\n        .tab_style(\n            style=style.text(weight=\"bold\"),\n            locations=loc.body(columns=\"Model\"),\n        )\n        .tab_options(\n            table_font_size=\"small\",\n            # row_strip_color=\"#fafafa\"\n        )\n    )\n    for col in [\"MAE\", \"RMSE\", \"R2\", \"NMPI\", \"PICP\"]:\n        best_idx = df[col].idxmax() if col in [\"R2\", \"PICP\"] else df[col].idxmin()\n        gt = gt.tab_style(style=style.fill(color=\"#E1DFDD\"), locations=loc.body(rows=best_idx, columns=col))\n\n    return gt\n\n\n\n\nShow the code\nmake_metrics_table(metrics_df, title=\"Model Evaluation Results\")\n\n\n\n\n\n\n\n\nModel Evaluation Results\n\n\nPer-battery performance\n\n\nTest Cell\nError Metrics\nR2\n\n\nMAE\nRMSE\nNMPI\nPICP\n\n\n\n\nCALCE_CS2_33\n0.010\n0.010\n0.100\n1.000\n0.990\n\n\nCALCE_CS2_34\n0.040\n0.050\n0.100\n0.750\n0.910\n\n\nCALCE_CS2_36\n0.020\n0.020\n0.100\n0.990\n0.990\n\n\nCALCE_CS2_37\n0.010\n0.010\n0.100\n1.000\n1.000\n\n\n\n\n\n\n\n\nAccuracy Metrics\nPoint-prediction accuracy is evaluated using the coefficient of determination (\\(R^2\\)), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE). Together, these metrics assess how well the model‚Äôs posterior predictive mean captures the observed capacity degradation trajectory.\n\n\\(R^2\\) quantifies the proportion of variance in the observed capacity explained by the model. Values close to 1 indicate that the predicted degradation curve accurately follows the overall trend and slope of capacity fade.\nMAE represents the average absolute deviation between predicted and observed capacity values, expressed in State-of-Health (SoH) units. MAE provides a physically interpretable measure of typical prediction error.\nRMSE penalizes larger deviations more strongly than MAE and is therefore particularly sensitive to localized mismatches, especially during the highly nonlinear end-of-life degradation phase.\n\nFrom the results summarized above, the model achieves \\(R^2\\) values between \\(0.910\\) and \\(1.0\\), with MAE in the range of \\(0.010\\)‚Äì\\(0.040\\) SoH units, indicating strong point-prediction accuracy across all evaluated cells. RMSE values range from \\(0.01\\) to \\(0.05\\) SoH units, confirming that large deviations are generally rare.\nHowever, Cell 34 exhibits the highest RMSE (\\(0.05\\)), along with comparatively lower \\(R^2\\) and higher MAE than the remaining cells. This combination indicates that, while the model captures the overall degradation trend for Cell 34, it experiences larger localized errors‚Äîparticularly near late-life degradation relative to other cells. These deviations suggest the presence of sharper nonlinear behavior or cell-specific degradation mechanisms not fully represented by the global model parameters.\nUncertainty Quality Metrics\nBeyond point accuracy, a central goal of Bayesian modeling is to provide reliable and interpretable uncertainty estimates. This is assessed using Prediction Interval Coverage Probability (PICP) and Normalized Mean Prediction Interval (NMPI).\n\nPICP measures the fraction of observed capacity values that fall within the model‚Äôs \\(90%\\) Highest Density Interval (HDI). A well-calibrated model should achieve PICP close to the nominal level (0.95), indicating that the predicted uncertainty accurately reflects real variability.\nNMPI quantifies the average width of the predictive interval, normalized by the observed capacity range. NMPI reflects the sharpness of predictions: lower values indicate tighter uncertainty bounds, which are essential for actionable maintenance and risk-based decision-making.\n\nResults show consistently low NMPI values (approximately \\(\\mathbf{0.1}\\)) across all test cells. This confirms that the predictive intervals are narrow relative to the capacity range, indicating high confidence in the model‚Äôs predictions. At the same time, PICP exceeds \\(0.90\\) for most cells, demonstrating that this confidence is not over-stated and that the uncertainty bounds are well calibrated.\nCell 34 again deviates from this pattern, exhibiting reduced PICP (\\(75\\%\\)). This indicates that a larger fraction of its observed capacity measurements fall outside the predicted \\(90%\\) HDI, suggesting either increased intrinsic variability or degradation dynamics that differ from those captured by the global model.\n\nüõ†Ô∏è Action: To diagnose the degraded predictive performance observed for Cell 34, compare all test cells by plotting \\(R^2\\) versus PICP to identify accuracy‚Äìreliability trade-offs. Examine whether Cell 34‚Äôs operational features fall outside the training-feature distribution, indicating extrapolation beyond the model‚Äôs learned domain. Finally, compare the capacity degradation distribution of Cell 34 with the training cell to assess whether it follows a distinct degradation regime.\n\n\n\n\nConclusion\nThis post demonstrates that a single-level Bayesian Beta regression model, informed by physics-aware priors and carefully engineered features, can deliver highly accurate capacity predictions together with trustworthy uncertainty bounds. The model achieves near-perfect predictive accuracy while maintaining well-calibrated \\(95%\\) credible intervals, validating the use of Beta likelihoods and informed priors for battery degradation modeling.\nHowever, the current formulation assumes that the degradation rate (\\(\\lambda_{\\text{rate}}\\)) and operational sensitivities (\\(\\boldsymbol{\\beta}\\)) are shared across the entire battery fleet. In practice, manufacturing variability and latent defects introduce unit-specific degradation behavior. As a result, even a highly accurate global model may underpredict risk for batteries from unfavorable batches or overestimate degradation for higher-quality units.\nComing Next: In Part 3, this limitation will be addressed by introducing Hierarchical Bayesian Models. These models learn a robust global degradation trend while allowing each individual battery to exhibit informed local deviations in parameters such as \\(\\lambda_{\\text{rate}}\\) and \\(\\boldsymbol{\\beta}\\).\n\nThe data and full code in pymc5 is available on my GITHUB page.\n\n\n\nReferences\n\nZhang, H., Li, Y., Zheng, S. et al.¬†Battery lifetime prediction across diverse ageing conditions with inter-cell deep learning. Nat Mach Intell 7, 270‚Äì277 (2025). https://doi.org/10.1038/s42256-024-00972-x\nFerrari, S. L. P., & Cribari-Neto, F. (2004). Beta regression for modelling rates and proportions. Journal of Applied Statistics, 31(7), 799‚Äì815.\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.\nMcElreath, R. (2020). Statistical Rethinking (2nd ed.). CRC Press.\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nFaustine, Anthony. 2025. ‚ÄúBayesian Regression: A Real-World\nBattery Degradation Case Study.‚Äù December 17, 2025. https://sambaiga.github.io/blog/2025/12/2025-12-1-bayesian-regression..html."
  },
  {
    "objectID": "blog/2024/03/optuna-hyperparmeter.html",
    "href": "blog/2024/03/optuna-hyperparmeter.html",
    "title": "Super-charge Deep learning hyper-parameter search with Optuna",
    "section": "",
    "text": "Training machine learning sometimes involves various hyperparameter settings. Performing a hyperparameter search is an integral element in building machine learning models. It consists of attuning different sets of parameters to find the best settings for best model performance. It should be remarked that deep neural networks can involve many hyperparameter settings. Getting the best set parameters for such a high dimensional space might a challenging task. Opportunely, different strategies and tools can be used to simplify the process. This post will guide you on how to use Optuna for a hyper-parameter search using PyTorch and PyTorch lightning framework. The notebook with all the code for this post can be found on this colab link.\n\n\nOptuna is an open-source hyperparameter optimization framework. It automates the process of searching for optimal hyperparameter using Python conditionals, loops, and syntax. The optuna library offers efficiently hyper-parameter search in large spaces while pruning unpromising trials for faster results. It is also possible to run a hyperparameter search over multiple processes without modifying code. For a brief introduction of optuna, you can watch this video.\n\nyoutube: https://youtu.be/J_aymk4YXhg\n\nLet \\(x\\) and \\(y\\) denote the length and width of a rectangular garden respectively. The area of the garden is \\(A = x \\cdot y\\). We want to find the maximum possible area subject to two constraints:\n\nThe perimeter of the garden must be exactly 500m: \\(2x + 2y = 500\\)\nThe ratio of length to width must be between 1:2 and 2:1 for aesthetic purposes\n\nTherefore, our constraints can be written as: \\[\n\\begin{aligned}\n500 &= 2x + 2y \\quad \\text{(perimeter constraint)}\\\\\n0.5 &\\leq \\frac{x}{y} \\leq 2 \\quad \\text{(ratio constraint)}\\\\\nx, y &&gt; 0 \\quad \\text{(positive dimensions)}\n\\end{aligned}\n\\]\nFrom the perimeter constraint, we can express \\(y\\) in terms of \\(x\\): \\[y = 250 - x\\]\nOur objective function remains: \\[A(x,y) = x \\cdot y = x(250-x)\\]\nThis gives us a two-parameter optimization problem where we need to: 1. Maximize \\(A(x,y)\\) 2. Ensure \\(x,y &gt; 0\\) 3. Maintain \\(0.5 \\leq \\frac{x}{y} \\leq 2\\)\nThis is analogous to hyperparameter optimization where we often need to: 1. Maximize model performance 2. Respect valid parameter ranges 3. Maintain relationships between parameters\n\n\nShow the code\nimport optuna\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef objective(trial):\n    # Define the parameters\n    x = trial.suggest_float('x', 0, 250)  # x cannot be larger than 250 (half the perimeter)\n    y = trial.suggest_float('y', 0, 250)  # y cannot be larger than 250\n    \n    # Calculate area\n    area = x * y\n    \n    # Calculate constraint violations\n    perimeter_violation = abs(2*x + 2*y - 500)  # Should be 0\n    ratio = x/y if y != 0 else float('inf')\n    ratio_violation = 0\n    if ratio &lt; 0.5:\n        ratio_violation = 0.5 - ratio\n    elif ratio &gt; 2:\n        ratio_violation = ratio - 2\n        \n    # Add penalties for constraint violations\n    penalty = 1000 * (perimeter_violation + ratio_violation)\n    \n    # We minimize negative area (equivalent to maximizing area)\n    # while penalizing constraint violations\n    return -area + penalty\n\n# Create and run the study\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\n\n[I 2025-10-11 21:55:14,238] A new study created in memory with name: no-name-b3710e9e-52e7-43ff-a5ad-6b931f93f5c2\n[I 2025-10-11 21:55:14,249] Trial 0 finished with value: 110329.64383994589 and parameters: {'x': 54.67087907256868, 'y': 136.48318680674706}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,250] Trial 1 finished with value: 234866.30379776255 and parameters: {'x': 136.60283303076554, 'y': 247.75214104629669}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,251] Trial 2 finished with value: 240804.98616892047 and parameters: {'x': 31.979462540502464, 'y': 96.16413035176174}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,252] Trial 3 finished with value: -7865.849322774017 and parameters: {'x': 49.909476197747075, 'y': 199.17773154225708}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,252] Trial 4 finished with value: 48408.142534433995 and parameters: {'x': 87.78621485789515, 'y': 194.94996687648646}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,253] Trial 5 finished with value: 332375.92764577566 and parameters: {'x': 55.02600459803997, 'y': 28.0152479965359}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,253] Trial 6 finished with value: 61310.59732051972 and parameters: {'x': 214.89180520758, 'y': 73.15494881504308}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,254] Trial 7 finished with value: 97615.29934188214 and parameters: {'x': 152.54108212136714, 'y': 158.34351295619206}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,254] Trial 8 finished with value: 153187.07731964788 and parameters: {'x': 148.09668489388423, 'y': 25.353102859717257}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,249] Trial 0 finished with value: 110329.64383994589 and parameters: {'x': 54.67087907256868, 'y': 136.48318680674706}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,250] Trial 1 finished with value: 234866.30379776255 and parameters: {'x': 136.60283303076554, 'y': 247.75214104629669}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,251] Trial 2 finished with value: 240804.98616892047 and parameters: {'x': 31.979462540502464, 'y': 96.16413035176174}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,252] Trial 3 finished with value: -7865.849322774017 and parameters: {'x': 49.909476197747075, 'y': 199.17773154225708}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,252] Trial 4 finished with value: 48408.142534433995 and parameters: {'x': 87.78621485789515, 'y': 194.94996687648646}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,253] Trial 5 finished with value: 332375.92764577566 and parameters: {'x': 55.02600459803997, 'y': 28.0152479965359}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,253] Trial 6 finished with value: 61310.59732051972 and parameters: {'x': 214.89180520758, 'y': 73.15494881504308}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,254] Trial 7 finished with value: 97615.29934188214 and parameters: {'x': 152.54108212136714, 'y': 158.34351295619206}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,254] Trial 8 finished with value: 153187.07731964788 and parameters: {'x': 148.09668489388423, 'y': 25.353102859717257}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,255] Trial 9 finished with value: 250023.91956192412 and parameters: {'x': 150.8201035172419, 'y': 242.47706422738582}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,255] Trial 9 finished with value: 250023.91956192412 and parameters: {'x': 150.8201035172419, 'y': 242.47706422738582}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,279] Trial 10 finished with value: 102317.0839826822 and parameters: {'x': 4.299011567205767, 'y': 194.36360155660202}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,282] Trial 11 finished with value: 46499.31734010883 and parameters: {'x': 91.47222521006, 'y': 190.4793280946149}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,286] Trial 12 finished with value: 78214.574610294 and parameters: {'x': 107.39013415294149, 'y': 192.02811570558518}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,288] Trial 13 finished with value: -2932.474079994916 and parameters: {'x': 80.07406305607542, 'y': 164.80108615054158}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,279] Trial 10 finished with value: 102317.0839826822 and parameters: {'x': 4.299011567205767, 'y': 194.36360155660202}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,282] Trial 11 finished with value: 46499.31734010883 and parameters: {'x': 91.47222521006, 'y': 190.4793280946149}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,286] Trial 12 finished with value: 78214.574610294 and parameters: {'x': 107.39013415294149, 'y': 192.02811570558518}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,288] Trial 13 finished with value: -2932.474079994916 and parameters: {'x': 80.07406305607542, 'y': 164.80108615054158}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,291] Trial 14 finished with value: 183686.25303597865 and parameters: {'x': 200.83996627193682, 'y': 156.74332199774133}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,294] Trial 15 finished with value: 276992.8041374956 and parameters: {'x': 6.382565648739956, 'y': 105.0055383448107}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,299] Trial 16 finished with value: 39754.85361933456 and parameters: {'x': 61.77311098313433, 'y': 214.62732237428844}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,301] Trial 17 finished with value: 24355.52913335966 and parameters: {'x': 110.66227636483784, 'y': 160.3900523515934}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,303] Trial 18 finished with value: 268460.2595153113 and parameters: {'x': 183.63041225931633, 'y': 220.87984609768552}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,291] Trial 14 finished with value: 183686.25303597865 and parameters: {'x': 200.83996627193682, 'y': 156.74332199774133}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,294] Trial 15 finished with value: 276992.8041374956 and parameters: {'x': 6.382565648739956, 'y': 105.0055383448107}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,299] Trial 16 finished with value: 39754.85361933456 and parameters: {'x': 61.77311098313433, 'y': 214.62732237428844}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,301] Trial 17 finished with value: 24355.52913335966 and parameters: {'x': 110.66227636483784, 'y': 160.3900523515934}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,303] Trial 18 finished with value: 268460.2595153113 and parameters: {'x': 183.63041225931633, 'y': 220.87984609768552}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,305] Trial 19 finished with value: 227786.08602433506 and parameters: {'x': 247.8363554823907, 'y': 132.47242960771345}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,307] Trial 20 finished with value: 293420.3407497198 and parameters: {'x': 32.34129010254482, 'y': 69.83767313887887}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,309] Trial 21 finished with value: 13056.06086696031 and parameters: {'x': 103.02710635457007, 'y': 161.8377622507046}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,313] Trial 22 finished with value: -9281.403497550622 and parameters: {'x': 81.47959365021738, 'y': 170.82767409557408}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,316] Trial 23 finished with value: 62694.78748760582 and parameters: {'x': 74.17065913936422, 'y': 215.0753025959735}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,305] Trial 19 finished with value: 227786.08602433506 and parameters: {'x': 247.8363554823907, 'y': 132.47242960771345}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,307] Trial 20 finished with value: 293420.3407497198 and parameters: {'x': 32.34129010254482, 'y': 69.83767313887887}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,309] Trial 21 finished with value: 13056.06086696031 and parameters: {'x': 103.02710635457007, 'y': 161.8377622507046}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,313] Trial 22 finished with value: -9281.403497550622 and parameters: {'x': 81.47959365021738, 'y': 170.82767409557408}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,316] Trial 23 finished with value: 62694.78748760582 and parameters: {'x': 74.17065913936422, 'y': 215.0753025959735}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,319] Trial 24 finished with value: 97046.28119502733 and parameters: {'x': 31.30528507796069, 'y': 167.70324009321678}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,322] Trial 25 finished with value: -2633.5706946751707 and parameters: {'x': 76.72520351493031, 'y': 178.78109781791116}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,324] Trial 26 finished with value: 191715.51220414293 and parameters: {'x': 38.62156448870921, 'y': 113.41036305932678}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,326] Trial 27 finished with value: -2657.111581815865 and parameters: {'x': 117.04965931966672, 'y': 139.80377713186934}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,328] Trial 28 finished with value: 87402.23770762346 and parameters: {'x': 81.7470382449801, 'y': 220.91882649497086}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,319] Trial 24 finished with value: 97046.28119502733 and parameters: {'x': 31.30528507796069, 'y': 167.70324009321678}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,322] Trial 25 finished with value: -2633.5706946751707 and parameters: {'x': 76.72520351493031, 'y': 178.78109781791116}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,324] Trial 26 finished with value: 191715.51220414293 and parameters: {'x': 38.62156448870921, 'y': 113.41036305932678}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,326] Trial 27 finished with value: -2657.111581815865 and parameters: {'x': 117.04965931966672, 'y': 139.80377713186934}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,328] Trial 28 finished with value: 87402.23770762346 and parameters: {'x': 81.7470382449801, 'y': 220.91882649497086}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,332] Trial 29 finished with value: 84356.45758846769 and parameters: {'x': 61.664366656156375, 'y': 141.8174552358999}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,335] Trial 30 finished with value: 94720.11122531281 and parameters: {'x': 129.888349768919, 'y': 179.10343034657177}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,338] Trial 31 finished with value: -2648.512664963435 and parameters: {'x': 122.1561704945539, 'y': 134.7498350874949}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,341] Trial 32 finished with value: 3557.745775427309 and parameters: {'x': 96.90550061005331, 'y': 144.3227903767821}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,332] Trial 29 finished with value: 84356.45758846769 and parameters: {'x': 61.664366656156375, 'y': 141.8174552358999}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,335] Trial 30 finished with value: 94720.11122531281 and parameters: {'x': 129.888349768919, 'y': 179.10343034657177}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,338] Trial 31 finished with value: -2648.512664963435 and parameters: {'x': 122.1561704945539, 'y': 134.7498350874949}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,341] Trial 32 finished with value: 3557.745775427309 and parameters: {'x': 96.90550061005331, 'y': 144.3227903767821}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,344] Trial 33 finished with value: 156621.7892139885 and parameters: {'x': 45.453527057605264, 'y': 123.49490878319249}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,351] Trial 34 finished with value: 70497.04014797616 and parameters: {'x': 121.34975098791644, 'y': 174.48566509094522}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,354] Trial 35 finished with value: 86636.50737729092 and parameters: {'x': 61.99215102287928, 'y': 238.60172698421275}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,359] Trial 36 finished with value: 277515.15548766 and parameters: {'x': 20.99845563911422, 'y': 89.43754968694287}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,361] Trial 37 finished with value: 32547.90197145704 and parameters: {'x': 71.00462051315202, 'y': 202.37970169600936}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,344] Trial 33 finished with value: 156621.7892139885 and parameters: {'x': 45.453527057605264, 'y': 123.49490878319249}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,351] Trial 34 finished with value: 70497.04014797616 and parameters: {'x': 121.34975098791644, 'y': 174.48566509094522}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,354] Trial 35 finished with value: 86636.50737729092 and parameters: {'x': 61.99215102287928, 'y': 238.60172698421275}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,359] Trial 36 finished with value: 277515.15548766 and parameters: {'x': 20.99845563911422, 'y': 89.43754968694287}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,361] Trial 37 finished with value: 32547.90197145704 and parameters: {'x': 71.00462051315202, 'y': 202.37970169600936}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,364] Trial 38 finished with value: 111647.77528873675 and parameters: {'x': 172.06903479058235, 'y': 146.34562836289865}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,366] Trial 39 finished with value: 421640.791938946 and parameters: {'x': 47.67157727607591, 'y': 2.0554452052293755}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,368] Trial 40 finished with value: 12449.578841327293 and parameters: {'x': 141.2482619627448, 'y': 123.71370001173806}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,370] Trial 41 finished with value: -4455.491815555131 and parameters: {'x': 115.51064515900063, 'y': 129.25210379974368}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,372] Trial 42 finished with value: -4528.211666841316 and parameters: {'x': 93.47921906293199, 'y': 151.69473412930537}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,374] Trial 43 finished with value: 33294.75185330259 and parameters: {'x': 91.94103324290091, 'y': 183.12467877308632}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,364] Trial 38 finished with value: 111647.77528873675 and parameters: {'x': 172.06903479058235, 'y': 146.34562836289865}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,366] Trial 39 finished with value: 421640.791938946 and parameters: {'x': 47.67157727607591, 'y': 2.0554452052293755}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,368] Trial 40 finished with value: 12449.578841327293 and parameters: {'x': 141.2482619627448, 'y': 123.71370001173806}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,370] Trial 41 finished with value: -4455.491815555131 and parameters: {'x': 115.51064515900063, 'y': 129.25210379974368}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,372] Trial 42 finished with value: -4528.211666841316 and parameters: {'x': 93.47921906293199, 'y': 151.69473412930537}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,374] Trial 43 finished with value: 33294.75185330259 and parameters: {'x': 91.94103324290091, 'y': 183.12467877308632}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,377] Trial 44 finished with value: 165590.6180369739 and parameters: {'x': 83.77054247840178, 'y': 80.07997694782276}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,380] Trial 45 finished with value: -85.79789319858537 and parameters: {'x': 105.30322890942762, 'y': 152.6934276250563}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,382] Trial 46 finished with value: 85727.48996863229 and parameters: {'x': 133.42823622233726, 'y': 170.8324446516915}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,384] Trial 47 finished with value: 71999.95671753245 and parameters: {'x': 95.1256534190648, 'y': 200.39292682394353}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,386] Trial 48 finished with value: 158636.8308695582 and parameters: {'x': 53.28255423389643, 'y': 114.36914902337229}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,388] Trial 49 finished with value: 33713.37709443542 and parameters: {'x': 68.17552496105833, 'y': 205.60559010881772}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,377] Trial 44 finished with value: 165590.6180369739 and parameters: {'x': 83.77054247840178, 'y': 80.07997694782276}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,380] Trial 45 finished with value: -85.79789319858537 and parameters: {'x': 105.30322890942762, 'y': 152.6934276250563}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,382] Trial 46 finished with value: 85727.48996863229 and parameters: {'x': 133.42823622233726, 'y': 170.8324446516915}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,384] Trial 47 finished with value: 71999.95671753245 and parameters: {'x': 95.1256534190648, 'y': 200.39292682394353}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,386] Trial 48 finished with value: 158636.8308695582 and parameters: {'x': 53.28255423389643, 'y': 114.36914902337229}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,388] Trial 49 finished with value: 33713.37709443542 and parameters: {'x': 68.17552496105833, 'y': 205.60559010881772}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,392] Trial 50 finished with value: 360077.999142224 and parameters: {'x': 13.294709863328642, 'y': 56.42341212045946}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,394] Trial 51 finished with value: 13096.081913502305 and parameters: {'x': 113.49976297123348, 'y': 151.6546620856177}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,396] Trial 52 finished with value: 166660.71845241755 and parameters: {'x': 161.36447903100517, 'y': 187.05815071447597}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,398] Trial 53 finished with value: 35603.420190956786 and parameters: {'x': 112.82781698573278, 'y': 164.23927239348671}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,400] Trial 54 finished with value: 46868.83386208502 and parameters: {'x': 88.17386614377163, 'y': 132.54807865281228}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,402] Trial 55 finished with value: 33614.82149739127 and parameters: {'x': 121.35321964444411, 'y': 105.44153474413415}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,392] Trial 50 finished with value: 360077.999142224 and parameters: {'x': 13.294709863328642, 'y': 56.42341212045946}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,394] Trial 51 finished with value: 13096.081913502305 and parameters: {'x': 113.49976297123348, 'y': 151.6546620856177}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,396] Trial 52 finished with value: 166660.71845241755 and parameters: {'x': 161.36447903100517, 'y': 187.05815071447597}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,398] Trial 53 finished with value: 35603.420190956786 and parameters: {'x': 112.82781698573278, 'y': 164.23927239348671}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,400] Trial 54 finished with value: 46868.83386208502 and parameters: {'x': 88.17386614377163, 'y': 132.54807865281228}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,402] Trial 55 finished with value: 33614.82149739127 and parameters: {'x': 121.35321964444411, 'y': 105.44153474413415}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,404] Trial 56 finished with value: 77199.33782515011 and parameters: {'x': 141.0635049139813, 'y': 158.7317957214744}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,408] Trial 57 finished with value: 61077.92732611317 and parameters: {'x': 98.16527851494139, 'y': 115.62078456264786}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,410] Trial 58 finished with value: 70517.62692325597 and parameters: {'x': 79.88365480279484, 'y': 129.67795715319832}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,412] Trial 59 finished with value: 72044.91339433355 and parameters: {'x': 65.46235405256009, 'y': 143.82989871576837}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,414] Trial 60 finished with value: 125223.53421001065 and parameters: {'x': 155.96524669073114, 'y': 169.89541019566946}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,404] Trial 56 finished with value: 77199.33782515011 and parameters: {'x': 141.0635049139813, 'y': 158.7317957214744}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,408] Trial 57 finished with value: 61077.92732611317 and parameters: {'x': 98.16527851494139, 'y': 115.62078456264786}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,410] Trial 58 finished with value: 70517.62692325597 and parameters: {'x': 79.88365480279484, 'y': 129.67795715319832}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,412] Trial 59 finished with value: 72044.91339433355 and parameters: {'x': 65.46235405256009, 'y': 143.82989871576837}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,414] Trial 60 finished with value: 125223.53421001065 and parameters: {'x': 155.96524669073114, 'y': 169.89541019566946}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,416] Trial 61 finished with value: -6824.146233345498 and parameters: {'x': 121.86168938425372, 'y': 132.81901209733817}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,418] Trial 62 finished with value: 56627.175655164894 and parameters: {'x': 111.83609783751182, 'y': 104.03299237795093}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,421] Trial 63 finished with value: 2277.572671508555 and parameters: {'x': 103.62452361172737, 'y': 138.08233211044762}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,423] Trial 64 finished with value: 41200.44874570341 and parameters: {'x': 128.05137353715068, 'y': 152.29996039480525}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,426] Trial 65 finished with value: 111095.24843211919 and parameters: {'x': 84.33575151235277, 'y': 230.88024165157458}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,416] Trial 61 finished with value: -6824.146233345498 and parameters: {'x': 121.86168938425372, 'y': 132.81901209733817}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,418] Trial 62 finished with value: 56627.175655164894 and parameters: {'x': 111.83609783751182, 'y': 104.03299237795093}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,421] Trial 63 finished with value: 2277.572671508555 and parameters: {'x': 103.62452361172737, 'y': 138.08233211044762}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,423] Trial 64 finished with value: 41200.44874570341 and parameters: {'x': 128.05137353715068, 'y': 152.29996039480525}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,426] Trial 65 finished with value: 111095.24843211919 and parameters: {'x': 84.33575151235277, 'y': 230.88024165157458}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,428] Trial 66 finished with value: 136085.05720613495 and parameters: {'x': 54.8828639379639, 'y': 123.70805709749239}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,430] Trial 67 finished with value: 103097.87950344205 and parameters: {'x': 117.74446132855665, 'y': 195.30236426123042}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,433] Trial 68 finished with value: 303377.3024283395 and parameters: {'x': 246.24484901435858, 'y': 177.26967428999305}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,434] Trial 69 finished with value: 10977.480859288014 and parameters: {'x': 73.67482159621474, 'y': 164.7923752308203}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,437] Trial 70 finished with value: 50855.52949973527 and parameters: {'x': 137.00677152081866, 'y': 148.60064020956887}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,428] Trial 66 finished with value: 136085.05720613495 and parameters: {'x': 54.8828639379639, 'y': 123.70805709749239}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,430] Trial 67 finished with value: 103097.87950344205 and parameters: {'x': 117.74446132855665, 'y': 195.30236426123042}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,433] Trial 68 finished with value: 303377.3024283395 and parameters: {'x': 246.24484901435858, 'y': 177.26967428999305}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,434] Trial 69 finished with value: 10977.480859288014 and parameters: {'x': 73.67482159621474, 'y': 164.7923752308203}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,437] Trial 70 finished with value: 50855.52949973527 and parameters: {'x': 137.00677152081866, 'y': 148.60064020956887}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,439] Trial 71 finished with value: 5243.806402325117 and parameters: {'x': 122.1346261412983, 'y': 138.9740488070607}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,443] Trial 72 finished with value: 17929.424358119904 and parameters: {'x': 101.88560683823891, 'y': 132.40461852918534}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,445] Trial 73 finished with value: 87473.3037098331 and parameters: {'x': 146.72051957450972, 'y': 158.65511255394003}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,447] Trial 74 finished with value: -766.110501912448 and parameters: {'x': 126.43859528404009, 'y': 116.57468994571198}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,449] Trial 75 finished with value: 119805.79362264856 and parameters: {'x': 91.93818551446344, 'y': 93.84495044252216}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,439] Trial 71 finished with value: 5243.806402325117 and parameters: {'x': 122.1346261412983, 'y': 138.9740488070607}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,443] Trial 72 finished with value: 17929.424358119904 and parameters: {'x': 101.88560683823891, 'y': 132.40461852918534}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,445] Trial 73 finished with value: 87473.3037098331 and parameters: {'x': 146.72051957450972, 'y': 158.65511255394003}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,447] Trial 74 finished with value: -766.110501912448 and parameters: {'x': 126.43859528404009, 'y': 116.57468994571198}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,449] Trial 75 finished with value: 119805.79362264856 and parameters: {'x': 91.93818551446344, 'y': 93.84495044252216}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,451] Trial 76 finished with value: -8399.783327793417 and parameters: {'x': 107.71826439198293, 'y': 138.9954528046656}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,454] Trial 77 finished with value: 60489.11918196906 and parameters: {'x': 106.23729130438943, 'y': 183.7688190686236}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,457] Trial 78 finished with value: 171076.77872942304 and parameters: {'x': 35.779486297985486, 'y': 126.52719482207904}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,459] Trial 79 finished with value: 52847.2520340949 and parameters: {'x': 77.69760971490919, 'y': 140.42347990000343}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,451] Trial 76 finished with value: -8399.783327793417 and parameters: {'x': 107.71826439198293, 'y': 138.9954528046656}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,454] Trial 77 finished with value: 60489.11918196906 and parameters: {'x': 106.23729130438943, 'y': 183.7688190686236}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,457] Trial 78 finished with value: 171076.77872942304 and parameters: {'x': 35.779486297985486, 'y': 126.52719482207904}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,459] Trial 79 finished with value: 52847.2520340949 and parameters: {'x': 77.69760971490919, 'y': 140.42347990000343}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,461] Trial 80 finished with value: 231536.46682359578 and parameters: {'x': 26.002425981145805, 'y': 106.96709458579718}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,464] Trial 81 finished with value: -12271.531168115056 and parameters: {'x': 115.97071443886341, 'y': 135.7659575222446}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,466] Trial 82 finished with value: 25648.4196491222 and parameters: {'x': 116.75348131432634, 'y': 155.12650846388203}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,468] Trial 83 finished with value: 38995.20220216554 and parameters: {'x': 133.66260293592967, 'y': 145.5631745672939}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,470] Trial 84 finished with value: 73367.3478536114 and parameters: {'x': 88.56529799970767, 'y': 119.4609794512674}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,471] Trial 85 finished with value: 21480.360582070996 and parameters: {'x': 98.17303743887757, 'y': 170.95892113469102}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,461] Trial 80 finished with value: 231536.46682359578 and parameters: {'x': 26.002425981145805, 'y': 106.96709458579718}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,464] Trial 81 finished with value: -12271.531168115056 and parameters: {'x': 115.97071443886341, 'y': 135.7659575222446}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,466] Trial 82 finished with value: 25648.4196491222 and parameters: {'x': 116.75348131432634, 'y': 155.12650846388203}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,468] Trial 83 finished with value: 38995.20220216554 and parameters: {'x': 133.66260293592967, 'y': 145.5631745672939}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,470] Trial 84 finished with value: 73367.3478536114 and parameters: {'x': 88.56529799970767, 'y': 119.4609794512674}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,471] Trial 85 finished with value: 21480.360582070996 and parameters: {'x': 98.17303743887757, 'y': 170.95892113469102}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,474] Trial 86 finished with value: 120636.83202700765 and parameters: {'x': 110.88965267299128, 'y': 211.13511301517534}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,474] Trial 86 finished with value: 120636.83202700765 and parameters: {'x': 110.88965267299128, 'y': 211.13511301517534}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,477] Trial 87 finished with value: -3662.3065250383097 and parameters: {'x': 107.74052703883869, 'y': 136.72520348234053}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,477] Trial 87 finished with value: -3662.3065250383097 and parameters: {'x': 107.74052703883869, 'y': 136.72520348234053}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,491] Trial 88 finished with value: 130218.77873830058 and parameters: {'x': 47.844443349902335, 'y': 133.9140080154505}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,494] Trial 89 finished with value: 69695.28534620462 and parameters: {'x': 100.58943050715362, 'y': 109.07693350821505}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,491] Trial 88 finished with value: 130218.77873830058 and parameters: {'x': 47.844443349902335, 'y': 133.9140080154505}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,494] Trial 89 finished with value: 69695.28534620462 and parameters: {'x': 100.58943050715362, 'y': 109.07693350821505}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,519] Trial 90 finished with value: 13611.392790075282 and parameters: {'x': 108.13069942866558, 'y': 128.13589234567004}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,527] Trial 91 finished with value: 15039.272005873652 and parameters: {'x': 116.74743302781677, 'y': 149.49901616377232}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,519] Trial 90 finished with value: 13611.392790075282 and parameters: {'x': 108.13069942866558, 'y': 128.13589234567004}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,527] Trial 91 finished with value: 15039.272005873652 and parameters: {'x': 116.74743302781677, 'y': 149.49901616377232}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,530] Trial 92 finished with value: 27139.50150610523 and parameters: {'x': 93.5449926828245, 'y': 136.50077458427972}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,535] Trial 93 finished with value: 57642.25899426801 and parameters: {'x': 128.49742108919992, 'y': 160.6449385663378}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,548] Trial 94 finished with value: 24324.04538916787 and parameters: {'x': 86.72482250555704, 'y': 144.83285306245176}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,551] Trial 95 finished with value: 29066.77895981224 and parameters: {'x': 106.0080650826467, 'y': 122.942117443759}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,553] Trial 96 finished with value: 76592.47124886255 and parameters: {'x': 143.6417217726292, 'y': 155.8476244035522}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,555] Trial 97 finished with value: 166286.48848715823 and parameters: {'x': 63.13051877558448, 'y': 100.55227823627494}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,530] Trial 92 finished with value: 27139.50150610523 and parameters: {'x': 93.5449926828245, 'y': 136.50077458427972}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,535] Trial 93 finished with value: 57642.25899426801 and parameters: {'x': 128.49742108919992, 'y': 160.6449385663378}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,548] Trial 94 finished with value: 24324.04538916787 and parameters: {'x': 86.72482250555704, 'y': 144.83285306245176}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,551] Trial 95 finished with value: 29066.77895981224 and parameters: {'x': 106.0080650826467, 'y': 122.942117443759}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,553] Trial 96 finished with value: 76592.47124886255 and parameters: {'x': 143.6417217726292, 'y': 155.8476244035522}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,555] Trial 97 finished with value: 166286.48848715823 and parameters: {'x': 63.13051877558448, 'y': 100.55227823627494}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,558] Trial 98 finished with value: 10914.151651453536 and parameters: {'x': 124.07737702457155, 'y': 140.06942204553397}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,561] Trial 99 finished with value: -3362.357981400788 and parameters: {'x': 81.73704868231124, 'y': 173.66451510052832}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,558] Trial 98 finished with value: 10914.151651453536 and parameters: {'x': 124.07737702457155, 'y': 140.06942204553397}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,561] Trial 99 finished with value: -3362.357981400788 and parameters: {'x': 81.73704868231124, 'y': 173.66451510052832}. Best is trial 81 with value: -12271.531168115056.\n\n\n\n\nShow the code\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create data for constraints\nx_grid = np.linspace(0, 250, 100)\ny_perimeter = 250 - x_grid  # perimeter constraint\ny_ratio_min = x_grid/2      # ratio 2:1 constraint\ny_ratio_max = 2*x_grid      # ratio 1:2 constraint\n\n# Create DataFrame for constraints\nconstraint_df = pd.DataFrame({\n    'x': np.concatenate([x_grid, x_grid, x_grid]),\n    'y': np.concatenate([y_perimeter, y_ratio_min, y_ratio_max]),\n    'Constraint': np.repeat(['Perimeter', 'Ratio (2:1)', 'Ratio (1:2)'], len(x_grid))\n})\n\n# Create DataFrame for trial points\ntrials_df = pd.DataFrame({\n    'x': [t.params['x'] for t in study.trials],\n    'y': [t.params['y'] for t in study.trials],\n    'Area': [-t.value for t in study.trials]  # Negative because we minimized negative area\n})\n\n# Create DataFrame for best point\nbest_point_df = pd.DataFrame({\n    'x': [study.best_params['x']],\n    'y': [study.best_params['y']]\n})\n\n# Create the base plot\nbase = alt.Chart().encode(\n    x=alt.X('x', title='Length (meters)', scale=alt.Scale(domain=[0, 250])),\n    y=alt.Y('y', title='Width (meters)', scale=alt.Scale(domain=[0, 250]))\n)\n\n# Plot constraints\nconstraints = base.mark_line(strokeDash=[4, 4]).encode(\n    color=alt.Color('Constraint:N', legend=alt.Legend(title='Constraints')),\n).transform_filter(\n    alt.datum.y &gt;= 0\n).data(constraint_df)\n\n# Plot trial points\npoints = base.mark_circle().encode(\n    color=alt.Color('Area:Q', scale=alt.Scale(scheme='viridis'), \n                   legend=alt.Legend(title='Area (sq meters)')),\n    tooltip=['x:Q', 'y:Q', 'Area:Q']\n).data(trials_df)\n\n# Plot best point\nbest_point = base.mark_star(\n    size=200,\n    color='red'\n).encode(\n    tooltip=['x:Q', 'y:Q']\n).data(best_point_df)\n\n# Combine all layers\nchart = (constraints + points + best_point).properties(\n    width=500,\n    height=500,\n    title='Optimization Search Space'\n)\n\nchart\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [9], in &lt;cell line: 1&gt;()\n----&gt; 1 import altair as alt\n      2 import pandas as pd\n      3 import numpy as np\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/__init__.py:4, in &lt;module&gt;\n      1 # flake8: noqa\n      2 __version__ = \"4.2.2\"\n----&gt; 4 from .vegalite import *\n      5 from . import examples\n      8 def load_ipython_extension(ipython):\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/vegalite/__init__.py:2, in &lt;module&gt;\n      1 # flake8: noqa\n----&gt; 2 from .v4 import *\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/vegalite/v4/__init__.py:2, in &lt;module&gt;\n      1 # flake8: noqa\n----&gt; 2 from .schema import *\n      3 from .api import *\n      5 from ...datasets import list_datasets, load_dataset\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/vegalite/v4/schema/__init__.py:2, in &lt;module&gt;\n      1 # flake8: noqa\n----&gt; 2 from .core import *\n      3 from .channels import *\n      4 SCHEMA_VERSION = 'v4.17.0'\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/vegalite/v4/schema/core.py:4, in &lt;module&gt;\n      1 # The contents of this file are automatically written by\n      2 # tools/generate_schema_wrapper.py. Do not modify directly.\n----&gt; 4 from altair.utils.schemapi import SchemaBase, Undefined, _subclasses\n      6 import pkgutil\n      7 import json\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/utils/__init__.py:1, in &lt;module&gt;\n----&gt; 1 from .core import (\n      2     infer_vegalite_type,\n      3     infer_encoding_types,\n      4     sanitize_dataframe,\n      5     parse_shorthand,\n      6     use_signature,\n      7     update_subtraits,\n      8     update_nested,\n      9     display_traceback,\n     10     SchemaBase,\n     11     Undefined,\n     12 )\n     13 from .html import spec_to_html\n     14 from .plugin_registry import PluginRegistry\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/utils/core.py:14, in &lt;module&gt;\n     11 import warnings\n     13 import jsonschema\n---&gt; 14 import pandas as pd\n     15 import numpy as np\n     17 from .schemapi import SchemaBase, Undefined\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/__init__.py:22, in &lt;module&gt;\n     19 del _hard_dependencies, _dependency, _missing_dependencies\n     21 # numpy compat\n---&gt; 22 from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401\n     24 try:\n     25     from pandas._libs import hashtable as _hashtable, lib as _lib, tslib as _tslib\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/compat/__init__.py:18, in &lt;module&gt;\n     15 from typing import TYPE_CHECKING\n     17 from pandas._typing import F\n---&gt; 18 from pandas.compat.numpy import (\n     19     is_numpy_dev,\n     20     np_version_under1p21,\n     21 )\n     22 from pandas.compat.pyarrow import (\n     23     pa_version_under1p01,\n     24     pa_version_under2p0,\n   (...)\n     31     pa_version_under9p0,\n     32 )\n     34 if TYPE_CHECKING:\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:4, in &lt;module&gt;\n      1 \"\"\" support numpy compatibility across versions \"\"\"\n      2 import numpy as np\n----&gt; 4 from pandas.util.version import Version\n      6 # numpy versioning\n      7 _np_version = np.__version__\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/__init__.py:2, in &lt;module&gt;\n      1 # pyright: reportUnusedImport = false\n----&gt; 2 from pandas.util._decorators import (  # noqa:F401\n      3     Appender,\n      4     Substitution,\n      5     cache_readonly,\n      6 )\n      8 from pandas.core.util.hashing import (  # noqa:F401\n      9     hash_array,\n     10     hash_pandas_object,\n     11 )\n     14 def __getattr__(name):\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:14, in &lt;module&gt;\n      6 from typing import (\n      7     Any,\n      8     Callable,\n      9     Mapping,\n     10     cast,\n     11 )\n     12 import warnings\n---&gt; 14 from pandas._libs.properties import cache_readonly\n     15 from pandas._typing import (\n     16     F,\n     17     T,\n     18 )\n     19 from pandas.util._exceptions import find_stack_level\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/__init__.py:13, in &lt;module&gt;\n      1 __all__ = [\n      2     \"NaT\",\n      3     \"NaTType\",\n   (...)\n      9     \"Interval\",\n     10 ]\n---&gt; 13 from pandas._libs.interval import Interval\n     14 from pandas._libs.tslibs import (\n     15     NaT,\n     16     NaTType,\n   (...)\n     21     iNaT,\n     22 )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/interval.pyx:1, in init pandas._libs.interval()\n\nValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n\n\n\n\n\nShow the code\n# Print the results\nbest_x = study.best_params['x']\nbest_y = study.best_params['y']\nbest_area = best_x * best_y\n\nprint(f\"Best solution found:\")\nprint(f\"x (length) = {best_x:.2f} meters\")\nprint(f\"y (width) = {best_y:.2f} meters\")\nprint(f\"Area = {best_area:.2f} square meters\")\nprint(f\"Perimeter = {2*best_x + 2*best_y:.2f} meters\")\nprint(f\"Length/Width ratio = {best_x/best_y:.2f}\")\n\n# Create visualization\ntrials_data = np.array([[t.params['x'], t.params['y'], t.value] for t in study.trials])\nx_values = trials_data[:, 0]\ny_values = trials_data[:, 1]\nobjective_values = -trials_data[:, 2]  # Negative because we minimized negative area\n\n# Create subplots\nfig = make_subplots(rows=1, cols=1, subplot_titles=('Trial Progress', 'Search Space'))\n\n\n\n# Plot 2: Search space with constraints\nx_grid = np.linspace(0, 250, 100)\ny_grid = 250 - x_grid  # Perimeter constraint\n\nfig.add_trace(\n    go.Scatter(x=x_values, y=y_values, \n               mode='markers',\n               marker=dict(color=objective_values, colorscale='Viridis'),\n               name='Trials'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=x_grid, y=y_grid,\n               mode='lines',\n               line=dict(color='red', dash='dash'),\n               name='Perimeter Constraint'),\n    row=1, col=2\n)\n\n# Add ratio constraints\ny_ratio_min = x_grid/2  # x/y = 2\ny_ratio_max = 2*x_grid  # x/y = 0.5\n\nfig.add_trace(\n    go.Scatter(x=x_grid, y=y_ratio_min,\n               mode='lines',\n               line=dict(color='orange', dash='dash'),\n               name='Ratio Constraint (2:1)'),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(x=x_grid, y=y_ratio_max,\n               mode='lines',\n               line=dict(color='orange', dash='dash'),\n               name='Ratio Constraint (1:2)'),\n    row=1, col=2\n)\n\n# Add best point\nfig.add_trace(\n    go.Scatter(x=[best_x], y=[best_y],\n               mode='markers',\n               marker=dict(color='red', size=10, symbol='star'),\n               name='Best Solution'),\n    row=1, col=2\n)\n\nfig.update_layout(height=500, width=1000, title_text=\"Optimization Results\")\nfig.update_xaxes(title_text=\"Trial Number\", row=1, col=1)\nfig.update_xaxes(title_text=\"x (length)\", row=1, col=2)\nfig.update_yaxes(title_text=\"Area\", row=1, col=1)\nfig.update_yaxes(title_text=\"y (width)\", row=1, col=2)\n\nfig.show()\n\n\nBest solution found:\nx (length) = 115.97 meters\ny (width) = 135.77 meters\nArea = 15744.88 square meters\nPerimeter = 503.47 meters\nLength/Width ratio = 0.85\n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/_subplots.py:1352, in _set_trace_grid_reference(trace, layout, grid_ref, row, col, secondary_y)\n   1351 try:\n-&gt; 1352     subplot_refs = grid_ref[row - 1][col - 1]\n   1353 except IndexError:\n\nIndexError: list index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nException                                 Traceback (most recent call last)\nInput In [8], in &lt;cell line: 36&gt;()\n     26 y_grid = 250 - x_grid  # Perimeter constraint\n     28 fig.add_trace(\n     29     go.Scatter(x=x_values, y=y_values, \n     30                mode='markers',\n   (...)\n     33     row=1, col=1\n     34 )\n---&gt; 36 fig.add_trace(\n     37     go.Scatter(x=x_grid, y=y_grid,\n     38                mode='lines',\n     39                line=dict(color='red', dash='dash'),\n     40                name='Perimeter Constraint'),\n     41     row=1, col=2\n     42 )\n     44 # Add ratio constraints\n     45 y_ratio_min = x_grid/2  # x/y = 2\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/graph_objs/_figure.py:865, in Figure.add_trace(self, trace, row, col, secondary_y, exclude_empty_subplots)\n    790 def add_trace(\n    791     self, trace, row=None, col=None, secondary_y=None, exclude_empty_subplots=False\n    792 ) -&gt; \"Figure\":\n    793     \"\"\"\n    794 \n    795     Add a trace to the figure\n   (...)\n    863 \n    864     \"\"\"\n--&gt; 865     return super(Figure, self).add_trace(\n    866         trace, row, col, secondary_y, exclude_empty_subplots\n    867     )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/basedatatypes.py:2097, in BaseFigure.add_trace(self, trace, row, col, secondary_y, exclude_empty_subplots)\n   2088         self.add_trace(\n   2089             trace,\n   2090             row=r,\n   (...)\n   2093             exclude_empty_subplots=exclude_empty_subplots,\n   2094         )\n   2095     return self\n-&gt; 2097 return self.add_traces(\n   2098     data=[trace],\n   2099     rows=[row] if row is not None else None,\n   2100     cols=[col] if col is not None else None,\n   2101     secondary_ys=[secondary_y] if secondary_y is not None else None,\n   2102     exclude_empty_subplots=exclude_empty_subplots,\n   2103 )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/graph_objs/_figure.py:945, in Figure.add_traces(self, data, rows, cols, secondary_ys, exclude_empty_subplots)\n    869 def add_traces(\n    870     self,\n    871     data,\n   (...)\n    875     exclude_empty_subplots=False,\n    876 ) -&gt; \"Figure\":\n    877     \"\"\"\n    878 \n    879     Add traces to the figure\n   (...)\n    943 \n    944     \"\"\"\n--&gt; 945     return super(Figure, self).add_traces(\n    946         data, rows, cols, secondary_ys, exclude_empty_subplots\n    947     )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/basedatatypes.py:2227, in BaseFigure.add_traces(self, data, rows, cols, secondary_ys, exclude_empty_subplots)\n   2225 if rows is not None:\n   2226     for trace, row, col, secondary_y in zip(data, rows, cols, secondary_ys):\n-&gt; 2227         self._set_trace_grid_position(trace, row, col, secondary_y)\n   2229 if exclude_empty_subplots:\n   2230     data = list(\n   2231         filter(\n   2232             lambda trace: self._subplot_not_empty(\n   (...)\n   2236         )\n   2237     )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/basedatatypes.py:2319, in BaseFigure._set_trace_grid_position(self, trace, row, col, secondary_y)\n   2316 from plotly._subplots import _set_trace_grid_reference\n   2318 grid_ref = self._validate_get_grid_ref()\n-&gt; 2319 return _set_trace_grid_reference(\n   2320     trace, self.layout, grid_ref, row, col, secondary_y\n   2321 )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/_subplots.py:1354, in _set_trace_grid_reference(trace, layout, grid_ref, row, col, secondary_y)\n   1352         subplot_refs = grid_ref[row - 1][col - 1]\n   1353     except IndexError:\n-&gt; 1354         raise Exception(\n   1355             \"The (row, col) pair sent is out of \"\n   1356             \"range. Use Figure.print_grid to view the \"\n   1357             \"subplot grid. \"\n   1358         )\n   1360     if not subplot_refs:\n   1361         raise ValueError(\n   1362             \"\"\"\n   1363 No subplot specified at grid position ({row}, {col})\"\"\".format(\n   1364                 row=row, col=col\n   1365             )\n   1366         )\n\nException: The (row, col) pair sent is out of range. Use Figure.print_grid to view the subplot grid. \n\n\n\n\n\nShow the code\nimport optuna\n\ndef objective(trial):\n    # Define the parameters with their constraints\n    x = trial.suggest_float('length', 0, 250)  # length can't be more than half the perimeter\n    y = trial.suggest_float('width', 0, 250)   # width can't be more than half the perimeter\n    \n    # Calculate area\n    area = x * y\n    \n    # Perimeter constraint penalty\n    perimeter_error = abs(2*x + 2*y - 500)\n    \n    # Ratio constraint penalty\n    ratio = x/y if y != 0 else float('inf')\n    ratio_error = 0\n    if ratio &lt; 0.5 or ratio &gt; 2:\n        ratio_error = min(abs(ratio - 0.5), abs(ratio - 2))\n    \n    # Return negative area (since Optuna minimizes) with penalties\n    return -area + 1000 * (perimeter_error + ratio_error)  # Large penalty for constraint violations\n\n# Create a study object and optimize\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n\n# Print results\nprint(f\"Best parameters: {study.best_params}\")\nprint(f\"Best value (negative area): {study.best_value}\")\nprint(f\"Actual area: {-study.best_value} square meters\")\nprint(f\"Perimeter: {2*study.best_params['length'] + 2*study.best_params['width']} meters\")\nprint(f\"Length/Width ratio: {study.best_params['length']/study.best_params['width']:.2f}\")\n\n\n\n\nShow the code\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import clear_output\n\ndef gardent_area(trial):\n    \"\"\"Objective function to maximize the garden area.\"\"\"\n    x = trial.suggest_uniform('x', 0, 250)\n    return (500*x - 2*x**2 ) \n\n\nOnce the objective function has been defined, the study object is used to start the optimization. The study is an optimization session, a set of trials. Optuna provide different sampler strategies such Tree-structured Parzen Estimator (TPE) sampler. A sampler has the responsibility to determine the parameter values to be evaluated in a trial. By default, Optuna uses TPE sampler, which is a form of Bayesian Optimization. The TPE provides a more efficient search than a random sampler search by choosing points closer to past good results. It possible to add a custom sampler as described in this link Let create a study and start the optimization process.\n\n\nShow the code\nstudy = optuna.create_study(study_name=\"garden\", direction=\"maximize\", sampler=optuna.samplers.TPESampler())\nstudy.optimize(gardent_area, n_trials=20)\n\n\n[I 2025-10-11 21:47:35,811] A new study created in memory with name: garden\n[I 2025-10-11 21:47:37,395] Trial 0 finished with value: 30246.525474019894 and parameters: {'x': 102.60050752829314}. Best is trial 0 with value: 30246.525474019894.\n[I 2025-10-11 21:47:37,396] Trial 1 finished with value: 28884.237809943665 and parameters: {'x': 90.60696153248207}. Best is trial 0 with value: 30246.525474019894.\n[I 2025-10-11 21:47:37,396] Trial 2 finished with value: 11385.14975589222 and parameters: {'x': 25.338447122002506}. Best is trial 0 with value: 30246.525474019894.\n[I 2025-10-11 21:47:37,396] Trial 3 finished with value: 31134.916939880397 and parameters: {'x': 117.41438663918338}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,397] Trial 4 finished with value: 28207.43211241709 and parameters: {'x': 164.0036401351394}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,398] Trial 5 finished with value: 30936.451045935133 and parameters: {'x': 112.47903849409194}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,398] Trial 6 finished with value: 11124.277714558026 and parameters: {'x': 24.68618668039288}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,400] Trial 7 finished with value: 30563.001391874684 and parameters: {'x': 106.46626578202181}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,400] Trial 8 finished with value: 13635.683343231602 and parameters: {'x': 31.153538540954038}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,400] Trial 9 finished with value: 23661.87149257028 and parameters: {'x': 186.59597595391162}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,403] Trial 10 finished with value: 460.2558044857433 and parameters: {'x': 249.0760738327786}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,404] Trial 11 finished with value: 30068.94348582362 and parameters: {'x': 149.30078717013478}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,406] Trial 12 finished with value: 25839.476920025998 and parameters: {'x': 72.98787122231008}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,407] Trial 13 finished with value: 16930.397894067137 and parameters: {'x': 209.61560762038192}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,409] Trial 14 finished with value: 30814.921648104246 and parameters: {'x': 139.74920933297372}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,410] Trial 15 finished with value: 24707.66600587945 and parameters: {'x': 67.80588319538211}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,414] Trial 16 finished with value: 31249.97209149898 and parameters: {'x': 124.88187188942693}. Best is trial 16 with value: 31249.97209149898.\n[I 2025-10-11 21:47:37,416] Trial 17 finished with value: 27127.961566034028 and parameters: {'x': 170.39844949976805}. Best is trial 16 with value: 31249.97209149898.\n[I 2025-10-11 21:47:37,417] Trial 18 finished with value: 22981.294702928128 and parameters: {'x': 60.701068060690645}. Best is trial 16 with value: 31249.97209149898.\n[I 2025-10-11 21:47:37,418] Trial 19 finished with value: 31074.348233056415 and parameters: {'x': 134.37154648240036}. Best is trial 16 with value: 31249.97209149898.\n\n\nOnce the study is completed, you can get the best parameters using study.best_params and study.best_value will give you the best value.\n\n\nShow the code\nprint(study.best_params)\nprint(study.best_value)\n\n\n{'x': 124.88187188942693}\n31249.97209149898"
  },
  {
    "objectID": "blog/2024/03/optuna-hyperparmeter.html#introduction",
    "href": "blog/2024/03/optuna-hyperparmeter.html#introduction",
    "title": "Super-charge Deep learning hyper-parameter search with Optuna",
    "section": "",
    "text": "Training machine learning sometimes involves various hyperparameter settings. Performing a hyperparameter search is an integral element in building machine learning models. It consists of attuning different sets of parameters to find the best settings for best model performance. It should be remarked that deep neural networks can involve many hyperparameter settings. Getting the best set parameters for such a high dimensional space might a challenging task. Opportunely, different strategies and tools can be used to simplify the process. This post will guide you on how to use Optuna for a hyper-parameter search using PyTorch and PyTorch lightning framework. The notebook with all the code for this post can be found on this colab link.\n\n\nOptuna is an open-source hyperparameter optimization framework. It automates the process of searching for optimal hyperparameter using Python conditionals, loops, and syntax. The optuna library offers efficiently hyper-parameter search in large spaces while pruning unpromising trials for faster results. It is also possible to run a hyperparameter search over multiple processes without modifying code. For a brief introduction of optuna, you can watch this video.\n\nyoutube: https://youtu.be/J_aymk4YXhg\n\nLet \\(x\\) and \\(y\\) denote the length and width of a rectangular garden respectively. The area of the garden is \\(A = x \\cdot y\\). We want to find the maximum possible area subject to two constraints:\n\nThe perimeter of the garden must be exactly 500m: \\(2x + 2y = 500\\)\nThe ratio of length to width must be between 1:2 and 2:1 for aesthetic purposes\n\nTherefore, our constraints can be written as: \\[\n\\begin{aligned}\n500 &= 2x + 2y \\quad \\text{(perimeter constraint)}\\\\\n0.5 &\\leq \\frac{x}{y} \\leq 2 \\quad \\text{(ratio constraint)}\\\\\nx, y &&gt; 0 \\quad \\text{(positive dimensions)}\n\\end{aligned}\n\\]\nFrom the perimeter constraint, we can express \\(y\\) in terms of \\(x\\): \\[y = 250 - x\\]\nOur objective function remains: \\[A(x,y) = x \\cdot y = x(250-x)\\]\nThis gives us a two-parameter optimization problem where we need to: 1. Maximize \\(A(x,y)\\) 2. Ensure \\(x,y &gt; 0\\) 3. Maintain \\(0.5 \\leq \\frac{x}{y} \\leq 2\\)\nThis is analogous to hyperparameter optimization where we often need to: 1. Maximize model performance 2. Respect valid parameter ranges 3. Maintain relationships between parameters\n\n\nShow the code\nimport optuna\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef objective(trial):\n    # Define the parameters\n    x = trial.suggest_float('x', 0, 250)  # x cannot be larger than 250 (half the perimeter)\n    y = trial.suggest_float('y', 0, 250)  # y cannot be larger than 250\n    \n    # Calculate area\n    area = x * y\n    \n    # Calculate constraint violations\n    perimeter_violation = abs(2*x + 2*y - 500)  # Should be 0\n    ratio = x/y if y != 0 else float('inf')\n    ratio_violation = 0\n    if ratio &lt; 0.5:\n        ratio_violation = 0.5 - ratio\n    elif ratio &gt; 2:\n        ratio_violation = ratio - 2\n        \n    # Add penalties for constraint violations\n    penalty = 1000 * (perimeter_violation + ratio_violation)\n    \n    # We minimize negative area (equivalent to maximizing area)\n    # while penalizing constraint violations\n    return -area + penalty\n\n# Create and run the study\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\n\n[I 2025-10-11 21:55:14,238] A new study created in memory with name: no-name-b3710e9e-52e7-43ff-a5ad-6b931f93f5c2\n[I 2025-10-11 21:55:14,249] Trial 0 finished with value: 110329.64383994589 and parameters: {'x': 54.67087907256868, 'y': 136.48318680674706}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,250] Trial 1 finished with value: 234866.30379776255 and parameters: {'x': 136.60283303076554, 'y': 247.75214104629669}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,251] Trial 2 finished with value: 240804.98616892047 and parameters: {'x': 31.979462540502464, 'y': 96.16413035176174}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,252] Trial 3 finished with value: -7865.849322774017 and parameters: {'x': 49.909476197747075, 'y': 199.17773154225708}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,252] Trial 4 finished with value: 48408.142534433995 and parameters: {'x': 87.78621485789515, 'y': 194.94996687648646}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,253] Trial 5 finished with value: 332375.92764577566 and parameters: {'x': 55.02600459803997, 'y': 28.0152479965359}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,253] Trial 6 finished with value: 61310.59732051972 and parameters: {'x': 214.89180520758, 'y': 73.15494881504308}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,254] Trial 7 finished with value: 97615.29934188214 and parameters: {'x': 152.54108212136714, 'y': 158.34351295619206}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,254] Trial 8 finished with value: 153187.07731964788 and parameters: {'x': 148.09668489388423, 'y': 25.353102859717257}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,249] Trial 0 finished with value: 110329.64383994589 and parameters: {'x': 54.67087907256868, 'y': 136.48318680674706}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,250] Trial 1 finished with value: 234866.30379776255 and parameters: {'x': 136.60283303076554, 'y': 247.75214104629669}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,251] Trial 2 finished with value: 240804.98616892047 and parameters: {'x': 31.979462540502464, 'y': 96.16413035176174}. Best is trial 0 with value: 110329.64383994589.\n[I 2025-10-11 21:55:14,252] Trial 3 finished with value: -7865.849322774017 and parameters: {'x': 49.909476197747075, 'y': 199.17773154225708}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,252] Trial 4 finished with value: 48408.142534433995 and parameters: {'x': 87.78621485789515, 'y': 194.94996687648646}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,253] Trial 5 finished with value: 332375.92764577566 and parameters: {'x': 55.02600459803997, 'y': 28.0152479965359}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,253] Trial 6 finished with value: 61310.59732051972 and parameters: {'x': 214.89180520758, 'y': 73.15494881504308}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,254] Trial 7 finished with value: 97615.29934188214 and parameters: {'x': 152.54108212136714, 'y': 158.34351295619206}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,254] Trial 8 finished with value: 153187.07731964788 and parameters: {'x': 148.09668489388423, 'y': 25.353102859717257}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,255] Trial 9 finished with value: 250023.91956192412 and parameters: {'x': 150.8201035172419, 'y': 242.47706422738582}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,255] Trial 9 finished with value: 250023.91956192412 and parameters: {'x': 150.8201035172419, 'y': 242.47706422738582}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,279] Trial 10 finished with value: 102317.0839826822 and parameters: {'x': 4.299011567205767, 'y': 194.36360155660202}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,282] Trial 11 finished with value: 46499.31734010883 and parameters: {'x': 91.47222521006, 'y': 190.4793280946149}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,286] Trial 12 finished with value: 78214.574610294 and parameters: {'x': 107.39013415294149, 'y': 192.02811570558518}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,288] Trial 13 finished with value: -2932.474079994916 and parameters: {'x': 80.07406305607542, 'y': 164.80108615054158}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,279] Trial 10 finished with value: 102317.0839826822 and parameters: {'x': 4.299011567205767, 'y': 194.36360155660202}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,282] Trial 11 finished with value: 46499.31734010883 and parameters: {'x': 91.47222521006, 'y': 190.4793280946149}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,286] Trial 12 finished with value: 78214.574610294 and parameters: {'x': 107.39013415294149, 'y': 192.02811570558518}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,288] Trial 13 finished with value: -2932.474079994916 and parameters: {'x': 80.07406305607542, 'y': 164.80108615054158}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,291] Trial 14 finished with value: 183686.25303597865 and parameters: {'x': 200.83996627193682, 'y': 156.74332199774133}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,294] Trial 15 finished with value: 276992.8041374956 and parameters: {'x': 6.382565648739956, 'y': 105.0055383448107}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,299] Trial 16 finished with value: 39754.85361933456 and parameters: {'x': 61.77311098313433, 'y': 214.62732237428844}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,301] Trial 17 finished with value: 24355.52913335966 and parameters: {'x': 110.66227636483784, 'y': 160.3900523515934}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,303] Trial 18 finished with value: 268460.2595153113 and parameters: {'x': 183.63041225931633, 'y': 220.87984609768552}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,291] Trial 14 finished with value: 183686.25303597865 and parameters: {'x': 200.83996627193682, 'y': 156.74332199774133}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,294] Trial 15 finished with value: 276992.8041374956 and parameters: {'x': 6.382565648739956, 'y': 105.0055383448107}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,299] Trial 16 finished with value: 39754.85361933456 and parameters: {'x': 61.77311098313433, 'y': 214.62732237428844}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,301] Trial 17 finished with value: 24355.52913335966 and parameters: {'x': 110.66227636483784, 'y': 160.3900523515934}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,303] Trial 18 finished with value: 268460.2595153113 and parameters: {'x': 183.63041225931633, 'y': 220.87984609768552}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,305] Trial 19 finished with value: 227786.08602433506 and parameters: {'x': 247.8363554823907, 'y': 132.47242960771345}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,307] Trial 20 finished with value: 293420.3407497198 and parameters: {'x': 32.34129010254482, 'y': 69.83767313887887}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,309] Trial 21 finished with value: 13056.06086696031 and parameters: {'x': 103.02710635457007, 'y': 161.8377622507046}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,313] Trial 22 finished with value: -9281.403497550622 and parameters: {'x': 81.47959365021738, 'y': 170.82767409557408}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,316] Trial 23 finished with value: 62694.78748760582 and parameters: {'x': 74.17065913936422, 'y': 215.0753025959735}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,305] Trial 19 finished with value: 227786.08602433506 and parameters: {'x': 247.8363554823907, 'y': 132.47242960771345}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,307] Trial 20 finished with value: 293420.3407497198 and parameters: {'x': 32.34129010254482, 'y': 69.83767313887887}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,309] Trial 21 finished with value: 13056.06086696031 and parameters: {'x': 103.02710635457007, 'y': 161.8377622507046}. Best is trial 3 with value: -7865.849322774017.\n[I 2025-10-11 21:55:14,313] Trial 22 finished with value: -9281.403497550622 and parameters: {'x': 81.47959365021738, 'y': 170.82767409557408}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,316] Trial 23 finished with value: 62694.78748760582 and parameters: {'x': 74.17065913936422, 'y': 215.0753025959735}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,319] Trial 24 finished with value: 97046.28119502733 and parameters: {'x': 31.30528507796069, 'y': 167.70324009321678}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,322] Trial 25 finished with value: -2633.5706946751707 and parameters: {'x': 76.72520351493031, 'y': 178.78109781791116}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,324] Trial 26 finished with value: 191715.51220414293 and parameters: {'x': 38.62156448870921, 'y': 113.41036305932678}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,326] Trial 27 finished with value: -2657.111581815865 and parameters: {'x': 117.04965931966672, 'y': 139.80377713186934}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,328] Trial 28 finished with value: 87402.23770762346 and parameters: {'x': 81.7470382449801, 'y': 220.91882649497086}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,319] Trial 24 finished with value: 97046.28119502733 and parameters: {'x': 31.30528507796069, 'y': 167.70324009321678}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,322] Trial 25 finished with value: -2633.5706946751707 and parameters: {'x': 76.72520351493031, 'y': 178.78109781791116}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,324] Trial 26 finished with value: 191715.51220414293 and parameters: {'x': 38.62156448870921, 'y': 113.41036305932678}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,326] Trial 27 finished with value: -2657.111581815865 and parameters: {'x': 117.04965931966672, 'y': 139.80377713186934}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,328] Trial 28 finished with value: 87402.23770762346 and parameters: {'x': 81.7470382449801, 'y': 220.91882649497086}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,332] Trial 29 finished with value: 84356.45758846769 and parameters: {'x': 61.664366656156375, 'y': 141.8174552358999}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,335] Trial 30 finished with value: 94720.11122531281 and parameters: {'x': 129.888349768919, 'y': 179.10343034657177}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,338] Trial 31 finished with value: -2648.512664963435 and parameters: {'x': 122.1561704945539, 'y': 134.7498350874949}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,341] Trial 32 finished with value: 3557.745775427309 and parameters: {'x': 96.90550061005331, 'y': 144.3227903767821}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,332] Trial 29 finished with value: 84356.45758846769 and parameters: {'x': 61.664366656156375, 'y': 141.8174552358999}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,335] Trial 30 finished with value: 94720.11122531281 and parameters: {'x': 129.888349768919, 'y': 179.10343034657177}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,338] Trial 31 finished with value: -2648.512664963435 and parameters: {'x': 122.1561704945539, 'y': 134.7498350874949}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,341] Trial 32 finished with value: 3557.745775427309 and parameters: {'x': 96.90550061005331, 'y': 144.3227903767821}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,344] Trial 33 finished with value: 156621.7892139885 and parameters: {'x': 45.453527057605264, 'y': 123.49490878319249}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,351] Trial 34 finished with value: 70497.04014797616 and parameters: {'x': 121.34975098791644, 'y': 174.48566509094522}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,354] Trial 35 finished with value: 86636.50737729092 and parameters: {'x': 61.99215102287928, 'y': 238.60172698421275}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,359] Trial 36 finished with value: 277515.15548766 and parameters: {'x': 20.99845563911422, 'y': 89.43754968694287}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,361] Trial 37 finished with value: 32547.90197145704 and parameters: {'x': 71.00462051315202, 'y': 202.37970169600936}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,344] Trial 33 finished with value: 156621.7892139885 and parameters: {'x': 45.453527057605264, 'y': 123.49490878319249}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,351] Trial 34 finished with value: 70497.04014797616 and parameters: {'x': 121.34975098791644, 'y': 174.48566509094522}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,354] Trial 35 finished with value: 86636.50737729092 and parameters: {'x': 61.99215102287928, 'y': 238.60172698421275}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,359] Trial 36 finished with value: 277515.15548766 and parameters: {'x': 20.99845563911422, 'y': 89.43754968694287}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,361] Trial 37 finished with value: 32547.90197145704 and parameters: {'x': 71.00462051315202, 'y': 202.37970169600936}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,364] Trial 38 finished with value: 111647.77528873675 and parameters: {'x': 172.06903479058235, 'y': 146.34562836289865}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,366] Trial 39 finished with value: 421640.791938946 and parameters: {'x': 47.67157727607591, 'y': 2.0554452052293755}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,368] Trial 40 finished with value: 12449.578841327293 and parameters: {'x': 141.2482619627448, 'y': 123.71370001173806}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,370] Trial 41 finished with value: -4455.491815555131 and parameters: {'x': 115.51064515900063, 'y': 129.25210379974368}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,372] Trial 42 finished with value: -4528.211666841316 and parameters: {'x': 93.47921906293199, 'y': 151.69473412930537}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,374] Trial 43 finished with value: 33294.75185330259 and parameters: {'x': 91.94103324290091, 'y': 183.12467877308632}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,364] Trial 38 finished with value: 111647.77528873675 and parameters: {'x': 172.06903479058235, 'y': 146.34562836289865}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,366] Trial 39 finished with value: 421640.791938946 and parameters: {'x': 47.67157727607591, 'y': 2.0554452052293755}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,368] Trial 40 finished with value: 12449.578841327293 and parameters: {'x': 141.2482619627448, 'y': 123.71370001173806}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,370] Trial 41 finished with value: -4455.491815555131 and parameters: {'x': 115.51064515900063, 'y': 129.25210379974368}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,372] Trial 42 finished with value: -4528.211666841316 and parameters: {'x': 93.47921906293199, 'y': 151.69473412930537}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,374] Trial 43 finished with value: 33294.75185330259 and parameters: {'x': 91.94103324290091, 'y': 183.12467877308632}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,377] Trial 44 finished with value: 165590.6180369739 and parameters: {'x': 83.77054247840178, 'y': 80.07997694782276}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,380] Trial 45 finished with value: -85.79789319858537 and parameters: {'x': 105.30322890942762, 'y': 152.6934276250563}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,382] Trial 46 finished with value: 85727.48996863229 and parameters: {'x': 133.42823622233726, 'y': 170.8324446516915}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,384] Trial 47 finished with value: 71999.95671753245 and parameters: {'x': 95.1256534190648, 'y': 200.39292682394353}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,386] Trial 48 finished with value: 158636.8308695582 and parameters: {'x': 53.28255423389643, 'y': 114.36914902337229}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,388] Trial 49 finished with value: 33713.37709443542 and parameters: {'x': 68.17552496105833, 'y': 205.60559010881772}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,377] Trial 44 finished with value: 165590.6180369739 and parameters: {'x': 83.77054247840178, 'y': 80.07997694782276}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,380] Trial 45 finished with value: -85.79789319858537 and parameters: {'x': 105.30322890942762, 'y': 152.6934276250563}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,382] Trial 46 finished with value: 85727.48996863229 and parameters: {'x': 133.42823622233726, 'y': 170.8324446516915}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,384] Trial 47 finished with value: 71999.95671753245 and parameters: {'x': 95.1256534190648, 'y': 200.39292682394353}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,386] Trial 48 finished with value: 158636.8308695582 and parameters: {'x': 53.28255423389643, 'y': 114.36914902337229}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,388] Trial 49 finished with value: 33713.37709443542 and parameters: {'x': 68.17552496105833, 'y': 205.60559010881772}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,392] Trial 50 finished with value: 360077.999142224 and parameters: {'x': 13.294709863328642, 'y': 56.42341212045946}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,394] Trial 51 finished with value: 13096.081913502305 and parameters: {'x': 113.49976297123348, 'y': 151.6546620856177}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,396] Trial 52 finished with value: 166660.71845241755 and parameters: {'x': 161.36447903100517, 'y': 187.05815071447597}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,398] Trial 53 finished with value: 35603.420190956786 and parameters: {'x': 112.82781698573278, 'y': 164.23927239348671}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,400] Trial 54 finished with value: 46868.83386208502 and parameters: {'x': 88.17386614377163, 'y': 132.54807865281228}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,402] Trial 55 finished with value: 33614.82149739127 and parameters: {'x': 121.35321964444411, 'y': 105.44153474413415}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,392] Trial 50 finished with value: 360077.999142224 and parameters: {'x': 13.294709863328642, 'y': 56.42341212045946}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,394] Trial 51 finished with value: 13096.081913502305 and parameters: {'x': 113.49976297123348, 'y': 151.6546620856177}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,396] Trial 52 finished with value: 166660.71845241755 and parameters: {'x': 161.36447903100517, 'y': 187.05815071447597}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,398] Trial 53 finished with value: 35603.420190956786 and parameters: {'x': 112.82781698573278, 'y': 164.23927239348671}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,400] Trial 54 finished with value: 46868.83386208502 and parameters: {'x': 88.17386614377163, 'y': 132.54807865281228}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,402] Trial 55 finished with value: 33614.82149739127 and parameters: {'x': 121.35321964444411, 'y': 105.44153474413415}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,404] Trial 56 finished with value: 77199.33782515011 and parameters: {'x': 141.0635049139813, 'y': 158.7317957214744}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,408] Trial 57 finished with value: 61077.92732611317 and parameters: {'x': 98.16527851494139, 'y': 115.62078456264786}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,410] Trial 58 finished with value: 70517.62692325597 and parameters: {'x': 79.88365480279484, 'y': 129.67795715319832}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,412] Trial 59 finished with value: 72044.91339433355 and parameters: {'x': 65.46235405256009, 'y': 143.82989871576837}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,414] Trial 60 finished with value: 125223.53421001065 and parameters: {'x': 155.96524669073114, 'y': 169.89541019566946}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,404] Trial 56 finished with value: 77199.33782515011 and parameters: {'x': 141.0635049139813, 'y': 158.7317957214744}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,408] Trial 57 finished with value: 61077.92732611317 and parameters: {'x': 98.16527851494139, 'y': 115.62078456264786}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,410] Trial 58 finished with value: 70517.62692325597 and parameters: {'x': 79.88365480279484, 'y': 129.67795715319832}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,412] Trial 59 finished with value: 72044.91339433355 and parameters: {'x': 65.46235405256009, 'y': 143.82989871576837}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,414] Trial 60 finished with value: 125223.53421001065 and parameters: {'x': 155.96524669073114, 'y': 169.89541019566946}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,416] Trial 61 finished with value: -6824.146233345498 and parameters: {'x': 121.86168938425372, 'y': 132.81901209733817}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,418] Trial 62 finished with value: 56627.175655164894 and parameters: {'x': 111.83609783751182, 'y': 104.03299237795093}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,421] Trial 63 finished with value: 2277.572671508555 and parameters: {'x': 103.62452361172737, 'y': 138.08233211044762}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,423] Trial 64 finished with value: 41200.44874570341 and parameters: {'x': 128.05137353715068, 'y': 152.29996039480525}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,426] Trial 65 finished with value: 111095.24843211919 and parameters: {'x': 84.33575151235277, 'y': 230.88024165157458}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,416] Trial 61 finished with value: -6824.146233345498 and parameters: {'x': 121.86168938425372, 'y': 132.81901209733817}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,418] Trial 62 finished with value: 56627.175655164894 and parameters: {'x': 111.83609783751182, 'y': 104.03299237795093}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,421] Trial 63 finished with value: 2277.572671508555 and parameters: {'x': 103.62452361172737, 'y': 138.08233211044762}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,423] Trial 64 finished with value: 41200.44874570341 and parameters: {'x': 128.05137353715068, 'y': 152.29996039480525}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,426] Trial 65 finished with value: 111095.24843211919 and parameters: {'x': 84.33575151235277, 'y': 230.88024165157458}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,428] Trial 66 finished with value: 136085.05720613495 and parameters: {'x': 54.8828639379639, 'y': 123.70805709749239}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,430] Trial 67 finished with value: 103097.87950344205 and parameters: {'x': 117.74446132855665, 'y': 195.30236426123042}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,433] Trial 68 finished with value: 303377.3024283395 and parameters: {'x': 246.24484901435858, 'y': 177.26967428999305}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,434] Trial 69 finished with value: 10977.480859288014 and parameters: {'x': 73.67482159621474, 'y': 164.7923752308203}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,437] Trial 70 finished with value: 50855.52949973527 and parameters: {'x': 137.00677152081866, 'y': 148.60064020956887}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,428] Trial 66 finished with value: 136085.05720613495 and parameters: {'x': 54.8828639379639, 'y': 123.70805709749239}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,430] Trial 67 finished with value: 103097.87950344205 and parameters: {'x': 117.74446132855665, 'y': 195.30236426123042}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,433] Trial 68 finished with value: 303377.3024283395 and parameters: {'x': 246.24484901435858, 'y': 177.26967428999305}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,434] Trial 69 finished with value: 10977.480859288014 and parameters: {'x': 73.67482159621474, 'y': 164.7923752308203}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,437] Trial 70 finished with value: 50855.52949973527 and parameters: {'x': 137.00677152081866, 'y': 148.60064020956887}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,439] Trial 71 finished with value: 5243.806402325117 and parameters: {'x': 122.1346261412983, 'y': 138.9740488070607}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,443] Trial 72 finished with value: 17929.424358119904 and parameters: {'x': 101.88560683823891, 'y': 132.40461852918534}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,445] Trial 73 finished with value: 87473.3037098331 and parameters: {'x': 146.72051957450972, 'y': 158.65511255394003}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,447] Trial 74 finished with value: -766.110501912448 and parameters: {'x': 126.43859528404009, 'y': 116.57468994571198}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,449] Trial 75 finished with value: 119805.79362264856 and parameters: {'x': 91.93818551446344, 'y': 93.84495044252216}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,439] Trial 71 finished with value: 5243.806402325117 and parameters: {'x': 122.1346261412983, 'y': 138.9740488070607}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,443] Trial 72 finished with value: 17929.424358119904 and parameters: {'x': 101.88560683823891, 'y': 132.40461852918534}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,445] Trial 73 finished with value: 87473.3037098331 and parameters: {'x': 146.72051957450972, 'y': 158.65511255394003}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,447] Trial 74 finished with value: -766.110501912448 and parameters: {'x': 126.43859528404009, 'y': 116.57468994571198}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,449] Trial 75 finished with value: 119805.79362264856 and parameters: {'x': 91.93818551446344, 'y': 93.84495044252216}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,451] Trial 76 finished with value: -8399.783327793417 and parameters: {'x': 107.71826439198293, 'y': 138.9954528046656}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,454] Trial 77 finished with value: 60489.11918196906 and parameters: {'x': 106.23729130438943, 'y': 183.7688190686236}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,457] Trial 78 finished with value: 171076.77872942304 and parameters: {'x': 35.779486297985486, 'y': 126.52719482207904}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,459] Trial 79 finished with value: 52847.2520340949 and parameters: {'x': 77.69760971490919, 'y': 140.42347990000343}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,451] Trial 76 finished with value: -8399.783327793417 and parameters: {'x': 107.71826439198293, 'y': 138.9954528046656}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,454] Trial 77 finished with value: 60489.11918196906 and parameters: {'x': 106.23729130438943, 'y': 183.7688190686236}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,457] Trial 78 finished with value: 171076.77872942304 and parameters: {'x': 35.779486297985486, 'y': 126.52719482207904}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,459] Trial 79 finished with value: 52847.2520340949 and parameters: {'x': 77.69760971490919, 'y': 140.42347990000343}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,461] Trial 80 finished with value: 231536.46682359578 and parameters: {'x': 26.002425981145805, 'y': 106.96709458579718}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,464] Trial 81 finished with value: -12271.531168115056 and parameters: {'x': 115.97071443886341, 'y': 135.7659575222446}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,466] Trial 82 finished with value: 25648.4196491222 and parameters: {'x': 116.75348131432634, 'y': 155.12650846388203}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,468] Trial 83 finished with value: 38995.20220216554 and parameters: {'x': 133.66260293592967, 'y': 145.5631745672939}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,470] Trial 84 finished with value: 73367.3478536114 and parameters: {'x': 88.56529799970767, 'y': 119.4609794512674}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,471] Trial 85 finished with value: 21480.360582070996 and parameters: {'x': 98.17303743887757, 'y': 170.95892113469102}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,461] Trial 80 finished with value: 231536.46682359578 and parameters: {'x': 26.002425981145805, 'y': 106.96709458579718}. Best is trial 22 with value: -9281.403497550622.\n[I 2025-10-11 21:55:14,464] Trial 81 finished with value: -12271.531168115056 and parameters: {'x': 115.97071443886341, 'y': 135.7659575222446}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,466] Trial 82 finished with value: 25648.4196491222 and parameters: {'x': 116.75348131432634, 'y': 155.12650846388203}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,468] Trial 83 finished with value: 38995.20220216554 and parameters: {'x': 133.66260293592967, 'y': 145.5631745672939}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,470] Trial 84 finished with value: 73367.3478536114 and parameters: {'x': 88.56529799970767, 'y': 119.4609794512674}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,471] Trial 85 finished with value: 21480.360582070996 and parameters: {'x': 98.17303743887757, 'y': 170.95892113469102}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,474] Trial 86 finished with value: 120636.83202700765 and parameters: {'x': 110.88965267299128, 'y': 211.13511301517534}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,474] Trial 86 finished with value: 120636.83202700765 and parameters: {'x': 110.88965267299128, 'y': 211.13511301517534}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,477] Trial 87 finished with value: -3662.3065250383097 and parameters: {'x': 107.74052703883869, 'y': 136.72520348234053}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,477] Trial 87 finished with value: -3662.3065250383097 and parameters: {'x': 107.74052703883869, 'y': 136.72520348234053}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,491] Trial 88 finished with value: 130218.77873830058 and parameters: {'x': 47.844443349902335, 'y': 133.9140080154505}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,494] Trial 89 finished with value: 69695.28534620462 and parameters: {'x': 100.58943050715362, 'y': 109.07693350821505}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,491] Trial 88 finished with value: 130218.77873830058 and parameters: {'x': 47.844443349902335, 'y': 133.9140080154505}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,494] Trial 89 finished with value: 69695.28534620462 and parameters: {'x': 100.58943050715362, 'y': 109.07693350821505}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,519] Trial 90 finished with value: 13611.392790075282 and parameters: {'x': 108.13069942866558, 'y': 128.13589234567004}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,527] Trial 91 finished with value: 15039.272005873652 and parameters: {'x': 116.74743302781677, 'y': 149.49901616377232}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,519] Trial 90 finished with value: 13611.392790075282 and parameters: {'x': 108.13069942866558, 'y': 128.13589234567004}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,527] Trial 91 finished with value: 15039.272005873652 and parameters: {'x': 116.74743302781677, 'y': 149.49901616377232}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,530] Trial 92 finished with value: 27139.50150610523 and parameters: {'x': 93.5449926828245, 'y': 136.50077458427972}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,535] Trial 93 finished with value: 57642.25899426801 and parameters: {'x': 128.49742108919992, 'y': 160.6449385663378}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,548] Trial 94 finished with value: 24324.04538916787 and parameters: {'x': 86.72482250555704, 'y': 144.83285306245176}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,551] Trial 95 finished with value: 29066.77895981224 and parameters: {'x': 106.0080650826467, 'y': 122.942117443759}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,553] Trial 96 finished with value: 76592.47124886255 and parameters: {'x': 143.6417217726292, 'y': 155.8476244035522}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,555] Trial 97 finished with value: 166286.48848715823 and parameters: {'x': 63.13051877558448, 'y': 100.55227823627494}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,530] Trial 92 finished with value: 27139.50150610523 and parameters: {'x': 93.5449926828245, 'y': 136.50077458427972}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,535] Trial 93 finished with value: 57642.25899426801 and parameters: {'x': 128.49742108919992, 'y': 160.6449385663378}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,548] Trial 94 finished with value: 24324.04538916787 and parameters: {'x': 86.72482250555704, 'y': 144.83285306245176}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,551] Trial 95 finished with value: 29066.77895981224 and parameters: {'x': 106.0080650826467, 'y': 122.942117443759}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,553] Trial 96 finished with value: 76592.47124886255 and parameters: {'x': 143.6417217726292, 'y': 155.8476244035522}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,555] Trial 97 finished with value: 166286.48848715823 and parameters: {'x': 63.13051877558448, 'y': 100.55227823627494}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,558] Trial 98 finished with value: 10914.151651453536 and parameters: {'x': 124.07737702457155, 'y': 140.06942204553397}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,561] Trial 99 finished with value: -3362.357981400788 and parameters: {'x': 81.73704868231124, 'y': 173.66451510052832}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,558] Trial 98 finished with value: 10914.151651453536 and parameters: {'x': 124.07737702457155, 'y': 140.06942204553397}. Best is trial 81 with value: -12271.531168115056.\n[I 2025-10-11 21:55:14,561] Trial 99 finished with value: -3362.357981400788 and parameters: {'x': 81.73704868231124, 'y': 173.66451510052832}. Best is trial 81 with value: -12271.531168115056.\n\n\n\n\nShow the code\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\n# Create data for constraints\nx_grid = np.linspace(0, 250, 100)\ny_perimeter = 250 - x_grid  # perimeter constraint\ny_ratio_min = x_grid/2      # ratio 2:1 constraint\ny_ratio_max = 2*x_grid      # ratio 1:2 constraint\n\n# Create DataFrame for constraints\nconstraint_df = pd.DataFrame({\n    'x': np.concatenate([x_grid, x_grid, x_grid]),\n    'y': np.concatenate([y_perimeter, y_ratio_min, y_ratio_max]),\n    'Constraint': np.repeat(['Perimeter', 'Ratio (2:1)', 'Ratio (1:2)'], len(x_grid))\n})\n\n# Create DataFrame for trial points\ntrials_df = pd.DataFrame({\n    'x': [t.params['x'] for t in study.trials],\n    'y': [t.params['y'] for t in study.trials],\n    'Area': [-t.value for t in study.trials]  # Negative because we minimized negative area\n})\n\n# Create DataFrame for best point\nbest_point_df = pd.DataFrame({\n    'x': [study.best_params['x']],\n    'y': [study.best_params['y']]\n})\n\n# Create the base plot\nbase = alt.Chart().encode(\n    x=alt.X('x', title='Length (meters)', scale=alt.Scale(domain=[0, 250])),\n    y=alt.Y('y', title='Width (meters)', scale=alt.Scale(domain=[0, 250]))\n)\n\n# Plot constraints\nconstraints = base.mark_line(strokeDash=[4, 4]).encode(\n    color=alt.Color('Constraint:N', legend=alt.Legend(title='Constraints')),\n).transform_filter(\n    alt.datum.y &gt;= 0\n).data(constraint_df)\n\n# Plot trial points\npoints = base.mark_circle().encode(\n    color=alt.Color('Area:Q', scale=alt.Scale(scheme='viridis'), \n                   legend=alt.Legend(title='Area (sq meters)')),\n    tooltip=['x:Q', 'y:Q', 'Area:Q']\n).data(trials_df)\n\n# Plot best point\nbest_point = base.mark_star(\n    size=200,\n    color='red'\n).encode(\n    tooltip=['x:Q', 'y:Q']\n).data(best_point_df)\n\n# Combine all layers\nchart = (constraints + points + best_point).properties(\n    width=500,\n    height=500,\n    title='Optimization Search Space'\n)\n\nchart\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nInput In [9], in &lt;cell line: 1&gt;()\n----&gt; 1 import altair as alt\n      2 import pandas as pd\n      3 import numpy as np\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/__init__.py:4, in &lt;module&gt;\n      1 # flake8: noqa\n      2 __version__ = \"4.2.2\"\n----&gt; 4 from .vegalite import *\n      5 from . import examples\n      8 def load_ipython_extension(ipython):\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/vegalite/__init__.py:2, in &lt;module&gt;\n      1 # flake8: noqa\n----&gt; 2 from .v4 import *\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/vegalite/v4/__init__.py:2, in &lt;module&gt;\n      1 # flake8: noqa\n----&gt; 2 from .schema import *\n      3 from .api import *\n      5 from ...datasets import list_datasets, load_dataset\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/vegalite/v4/schema/__init__.py:2, in &lt;module&gt;\n      1 # flake8: noqa\n----&gt; 2 from .core import *\n      3 from .channels import *\n      4 SCHEMA_VERSION = 'v4.17.0'\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/vegalite/v4/schema/core.py:4, in &lt;module&gt;\n      1 # The contents of this file are automatically written by\n      2 # tools/generate_schema_wrapper.py. Do not modify directly.\n----&gt; 4 from altair.utils.schemapi import SchemaBase, Undefined, _subclasses\n      6 import pkgutil\n      7 import json\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/utils/__init__.py:1, in &lt;module&gt;\n----&gt; 1 from .core import (\n      2     infer_vegalite_type,\n      3     infer_encoding_types,\n      4     sanitize_dataframe,\n      5     parse_shorthand,\n      6     use_signature,\n      7     update_subtraits,\n      8     update_nested,\n      9     display_traceback,\n     10     SchemaBase,\n     11     Undefined,\n     12 )\n     13 from .html import spec_to_html\n     14 from .plugin_registry import PluginRegistry\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/altair/utils/core.py:14, in &lt;module&gt;\n     11 import warnings\n     13 import jsonschema\n---&gt; 14 import pandas as pd\n     15 import numpy as np\n     17 from .schemapi import SchemaBase, Undefined\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/__init__.py:22, in &lt;module&gt;\n     19 del _hard_dependencies, _dependency, _missing_dependencies\n     21 # numpy compat\n---&gt; 22 from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401\n     24 try:\n     25     from pandas._libs import hashtable as _hashtable, lib as _lib, tslib as _tslib\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/compat/__init__.py:18, in &lt;module&gt;\n     15 from typing import TYPE_CHECKING\n     17 from pandas._typing import F\n---&gt; 18 from pandas.compat.numpy import (\n     19     is_numpy_dev,\n     20     np_version_under1p21,\n     21 )\n     22 from pandas.compat.pyarrow import (\n     23     pa_version_under1p01,\n     24     pa_version_under2p0,\n   (...)\n     31     pa_version_under9p0,\n     32 )\n     34 if TYPE_CHECKING:\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:4, in &lt;module&gt;\n      1 \"\"\" support numpy compatibility across versions \"\"\"\n      2 import numpy as np\n----&gt; 4 from pandas.util.version import Version\n      6 # numpy versioning\n      7 _np_version = np.__version__\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/__init__.py:2, in &lt;module&gt;\n      1 # pyright: reportUnusedImport = false\n----&gt; 2 from pandas.util._decorators import (  # noqa:F401\n      3     Appender,\n      4     Substitution,\n      5     cache_readonly,\n      6 )\n      8 from pandas.core.util.hashing import (  # noqa:F401\n      9     hash_array,\n     10     hash_pandas_object,\n     11 )\n     14 def __getattr__(name):\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:14, in &lt;module&gt;\n      6 from typing import (\n      7     Any,\n      8     Callable,\n      9     Mapping,\n     10     cast,\n     11 )\n     12 import warnings\n---&gt; 14 from pandas._libs.properties import cache_readonly\n     15 from pandas._typing import (\n     16     F,\n     17     T,\n     18 )\n     19 from pandas.util._exceptions import find_stack_level\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/__init__.py:13, in &lt;module&gt;\n      1 __all__ = [\n      2     \"NaT\",\n      3     \"NaTType\",\n   (...)\n      9     \"Interval\",\n     10 ]\n---&gt; 13 from pandas._libs.interval import Interval\n     14 from pandas._libs.tslibs import (\n     15     NaT,\n     16     NaTType,\n   (...)\n     21     iNaT,\n     22 )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/interval.pyx:1, in init pandas._libs.interval()\n\nValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n\n\n\n\n\nShow the code\n# Print the results\nbest_x = study.best_params['x']\nbest_y = study.best_params['y']\nbest_area = best_x * best_y\n\nprint(f\"Best solution found:\")\nprint(f\"x (length) = {best_x:.2f} meters\")\nprint(f\"y (width) = {best_y:.2f} meters\")\nprint(f\"Area = {best_area:.2f} square meters\")\nprint(f\"Perimeter = {2*best_x + 2*best_y:.2f} meters\")\nprint(f\"Length/Width ratio = {best_x/best_y:.2f}\")\n\n# Create visualization\ntrials_data = np.array([[t.params['x'], t.params['y'], t.value] for t in study.trials])\nx_values = trials_data[:, 0]\ny_values = trials_data[:, 1]\nobjective_values = -trials_data[:, 2]  # Negative because we minimized negative area\n\n# Create subplots\nfig = make_subplots(rows=1, cols=1, subplot_titles=('Trial Progress', 'Search Space'))\n\n\n\n# Plot 2: Search space with constraints\nx_grid = np.linspace(0, 250, 100)\ny_grid = 250 - x_grid  # Perimeter constraint\n\nfig.add_trace(\n    go.Scatter(x=x_values, y=y_values, \n               mode='markers',\n               marker=dict(color=objective_values, colorscale='Viridis'),\n               name='Trials'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=x_grid, y=y_grid,\n               mode='lines',\n               line=dict(color='red', dash='dash'),\n               name='Perimeter Constraint'),\n    row=1, col=2\n)\n\n# Add ratio constraints\ny_ratio_min = x_grid/2  # x/y = 2\ny_ratio_max = 2*x_grid  # x/y = 0.5\n\nfig.add_trace(\n    go.Scatter(x=x_grid, y=y_ratio_min,\n               mode='lines',\n               line=dict(color='orange', dash='dash'),\n               name='Ratio Constraint (2:1)'),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(x=x_grid, y=y_ratio_max,\n               mode='lines',\n               line=dict(color='orange', dash='dash'),\n               name='Ratio Constraint (1:2)'),\n    row=1, col=2\n)\n\n# Add best point\nfig.add_trace(\n    go.Scatter(x=[best_x], y=[best_y],\n               mode='markers',\n               marker=dict(color='red', size=10, symbol='star'),\n               name='Best Solution'),\n    row=1, col=2\n)\n\nfig.update_layout(height=500, width=1000, title_text=\"Optimization Results\")\nfig.update_xaxes(title_text=\"Trial Number\", row=1, col=1)\nfig.update_xaxes(title_text=\"x (length)\", row=1, col=2)\nfig.update_yaxes(title_text=\"Area\", row=1, col=1)\nfig.update_yaxes(title_text=\"y (width)\", row=1, col=2)\n\nfig.show()\n\n\nBest solution found:\nx (length) = 115.97 meters\ny (width) = 135.77 meters\nArea = 15744.88 square meters\nPerimeter = 503.47 meters\nLength/Width ratio = 0.85\n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/_subplots.py:1352, in _set_trace_grid_reference(trace, layout, grid_ref, row, col, secondary_y)\n   1351 try:\n-&gt; 1352     subplot_refs = grid_ref[row - 1][col - 1]\n   1353 except IndexError:\n\nIndexError: list index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nException                                 Traceback (most recent call last)\nInput In [8], in &lt;cell line: 36&gt;()\n     26 y_grid = 250 - x_grid  # Perimeter constraint\n     28 fig.add_trace(\n     29     go.Scatter(x=x_values, y=y_values, \n     30                mode='markers',\n   (...)\n     33     row=1, col=1\n     34 )\n---&gt; 36 fig.add_trace(\n     37     go.Scatter(x=x_grid, y=y_grid,\n     38                mode='lines',\n     39                line=dict(color='red', dash='dash'),\n     40                name='Perimeter Constraint'),\n     41     row=1, col=2\n     42 )\n     44 # Add ratio constraints\n     45 y_ratio_min = x_grid/2  # x/y = 2\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/graph_objs/_figure.py:865, in Figure.add_trace(self, trace, row, col, secondary_y, exclude_empty_subplots)\n    790 def add_trace(\n    791     self, trace, row=None, col=None, secondary_y=None, exclude_empty_subplots=False\n    792 ) -&gt; \"Figure\":\n    793     \"\"\"\n    794 \n    795     Add a trace to the figure\n   (...)\n    863 \n    864     \"\"\"\n--&gt; 865     return super(Figure, self).add_trace(\n    866         trace, row, col, secondary_y, exclude_empty_subplots\n    867     )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/basedatatypes.py:2097, in BaseFigure.add_trace(self, trace, row, col, secondary_y, exclude_empty_subplots)\n   2088         self.add_trace(\n   2089             trace,\n   2090             row=r,\n   (...)\n   2093             exclude_empty_subplots=exclude_empty_subplots,\n   2094         )\n   2095     return self\n-&gt; 2097 return self.add_traces(\n   2098     data=[trace],\n   2099     rows=[row] if row is not None else None,\n   2100     cols=[col] if col is not None else None,\n   2101     secondary_ys=[secondary_y] if secondary_y is not None else None,\n   2102     exclude_empty_subplots=exclude_empty_subplots,\n   2103 )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/graph_objs/_figure.py:945, in Figure.add_traces(self, data, rows, cols, secondary_ys, exclude_empty_subplots)\n    869 def add_traces(\n    870     self,\n    871     data,\n   (...)\n    875     exclude_empty_subplots=False,\n    876 ) -&gt; \"Figure\":\n    877     \"\"\"\n    878 \n    879     Add traces to the figure\n   (...)\n    943 \n    944     \"\"\"\n--&gt; 945     return super(Figure, self).add_traces(\n    946         data, rows, cols, secondary_ys, exclude_empty_subplots\n    947     )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/basedatatypes.py:2227, in BaseFigure.add_traces(self, data, rows, cols, secondary_ys, exclude_empty_subplots)\n   2225 if rows is not None:\n   2226     for trace, row, col, secondary_y in zip(data, rows, cols, secondary_ys):\n-&gt; 2227         self._set_trace_grid_position(trace, row, col, secondary_y)\n   2229 if exclude_empty_subplots:\n   2230     data = list(\n   2231         filter(\n   2232             lambda trace: self._subplot_not_empty(\n   (...)\n   2236         )\n   2237     )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/basedatatypes.py:2319, in BaseFigure._set_trace_grid_position(self, trace, row, col, secondary_y)\n   2316 from plotly._subplots import _set_trace_grid_reference\n   2318 grid_ref = self._validate_get_grid_ref()\n-&gt; 2319 return _set_trace_grid_reference(\n   2320     trace, self.layout, grid_ref, row, col, secondary_y\n   2321 )\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/plotly/_subplots.py:1354, in _set_trace_grid_reference(trace, layout, grid_ref, row, col, secondary_y)\n   1352         subplot_refs = grid_ref[row - 1][col - 1]\n   1353     except IndexError:\n-&gt; 1354         raise Exception(\n   1355             \"The (row, col) pair sent is out of \"\n   1356             \"range. Use Figure.print_grid to view the \"\n   1357             \"subplot grid. \"\n   1358         )\n   1360     if not subplot_refs:\n   1361         raise ValueError(\n   1362             \"\"\"\n   1363 No subplot specified at grid position ({row}, {col})\"\"\".format(\n   1364                 row=row, col=col\n   1365             )\n   1366         )\n\nException: The (row, col) pair sent is out of range. Use Figure.print_grid to view the subplot grid. \n\n\n\n\n\nShow the code\nimport optuna\n\ndef objective(trial):\n    # Define the parameters with their constraints\n    x = trial.suggest_float('length', 0, 250)  # length can't be more than half the perimeter\n    y = trial.suggest_float('width', 0, 250)   # width can't be more than half the perimeter\n    \n    # Calculate area\n    area = x * y\n    \n    # Perimeter constraint penalty\n    perimeter_error = abs(2*x + 2*y - 500)\n    \n    # Ratio constraint penalty\n    ratio = x/y if y != 0 else float('inf')\n    ratio_error = 0\n    if ratio &lt; 0.5 or ratio &gt; 2:\n        ratio_error = min(abs(ratio - 0.5), abs(ratio - 2))\n    \n    # Return negative area (since Optuna minimizes) with penalties\n    return -area + 1000 * (perimeter_error + ratio_error)  # Large penalty for constraint violations\n\n# Create a study object and optimize\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n\n# Print results\nprint(f\"Best parameters: {study.best_params}\")\nprint(f\"Best value (negative area): {study.best_value}\")\nprint(f\"Actual area: {-study.best_value} square meters\")\nprint(f\"Perimeter: {2*study.best_params['length'] + 2*study.best_params['width']} meters\")\nprint(f\"Length/Width ratio: {study.best_params['length']/study.best_params['width']:.2f}\")\n\n\n\n\nShow the code\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import clear_output\n\ndef gardent_area(trial):\n    \"\"\"Objective function to maximize the garden area.\"\"\"\n    x = trial.suggest_uniform('x', 0, 250)\n    return (500*x - 2*x**2 ) \n\n\nOnce the objective function has been defined, the study object is used to start the optimization. The study is an optimization session, a set of trials. Optuna provide different sampler strategies such Tree-structured Parzen Estimator (TPE) sampler. A sampler has the responsibility to determine the parameter values to be evaluated in a trial. By default, Optuna uses TPE sampler, which is a form of Bayesian Optimization. The TPE provides a more efficient search than a random sampler search by choosing points closer to past good results. It possible to add a custom sampler as described in this link Let create a study and start the optimization process.\n\n\nShow the code\nstudy = optuna.create_study(study_name=\"garden\", direction=\"maximize\", sampler=optuna.samplers.TPESampler())\nstudy.optimize(gardent_area, n_trials=20)\n\n\n[I 2025-10-11 21:47:35,811] A new study created in memory with name: garden\n[I 2025-10-11 21:47:37,395] Trial 0 finished with value: 30246.525474019894 and parameters: {'x': 102.60050752829314}. Best is trial 0 with value: 30246.525474019894.\n[I 2025-10-11 21:47:37,396] Trial 1 finished with value: 28884.237809943665 and parameters: {'x': 90.60696153248207}. Best is trial 0 with value: 30246.525474019894.\n[I 2025-10-11 21:47:37,396] Trial 2 finished with value: 11385.14975589222 and parameters: {'x': 25.338447122002506}. Best is trial 0 with value: 30246.525474019894.\n[I 2025-10-11 21:47:37,396] Trial 3 finished with value: 31134.916939880397 and parameters: {'x': 117.41438663918338}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,397] Trial 4 finished with value: 28207.43211241709 and parameters: {'x': 164.0036401351394}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,398] Trial 5 finished with value: 30936.451045935133 and parameters: {'x': 112.47903849409194}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,398] Trial 6 finished with value: 11124.277714558026 and parameters: {'x': 24.68618668039288}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,400] Trial 7 finished with value: 30563.001391874684 and parameters: {'x': 106.46626578202181}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,400] Trial 8 finished with value: 13635.683343231602 and parameters: {'x': 31.153538540954038}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,400] Trial 9 finished with value: 23661.87149257028 and parameters: {'x': 186.59597595391162}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,403] Trial 10 finished with value: 460.2558044857433 and parameters: {'x': 249.0760738327786}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,404] Trial 11 finished with value: 30068.94348582362 and parameters: {'x': 149.30078717013478}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,406] Trial 12 finished with value: 25839.476920025998 and parameters: {'x': 72.98787122231008}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,407] Trial 13 finished with value: 16930.397894067137 and parameters: {'x': 209.61560762038192}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,409] Trial 14 finished with value: 30814.921648104246 and parameters: {'x': 139.74920933297372}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,410] Trial 15 finished with value: 24707.66600587945 and parameters: {'x': 67.80588319538211}. Best is trial 3 with value: 31134.916939880397.\n[I 2025-10-11 21:47:37,414] Trial 16 finished with value: 31249.97209149898 and parameters: {'x': 124.88187188942693}. Best is trial 16 with value: 31249.97209149898.\n[I 2025-10-11 21:47:37,416] Trial 17 finished with value: 27127.961566034028 and parameters: {'x': 170.39844949976805}. Best is trial 16 with value: 31249.97209149898.\n[I 2025-10-11 21:47:37,417] Trial 18 finished with value: 22981.294702928128 and parameters: {'x': 60.701068060690645}. Best is trial 16 with value: 31249.97209149898.\n[I 2025-10-11 21:47:37,418] Trial 19 finished with value: 31074.348233056415 and parameters: {'x': 134.37154648240036}. Best is trial 16 with value: 31249.97209149898.\n\n\nOnce the study is completed, you can get the best parameters using study.best_params and study.best_value will give you the best value.\n\n\nShow the code\nprint(study.best_params)\nprint(study.best_value)\n\n\n{'x': 124.88187188942693}\n31249.97209149898"
  },
  {
    "objectID": "blog/2024/03/optuna-hyperparmeter.html#hyper-param-search-for-neural-network",
    "href": "blog/2024/03/optuna-hyperparmeter.html#hyper-param-search-for-neural-network",
    "title": "Super-charge Deep learning hyper-parameter search with Optuna",
    "section": "Hyper-param search for neural network",
    "text": "Hyper-param search for neural network\nSuppose we want to build MLP classifier to recognize handwritten digits using the MNIST dataset. We will first build a pytorch MLP model with the following default parameters hparams = {\"in_size\": 28*28, \"hidden_size\":128, \"out_size\":10, \"layer_size\":5, \"dropout\":0.2}\n\n\nShow the code\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass MLP(nn.Module):\n    \"\"\"A simple Multi-layer perceptron model.\"\"\"\n    def __init__(self, hparams):\n        super().__init__()\n        \"\"\"Initialize the MLP model with given hyperparameters.\"\"\"\n        layers = [hparams['in_size'], hparams['hidden_size']]+[hparams['hidden_size']*2**i  for i in range(hparams['layer_size']-1)] \n        self.layers = []\n        \n        for i in range(len(layers)-1):\n            layer = nn.Linear(layers[i], layers[i+1])\n            self.layers.append(layer) \n            self.layers.append(nn.ReLU())\n            if i!=0:\n                self.layers.append(nn.Dropout(hparams['dropout']))\n                \n        out_layer = nn.Linear(layers[-1], hparams['out_size'])\n        self.layers.append(out_layer)\n        \n        ##initilize weights\n        for layer in self.layers:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)\n        self.mlp =  nn.Sequential(*self.layers)\n        \n        \n    def forward(self, x):\n        return self.mlp(x)\n\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nInput In [1], in &lt;cell line: 1&gt;()\n----&gt; 1 import torch\n      2 import torch.nn as nn\n      3 from torch.nn import functional as F\n\nFile ~/opt/anaconda3/lib/python3.9/site-packages/torch/__init__.py:229, in &lt;module&gt;\n    227     if USE_GLOBAL_DEPS:\n    228         _load_global_deps()\n--&gt; 229     from torch._C import *  # noqa: F403\n    231 # Appease the type checker; ordinarily this binding is inserted by the\n    232 # torch._C module initialization code in C\n    233 if TYPE_CHECKING:\n\nImportError: dlopen(/Users/anthonyfaustine/opt/anaconda3/lib/python3.9/site-packages/torch/_C.cpython-39-darwin.so, 0x0002): Library not loaded: @rpath/libtorch_cpu.dylib\n  Referenced from: &lt;0567324D-600D-3626-8E46-016BA918978A&gt; /Users/anthonyfaustine/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_python.dylib\n  Reason: tried: '/Users/anthonyfaustine/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/runner/work/_temp/anaconda/envs/wheel_py39/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/_temp/anaconda/envs/wheel_py39/lib/libtorch_cpu.dylib' (no such file), '/Users/runner/work/_temp/anaconda/envs/wheel_py39/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/_temp/anaconda/envs/wheel_py39/lib/libtorch_cpu.dylib' (no such file), '/Users/anthonyfaustine/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/runner/work/_temp/anaconda/envs/wheel_py39/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/_temp/anaconda/envs/wheel_py39/lib/libtorch_cpu.dylib' (no such file), '/Users/runner/work/_temp/anaconda/envs/wheel_py39/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/_temp/anaconda/envs/wheel_py39/lib/libtorch_cpu.dylib' (no such file), '/Users/anthonyfaustine/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/anthonyfaustine/opt/anaconda3/bin/../lib/libtorch_cpu.dylib' (no such file), '/Users/anthonyfaustine/opt/anaconda3/bin/../lib/libtorch_cpu.dylib' (no such file), '/usr/local/lib/libtorch_cpu.dylib' (no such file), '/usr/lib/libtorch_cpu.dylib' (no such file, not in dyld cache)\n\n\n\n\n\nShow the code\n#collapse\nhparams = {\"in_size\": 28*28, \"hidden_size\":128, \"out_size\":10, \"layer_size\":5, \"dropout\":0.2}\nmodel = MLP(hparams)\nmodel\n\n\nMLP(\n  (mlp): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=128, bias=True)\n    (3): ReLU()\n    (4): Dropout(p=0.2, inplace=False)\n    (5): Linear(in_features=128, out_features=256, bias=True)\n    (6): ReLU()\n    (7): Dropout(p=0.2, inplace=False)\n    (8): Linear(in_features=256, out_features=512, bias=True)\n    (9): ReLU()\n    (10): Dropout(p=0.2, inplace=False)\n    (11): Linear(in_features=512, out_features=1024, bias=True)\n    (12): ReLU()\n    (13): Dropout(p=0.2, inplace=False)\n    (14): Linear(in_features=1024, out_features=10, bias=True)\n  )\n)\n\n\nFor the above MLP model, we need to specify the following parameters hidden size, dropout, and number of linear layers. The critical question is, how do we pick these parameters. We will use optuna to search for optimal parameters that will give us an excellent performance. First, we will create a PyTorch lightning model that will provide the structure for organizing the fundamentals component of any machine learning project. These elements include the data, architecture or model, optimizer, loss function, training, and evaluation step. Since we fine defined our MLP, we go ahead and create a PyTorch lightning module.\n\n\nShow the code\n#collapse\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import MNIST\nimport os\nfrom torchvision import datasets, transforms\nimport pytorch_lightning as pl\nimport  pytorch_lightning.metrics.functional as metrics\n\nclass MLPIL(pl.LightningModule):\n    \n    def __init__(self, hparams):\n        super().__init__()\n        self.hparams = hparams\n        self.model = MLP(hparams)\n    \n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x.reshape(x.size(0), -1))\n        loss = F.cross_entropy(logits, y)\n        acc  = metrics.accuracy(torch.max(logits, 1)[1], y)\n        \n        logs = {'loss': loss, \"tra_acc\":acc}\n        return {'loss': loss, 'log': logs}\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x.reshape(x.size(0), -1))\n        loss = F.cross_entropy(logits, y)\n        acc  = metrics.accuracy(torch.max(logits, 1)[1], y)\n        \n        logs = {'val_loss': loss, \"val_acc\":acc}\n        return logs\n\n    def validation_epoch_end(self, outputs):\n        # OPTIONAL\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n        logs = {'val_loss': avg_loss, \"val_acc\":avg_acc}\n        return {'log':logs}\n\n    \n    def train_dataloader(self):\n        # REQUIRED\n        return DataLoader(MNIST(os.getcwd(), train=True, download=True, \n                                transform=transforms.ToTensor()), \n                                batch_size=self.hparams[\"batch_size\"], num_workers=4)\n\n    def val_dataloader(self):\n        # OPTIONAL\n        return DataLoader(MNIST(os.getcwd(), train=False, download=True, \n                                transform=transforms.ToTensor()),\n                                batch_size=self.hparams[\"batch_size\"], num_workers=4)\n\n    \n    def configure_optimizers(self):\n        \n        optimizer =  torch.optim.SGD(self.model.parameters(), \n                                         lr=self.hparams['learning_rate'], \n                                         momentum=self.hparams['momentum'], \n                                         nesterov=self.hparams['nesterov'],\n                                         weight_decay=self.hparams['weight_decay'])    \n        \n        return optimizer\n         \n\n\nTo learn the parameters of the MLP we will use Stochastic Gradient Descent Optimizer (SGD) optimizer. The SGD has several other hper-parameters such as learning rate which we can also optimize.\n optimizer = torch.optim.SGD(self.model.parameters(), \n lr=self.hparams['learning_rate'], \n momentum=self.hparams['momentum'], \n nesterov=self.hparams['nesterov'],\n weight_decay=self.hparams['weight_decay']) \nThus the SGD optimizer will add four additional parameters. We can also treat the batch size as hyper-parameter to optimize. We will have the following set of parameters to optimizers.\ndefault_params = {\"in_size\": 28*28, \"hidden_size\":128, \"out_size\":10, \n \"layer_size\":5, \"dropout\":0.2, \"batch_size\":32,\n 'learning_rate':1e-3, 'momentum':0.9, 'nesterov': True,\n 'weight_decay':1e-5,\n 'epochs':2}\n\nDefining the hyperparameters and objective function to be optimized\nSince we know all the parameters that we want to optimize, we will use the optuna suggest to define a search space for each hyperparameter that we want to tune. Optuna supports a variety of suggests which can be used to optimize floats, integers, or discrete categorical values. Numerical values such as learning rate can be suggested using a logarithmic scale.\n\n\nShow the code\ndef get_search_space(trial, default_params):\n    lr_param = {'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-3)}\n    default_params.update(lr_param)\n    wdecay_param = {'weight_decay': trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)}\n    default_params.update(wdecay_param)\n    hidden_size_param={'hidden_size': trial.suggest_categorical(\"hidden_size\", [8*2**i for i in range(6)])}\n    default_params.update(hidden_size_param)\n    batch_size_param={'hidden_size': trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])}\n    default_params.update(hidden_size_param)\n    layer_size_param={\"layer_size\": trial.suggest_int(\"layer_size\", 2, 5)}\n    default_params.update(layer_size_param)\n    dropout_param={\"dropout\": trial.suggest_float('dropout', 0.1, .5)}\n    default_params.update(dropout_param)\n    momentum_param = {'momentum': trial.suggest_float('momentum', 0.8, 1.0)}\n    default_params.update(momentum_param)\n    nest_param={'nesterov':trial.suggest_categorical(\"nesterov\", [False, True])}\n    default_params.update(nest_param)\n    return default_params\n\n\nTo create an objective function, we use the trainer module within PyTorch lightning with the default TensorBoard logger. The trainer will return the validation score. Optuna will use this score to evaluate the performance of the hyperparameters and decide where to sample in upcoming trials.\n\n\nShow the code\n#collapse\nfrom pathlib import Path\nfrom optuna.integration import PyTorchLightningPruningCallback\nDIR = Path(os.getcwd())\nMODEL_DIR = DIR/ \"MLP\"\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\nprint(f\"now run `tensorboard --logdir {MODEL_DIR}\")\n\nclass DictLogger(pl.loggers.TensorBoardLogger):\n    \"\"\"PyTorch Lightning `dict` logger.\"\"\"\n    # see https://github.com/PyTorchLightning/pytorch-lightning/blob/50881c0b31/pytorch_lightning/logging/base.py\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.metrics = [] \n\n    def log_metrics(self, metrics, step=None):\n        super().log_metrics(metrics, step=step)\n        self.metrics.append(metrics)\n\n\nnow run `tensorboard --logdir /Users/sambaiga/Documents/sambaiga/_notebooks/MLP\n\n\nIn addition to sampling strategies, Optuna provides a mechanism to automatically stops unpromising trials at the early stages of the training. This allows computing time to be used for tests that show more potential. This feature is called pruning, and it is a form of automated early-stopping. The PyTorchLightingPruningCallBack provides integration Optuna pruning function to PyTorch lightning. Pruner using the median stopping rule.\n\n\nShow the code\n#collapse\ndefault_params = {\"in_size\": 28*28, \"hidden_size\":128, \"out_size\":10, \n           \"layer_size\":5, \"dropout\":0.2, \"batch_size\":32,\n          'learning_rate':1e-3, 'momentum':0.9, 'nesterov': True,\n          'weight_decay':1e-5,\n          'epochs':50}\n\ndef objective(trial=None):\n    \n   \n    hparams = get_search_space(trial, default_params) \n    early_stop_callback =  PyTorchLightningPruningCallback(trial, monitor='val_acc')\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        os.path.join(MODEL_DIR, \"trial_{}\".format(trial.number)), monitor=\"val_acc\"\n        )   \n    \n    logger = DictLogger(MODEL_DIR,  version=trial.number)\n    trainer = pl.Trainer(\n                    logger = logger,\n                    checkpoint_callback=checkpoint_callback,\n                    max_epochs=10,\n                    gpus=1 if torch.cuda.is_available() else None,\n                    callbacks=[early_stop_callback],\n                     )\n    \n    model = MLPIL(hparams)\n    trainer.fit(model)\n    if trial is not None:\n        return logger.metrics[-1]['val_acc']\n\n\nTo start the optimization, we create a study object and pass the objective function to method optimize() as follows.\n\n\nShow the code\n##collapse\nimport optuna\ndef run_study(num_trials=10):\n    #Activate the pruning feature. `MedianPruner` stops unpromising \n    # Set up the median stopping rule as the pruning condition\n    pruner = optuna.pruners.MedianPruner()\n    study = optuna.create_study(pruner=pruner, direction='maximize')\n    study.optimize(objective, n_trials=num_trials)\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n    return study\n\n\nTo turn on the pruning feature, we hve to set up the the pruning condition which periodically monitors the intermediate objective values. Several pruning condistions such as Hyperband, Successive Halving exists as decribed in optuna documentation. For this example we will use the MedianPruner() which prune if the trial‚Äôs best intermediate result is worse than median of intermediate results of previous trials at the same step.\n\n\nShow the code\n#collapse\nstudy = run_study(num_trials=10)\n\n\n\n[I 2020-12-28 14:27:44,869] A new study created in memory with name: no-name-cad64f6f-c459-4dac-906c-d0b661f26694\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 64.0 K\n\n-------------------------------\n\n64.0 K    Trainable params\n\n0         Non-trainable params\n\n64.0 K    Total params\n\n\n\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw/train-images-idx3-ubyte.gz to /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n\n\n\nExtracting /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw/train-labels-idx1-ubyte.gz to /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n\n\nExtracting /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw/t10k-images-idx3-ubyte.gz to /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n\n\n\n\n\nExtracting /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/sambaiga/Documents/sambaiga/_notebooks/MNIST/raw\nProcessing...\nDone!\n\n\n/Applications/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1607370249289/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n\n\n\n\n\n\n\n\n\n/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The validation_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule\n  warnings.warn(*args, **kwargs)\n/Applications/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\nPlease use self.log(...) inside the lightningModule instead.\n\n# log on a step or aggregate epoch metric to the logger and/or progress bar\n# (inside LightningModule)\nself.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n  warnings.warn(*args, **kwargs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:31:09,612] Trial 0 finished with value: 0.9417931437492371 and parameters: {'learning_rate': 0.0004392222423308108, 'weight_decay': 0.0009213530236175221, 'hidden_size': 64, 'batch_size': 32, 'layer_size': 3, 'dropout': 0.1884183048052338, 'momentum': 0.8907811858761201, 'nesterov': True}. Best is trial 0 with value: 0.9417931437492371.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 9.8 K \n\n-------------------------------\n\n9.8 K     Trainable params\n\n0         Non-trainable params\n\n9.8 K     Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:34:24,649] Trial 1 finished with value: 0.47773560881614685 and parameters: {'learning_rate': 0.000210847455990759, 'weight_decay': 0.0029306655542649016, 'hidden_size': 8, 'batch_size': 16, 'layer_size': 5, 'dropout': 0.4490430509829202, 'momentum': 0.886765777339878, 'nesterov': False}. Best is trial 0 with value: 0.9417931437492371.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 403 K \n\n-------------------------------\n\n403 K     Trainable params\n\n0         Non-trainable params\n\n403 K     Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:37:38,511] Trial 2 finished with value: 0.9717451930046082 and parameters: {'learning_rate': 0.0003310757219532612, 'weight_decay': 0.0020794632433942238, 'hidden_size': 256, 'batch_size': 32, 'layer_size': 3, 'dropout': 0.4898375477885403, 'momentum': 0.976922034066602, 'nesterov': False}. Best is trial 2 with value: 0.9717451930046082.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 98.3 K\n\n-------------------------------\n\n98.3 K    Trainable params\n\n0         Non-trainable params\n\n98.3 K    Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:40:41,011] Trial 3 finished with value: 0.873302698135376 and parameters: {'learning_rate': 0.00016992353395788796, 'weight_decay': 0.002897482820811745, 'hidden_size': 64, 'batch_size': 16, 'layer_size': 4, 'dropout': 0.2518722294101543, 'momentum': 0.803572282976545, 'nesterov': True}. Best is trial 2 with value: 0.9717451930046082.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 13.0 K\n\n-------------------------------\n\n13.0 K    Trainable params\n\n0         Non-trainable params\n\n13.0 K    Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:43:40,189] Trial 4 finished with value: 0.9001597166061401 and parameters: {'learning_rate': 0.0004095091759246433, 'weight_decay': 0.00015252611014710424, 'hidden_size': 16, 'batch_size': 32, 'layer_size': 2, 'dropout': 0.15333728438176364, 'momentum': 0.8608018414539563, 'nesterov': True}. Best is trial 2 with value: 0.9717451930046082.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 72.2 K\n\n-------------------------------\n\n72.2 K    Trainable params\n\n0         Non-trainable params\n\n72.2 K    Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:43:58,553] Trial 5 pruned. Trial was pruned at epoch 0.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 3.0 M \n\n-------------------------------\n\n3.0 M     Trainable params\n\n0         Non-trainable params\n\n3.0 M     Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:45:07,262] Trial 6 pruned. Trial was pruned at epoch 0.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 152 K \n\n-------------------------------\n\n152 K     Trainable params\n\n0         Non-trainable params\n\n152 K     Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:47:59,340] Trial 7 finished with value: 0.9574680328369141 and parameters: {'learning_rate': 0.00026735474130607265, 'weight_decay': 0.00018664178193905678, 'hidden_size': 128, 'batch_size': 64, 'layer_size': 3, 'dropout': 0.3290301121858942, 'momentum': 0.9583935569391092, 'nesterov': True}. Best is trial 2 with value: 0.9717451930046082.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 403 K \n\n-------------------------------\n\n403 K     Trainable params\n\n0         Non-trainable params\n\n403 K     Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:51:16,962] Trial 8 finished with value: 0.953973650932312 and parameters: {'learning_rate': 0.0006495764965811859, 'weight_decay': 5.776203196248651e-05, 'hidden_size': 256, 'batch_size': 16, 'layer_size': 3, 'dropout': 0.28565278565016705, 'momentum': 0.8515094054632695, 'nesterov': False}. Best is trial 2 with value: 0.9717451930046082.\n\nGPU available: False, used: False\n\nTPU available: None, using: 0 TPU cores\n\n\n\n  | Name  | Type | Params\n\n-------------------------------\n\n0 | model | MLP  | 3.0 M \n\n-------------------------------\n\n3.0 M     Trainable params\n\n0         Non-trainable params\n\n3.0 M     Total params\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[I 2020-12-28 14:52:40,798] Trial 9 pruned. Trial was pruned at epoch 0.\n\n\n\n\n\nNumber of finished trials: 10\nBest trial:\n  Value: 0.9717451930046082\n  Params: \n    learning_rate: 0.0003310757219532612\n    weight_decay: 0.0020794632433942238\n    hidden_size: 256\n    batch_size: 32\n    layer_size: 3\n    dropout: 0.4898375477885403\n    momentum: 0.976922034066602\n    nesterov: False\n\n\nAfter the study is completed, we can export trials as a pandas data frame. This provides various features to analyze studies. It is also useful to draw a histogram of objective values and to export trials as a CSV file.\ndf = study.trials_dataframe()"
  },
  {
    "objectID": "blog/2024/03/optuna-hyperparmeter.html#visualize-study",
    "href": "blog/2024/03/optuna-hyperparmeter.html#visualize-study",
    "title": "Super-charge Deep learning hyper-parameter search with Optuna",
    "section": "Visualize study",
    "text": "Visualize study\n\n\nShow the code\nimport pandas as pd\ndf = study.trials_dataframe()\n\n\n\n\nShow the code\ndf\n\n\n\n\n\n\n\n\n\nnumber\nvalue\ndatetime_start\ndatetime_complete\nduration\nparams_batch_size\nparams_dropout\nparams_hidden_size\nparams_layer_size\nparams_learning_rate\nparams_momentum\nparams_nesterov\nparams_weight_decay\nstate\n\n\n\n\n0\n0\n0.941793\n2020-12-28 14:27:44.873276\n2020-12-28 14:31:09.610166\n00:03:24.736890\n32\n0.188418\n64\n3\n0.000439\n0.890781\nTrue\n0.000921\nCOMPLETE\n\n\n1\n1\n0.477736\n2020-12-28 14:31:09.614199\n2020-12-28 14:34:24.648593\n00:03:15.034394\n16\n0.449043\n8\n5\n0.000211\n0.886766\nFalse\n0.002931\nCOMPLETE\n\n\n2\n2\n0.971745\n2020-12-28 14:34:24.651528\n2020-12-28 14:37:38.510598\n00:03:13.859070\n32\n0.489838\n256\n3\n0.000331\n0.976922\nFalse\n0.002079\nCOMPLETE\n\n\n3\n3\n0.873303\n2020-12-28 14:37:38.513092\n2020-12-28 14:40:41.011327\n00:03:02.498235\n16\n0.251872\n64\n4\n0.000170\n0.803572\nTrue\n0.002897\nCOMPLETE\n\n\n4\n4\n0.900160\n2020-12-28 14:40:41.013425\n2020-12-28 14:43:40.182547\n00:02:59.169122\n32\n0.153337\n16\n2\n0.000410\n0.860802\nTrue\n0.000153\nCOMPLETE\n\n\n5\n5\n0.159645\n2020-12-28 14:43:40.190227\n2020-12-28 14:43:58.553771\n00:00:18.363544\n128\n0.383002\n32\n5\n0.000161\n0.837912\nTrue\n0.000282\nPRUNED\n\n\n6\n6\n0.387879\n2020-12-28 14:43:58.557600\n2020-12-28 14:45:07.261946\n00:01:08.704346\n64\n0.481376\n256\n5\n0.000113\n0.883971\nTrue\n0.002747\nPRUNED\n\n\n7\n7\n0.957468\n2020-12-28 14:45:07.265347\n2020-12-28 14:47:59.339796\n00:02:52.074449\n64\n0.329030\n128\n3\n0.000267\n0.958394\nTrue\n0.000187\nCOMPLETE\n\n\n8\n8\n0.953974\n2020-12-28 14:47:59.343730\n2020-12-28 14:51:16.962027\n00:03:17.618297\n16\n0.285653\n256\n3\n0.000650\n0.851509\nFalse\n0.000058\nCOMPLETE\n\n\n9\n9\n0.522664\n2020-12-28 14:51:16.964371\n2020-12-28 14:52:40.798304\n00:01:23.833933\n64\n0.191783\n256\n5\n0.000165\n0.863137\nTrue\n0.000017\nPRUNED\n\n\n\n\n\n\n\n\n\nShow the code\nstudy.best_params\n\n\n{'learning_rate': 0.0003310757219532612,\n 'weight_decay': 0.0020794632433942238,\n 'hidden_size': 256,\n 'batch_size': 32,\n 'layer_size': 3,\n 'dropout': 0.4898375477885403,\n 'momentum': 0.976922034066602,\n 'nesterov': False}\n\n\n\n\nShow the code\noptuna.visualization.plot_contour(study)\n\n\n                                                \n\n\n\n\nShow the code\noptuna.visualization.plot_optimization_history(study)\n\n\n                                                \n\n\n\n\nShow the code\noptuna.visualization.plot_param_importances(study)\n\n\n                                                \n\n\n\n\nShow the code\nimport shutil\nshutil.rmtree(MODEL_DIR)"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "This page includes a list of talks, lectures, workshops, and presentations I‚Äôve given, along with links to their slides."
  },
  {
    "objectID": "talks/index.html#section",
    "href": "talks/index.html#section",
    "title": "Talks",
    "section": "2024",
    "text": "2024\n\n\n\n    \n    \n                  \n            September 17, 2024\n        \n        \n            Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with  PV \n            IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids | Oslo, Norway\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n            \n            \n        \n    \n    \n    \n                  \n            July 2, 2024\n        \n        \n            Scalable and Efficient MLP-based  Fully Parameterised Quantile for Probabilistic Power Forecasting\n            44th International Symposium on Forecasting | Dijon, France\n\n            \n            \n                \n                \n                    \n                    \n                    \n                         Slides\n                    \n                \n                \n            \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Anthony Faustine",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n    \n        \n            \n                Anthony Faustine, and Pereira, Lucas, ‚ÄúFPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates,‚Äù IEEE Transactions on Smart Grid 2024, doi: 10.1109/TSG.2022.3148699\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Quantile regression\n            \n                 / Low voltage substation\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Analysis notebook\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine,Nunes, Nuno Jardim , and Pereira, Lucas, ‚ÄúEfficiency Through Simplicity: MLP-Based Approach for Net-Load Forecasting With Uncertainty Estimates in Low-Voltage Distribution Networks,‚Äù IEEE Transactions on Power Systems 2024, doi: 10.1109/TPWRS.2024.3400123\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Quantile regression\n            \n                 / Low voltage substation\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Hafsa Bousbiat,Anthony Faustine, Christoph Klemenjak, and Lucas Pereira ‚ÄúUnlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines,‚Äù IEEE Transactions on Industrial Informatics  vol. 19, no. 5, pp. 7002-7010 May 2023, doi: 10.1109/TII.2022.3206322.\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúImproved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks,‚Äù Energies 2020, 13, 3374, doi: 10.3390/en13133374\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n                 / nilm state prediction\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Benjamin V√∂lker, Andreas Reinhardt, Anthony Faustine and Pereira, Lucas, ‚ÄúWatt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective,‚Äù Energies 2021, 14(3), 719, doi: 10.1109/10.3390/en14030719\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n                    \n                            disagregation\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n                 / nilm state prediction\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine,  Lucas Pereira and Christoph Klemenjak ‚ÄúAdaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring,‚Äù  IEEE Transactions on Smart Grid  vol. 12, no. 1, pp. 398-406, Jan. 2021, doi: 10.1109/TSG.2020.3010621.\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n                 / nilm state prediction\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine, and Pereira, Lucas, ‚ÄúMulti-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network,‚Äù Energies  2020, 13, 4154., doi: 10.1109/TSG.2022.3148699\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                sustainable energy\n            \n                 / buildings\n            \n                 / nilm state prediction\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Published\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#conference-papers",
    "href": "research/index.html#conference-papers",
    "title": "Anthony Faustine",
    "section": "Conference papers",
    "text": "Conference papers\n\n\n\n    \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúEnhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power,‚Äù  CIRED Chicago Workshop 2024: Resilience of Electric Distribution Systems, Chicago, USA,  2025, pp. 27-31, doi: 10.1049/icp.2024.2555\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúScalable and Efficient MLP-based  Fully Parameterised Quantile for Probabilistic Power Forecasting,‚Äù 44th International Symposium on Forecasting, Dijon, France  2024\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúApplying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring,‚Äù ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece  2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10096324\n            \n\n            \n            \n                \n                    \n                            nilm\n                        \n                    \n                    \n                            disaggregation\n                        \n                    \n                    \n                            buildings\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Anthony Faustine and Pereira, Lucas, ‚ÄúConformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation,‚Äù 2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm), Oslo, Norway  2024, pp. 59-64, doi: 10.1109/SmartGridComm60555.2024.10738106\n            \n\n            \n            \n                \n                    \n                            forecasting\n                        \n                    \n                    \n                            probabilistic\n                        \n                    \n                    \n                            machine learning\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details ¬ª\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\nNo matching items"
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html",
    "href": "research/conferences/conformalmlpf-2024/index.html",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html#important-links",
    "href": "research/conferences/conformalmlpf-2024/index.html#important-links",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html#abstract",
    "href": "research/conferences/conformalmlpf-2024/index.html#abstract",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "Abstract",
    "text": "Abstract\nProbabilistic net-load forecasting in Low-Voltage (LV) distribution networks is essential in light of the increased variability introduced by the widespread integration of renewable energy sources (RES). Various probabilistic approaches based on neural networks have been proposed to solve this challenge. This study introduces lightweight neural network-based conformal prediction (Conformal-MLPF) for net-load forecasting within an LV power distribution network. It uses Split Conformal prediction to transform a lightweight MLP-based point forecast into a probabilistic forecast. Our validation on two real-life LV substations datasets suggests that the proposed Conformal-MLPF achieves a better tradeoff between forecasting performance and model complexity without requiring restrictive assumptions about data distribution."
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html#important-figure",
    "href": "research/conferences/conformalmlpf-2024/index.html#important-figure",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "Important figure",
    "text": "Important figure\nFigure 5: Derogation decisions across pandemic violations and pandemic backsliding\n\n\n\nFigure 5: Derogation decisions across pandemic violations and pandemic backsliding"
  },
  {
    "objectID": "research/conferences/conformalmlpf-2024/index.html#bibtex-citation",
    "href": "research/conferences/conformalmlpf-2024/index.html#bibtex-citation",
    "title": "Conformal Multilayer Perceptron-Based Probabilistic Net-Load Forecasting for Low-Voltage Distribution Systems with Photovoltaic Generation",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{ChaudhryHeiss:2021,\n    Author = {Suparna Chaudhry and Anthony Faustine},\n    Note = {Working paper},\n    Title = {Derogations and Democratic Backsliding: Exploring the Pandemic's Effects on Civic Spaces},\n    Year = {2021}}"
  },
  {
    "objectID": "research/conferences/ICASSP 2023/index.html",
    "href": "research/conferences/ICASSP 2023/index.html",
    "title": "Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/ICASSP 2023/index.html#important-links",
    "href": "research/conferences/ICASSP 2023/index.html#important-links",
    "title": "Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/ICASSP 2023/index.html#abstract",
    "href": "research/conferences/ICASSP 2023/index.html#abstract",
    "title": "Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring",
    "section": "Abstract",
    "text": "Abstract\nIndustrial loads offer challenges for Non-intrusive Load Monitoring (NILM), such as phase imbalance associated with 3-phase lines. However, very little NILM research has been developed so far with this respect. This work presents a load recognition technique for NILM applying low complexity Fortesque Transform (FT). The FT decomposes the unbalanced 3-phase current waveform extracted from 3-phase aggregate power measurements to balance the given load. The 3-phases current waveform is transformed into an image-like representation using a compressedeuclidean distance matrix to improve the recognition ability further. The image representation is used as input to Convolutional Neural Network (CNN) classifier to learn the patterns of labeled data. Experimental evaluation of the industrial aggregated dataset shows that FT improves recognition performance by 5.8%, compared to the case without FT."
  },
  {
    "objectID": "research/conferences/ICASSP 2023/index.html#bibtex-citation",
    "href": "research/conferences/ICASSP 2023/index.html#bibtex-citation",
    "title": "Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@INPROCEEDINGS{10096324,\n  author={Faustine, Anthony and Pereira, Lucas},\n  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \n  title={Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring}, \n  year={2023},\n  volume={},\n  number={},\n  pages={1-5},\n  keywords={Load monitoring;Symmetric matrices;Power measurement;Power demand;Transforms;Signal processing;Convolutional neural networks;NILM;Industrial Appliances;Three-Phase;Fortesque Transform;Symmetrical Components},\n  doi={10.1109/ICASSP49357.2023.10096324}}"
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html",
    "href": "research/articles/recurrence-nilm/index.html",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html#important-links",
    "href": "research/articles/recurrence-nilm/index.html#important-links",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html#abstract",
    "href": "research/articles/recurrence-nilm/index.html#abstract",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "Abstract",
    "text": "Abstract\nPower demand forecasting is becoming a crucial tool for the planning and operation of Low Voltage (LV) distribution systems. Most importantly, the high penetration of Photovoltaics (PV) power generation as part of Distributed Energy Resource (DER)s has transformed the power demand forecasting problem at the distribution level into net-load forecasting. This paper introduces a novel and scalable approach to probabilistic forecasting at LV substation with PV generation. It presents a multi-variates probabilistic forecasting approach, leveraging Quantile Regression (QR). The proposed architecture uses a computationally efficient feed-forward neural net to capture the complex interaction between the historical load demands and covariate variables such as solar irradiance. It is empirically demonstrated that the proposed method can efficiently produce well-calibrated forecasts, both auto-regressively or in a single forward pass. Furthermore, a benchmark against four state-of-the-art forecasting approaches shows that the proposed approach offers a desirable trade-off between forecasting accuracies, calibrated uncertainty, and computation complexity."
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html#important-figures",
    "href": "research/articles/recurrence-nilm/index.html#important-figures",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2: Predicted probability of impartiality by levels of equality and democracy\n\n\n\n\n\nFigure 4: Tangents to the prediction function at 25 and 50"
  },
  {
    "objectID": "research/articles/recurrence-nilm/index.html#citation",
    "href": "research/articles/recurrence-nilm/index.html#citation",
    "title": "Improved Appliance Classification in Non-Intrusive Load Monitoring Using Weighted Recurrence Graph and Convolutional Neural Networks",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{10529636,\n  author={Faustine, Anthony and Nunes, Nuno Jardim and Pereira, Lucas},\n  journal={IEEE Transactions on Power Systems}, \n  title={Efficiency Through Simplicity: MLP-Based Approach for Net-Load Forecasting With Uncertainty Estimates in Low-Voltage Distribution Networks}, \n  year={2025},\n  volume={40},\n  number={1},\n  pages={46-56},\n  keywords={Forecasting;Uncertainty;Probabilistic logic;Predictive models;Substations;Load modeling;Distribution networks;Deep Neural Networks (DNN) Feed-forward Neural Network (FFN) Low Voltage (LV) distribution substation;Multilayer Perceptron (MLP);net-load;probabilistic forecasting;Photovoltaics (PV) generation;quantile regression (QR)},\n  doi={10.1109/TPWRS.2024.3400123}}"
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html",
    "href": "research/articles/multilabel-nilm/index.html",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html#important-links",
    "href": "research/articles/multilabel-nilm/index.html#important-links",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html#abstract",
    "href": "research/articles/multilabel-nilm/index.html#abstract",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "Abstract",
    "text": "Abstract\nThe increased penetration of Renewable Energy Sources (RES) as part of a decentralized and distributed power system makes net-load forecasting a critical component in the planning and operation of power systems. However, compared to the transmission level, producing accurate short-term net-load forecasts at the distribution level is complex due to the small number of consumers. Moreover, owing to the stochastic nature of RES, it is necessary to quantify the uncertainty of the forecasted net-load at any given time, which is critical for the real-world decision process. This work presents parameterized deep quantile regression for short-term probabilistic net-load forecasting at the distribution level. To be precise, we use a Deep Neural Network (DNN) to learn both the quantile fractions and quantile values of the quantile function. Furthermore, we propose a scoring metric that reflects the trade-off between predictive uncertainty performance and forecast accuracy. We evaluate the proposed techniques on historical real-world data from a low-voltage distribution substation and further assess its robustness when applied in real-time. The experiment‚Äôs outcomes show that the resulting forecasts from our approach are well-calibrated and provide a desirable trade-off between forecasting accuracies and predictive uncertainty performance that are very robust even when applied in real-time."
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html#important-figures",
    "href": "research/articles/multilabel-nilm/index.html#important-figures",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR\n\n\n\n\n\nFigure 3a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR"
  },
  {
    "objectID": "research/articles/multilabel-nilm/index.html#citation",
    "href": "research/articles/multilabel-nilm/index.html#citation",
    "title": "Multi-Label Learning for Appliance Recognition in NILM Using Fryze-Current Decomposition and Convolutional Neural Network",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{9701598,\n  author={Faustine, Anthony and Pereira, Lucas},\n  journal={IEEE Transactions on Smart Grid}, \n  title={FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates}, \n  year={2022},\n  volume={13},\n  number={3},\n  pages={2440-2451},\n  keywords={Forecasting;Uncertainty;Predictive models;Probabilistic logic;Renewable energy sources;Load modeling;Additives;Net-load;forecasting;uncertainity;deep neural network;quantile regression},\n  doi={10.1109/TSG.2022.3148699}}"
  },
  {
    "objectID": "research/articles/fqr-2024/index.html",
    "href": "research/articles/fqr-2024/index.html",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/fqr-2024/index.html#important-links",
    "href": "research/articles/fqr-2024/index.html#important-links",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "",
    "text": "Paper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/fqr-2024/index.html#abstract",
    "href": "research/articles/fqr-2024/index.html#abstract",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "Abstract",
    "text": "Abstract\nThe increased penetration of Renewable Energy Sources (RES) as part of a decentralized and distributed power system makes net-load forecasting a critical component in the planning and operation of power systems. However, compared to the transmission level, producing accurate short-term net-load forecasts at the distribution level is complex due to the small number of consumers. Moreover, owing to the stochastic nature of RES, it is necessary to quantify the uncertainty of the forecasted net-load at any given time, which is critical for the real-world decision process. This work presents parameterized deep quantile regression for short-term probabilistic net-load forecasting at the distribution level. To be precise, we use a Deep Neural Network (DNN) to learn both the quantile fractions and quantile values of the quantile function. Furthermore, we propose a scoring metric that reflects the trade-off between predictive uncertainty performance and forecast accuracy. We evaluate the proposed techniques on historical real-world data from a low-voltage distribution substation and further assess its robustness when applied in real-time. The experiment‚Äôs outcomes show that the resulting forecasts from our approach are well-calibrated and provide a desirable trade-off between forecasting accuracies and predictive uncertainty performance that are very robust even when applied in real-time."
  },
  {
    "objectID": "research/articles/fqr-2024/index.html#important-figures",
    "href": "research/articles/fqr-2024/index.html#important-figures",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR\n\n\n\n\n\nFigure 3a: Predicted effects of imposing specific emergency public health measures over first 15 months of the COVID pandemic, split by whether states formally derogated from the ICCPR"
  },
  {
    "objectID": "research/articles/fqr-2024/index.html#citation",
    "href": "research/articles/fqr-2024/index.html#citation",
    "title": "FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{9701598,\n  author={Faustine, Anthony and Pereira, Lucas},\n  journal={IEEE Transactions on Smart Grid}, \n  title={FPSeq2Q: Fully Parameterized Sequence to Quantile Regression for Net-Load Forecasting With Uncertainty Estimates}, \n  year={2022},\n  volume={13},\n  number={3},\n  pages={2440-2451},\n  keywords={Forecasting;Uncertainty;Predictive models;Probabilistic logic;Renewable energy sources;Load modeling;Additives;Net-load;forecasting;uncertainity;deep neural network;quantile regression},\n  doi={10.1109/TSG.2022.3148699}}"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Data Visualization with R \n                \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Fall 2024 (asynchronous online)\n                \n                \n                \n                 Spring 2025 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Comparative Public Administration \n                \n            \n            \n                PMAP 8441/4441 | \n                Georgia State University\n                \n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n\n            \n                \n                \n                 Fall 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2025\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "project/index.html#section",
    "href": "project/index.html#section",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Data Visualization with R \n                \n            \n            \n                PMAP 8551/4551 | \n                Georgia State University\n                \n            \n            Use R, ggplot2, and the principles of graphic design to create beautiful and truthful visualizations of data\n\n            \n                \n                \n                 Fall 2024 (asynchronous online)\n                \n                \n                \n                 Spring 2025 (asynchronous online)\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Comparative Public Administration \n                \n            \n            \n                PMAP 8441/4441 | \n                Georgia State University\n                \n            \n            Explore how the public and nonprofit sectors work around the world and learn how political institutions shape government and civil society\n\n            \n                \n                \n                 Fall 2024\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Program Evaluation for Public Service \n                \n            \n            \n                PMAP 8521 | \n                Georgia State University\n                \n            \n            Combine research design, causal inference, and econometric tools to measure the effects of social programs\n\n            \n                \n                \n                 Spring 2025\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "I‚Äôm Anthony Faustine, a Lead Data Scientist at Eaton‚Äôs Centre for Intelligent Power (CIP) in Dublin, Ireland. At Eaton, my focus is on applying machine learning and data-driven innovation initiatives that contribute to building intelligent power systems for a sustainable energy future.\nPlease feel free to explore my website to learn more about my work, and research interests. I‚Äôm always interested in connecting with others who share my passion for data science, AI, and innovation.\nRead more ‚Üí"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Bayesian Regression: A Real-World Battery Degradation Case Study\n      \n      Learn how Bayesian regression works in practice using PyMC, through a real-world battery degradation case study.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          bayesian\n          \n        \n        \n      \n\n      \n        \n        December 17, 2025\n        \n      \n    \n  \n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        Understanding Bayesian Thinking for Industrial Applications\n      \n      Learn how Bayesian thinking can enhance decision-making in industrial applications. This article lay the foundation of bayesion modelling with Pymc and their practical use cases.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          bayesian\n          \n        \n        \n      \n\n      \n        \n        October 10, 2025\n        \n      \n    \n  \n  \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Bayesian Regression: A Real-World Battery Degradation Case Study\n      \n      Learn how Bayesian regression works in practice using PyMC, through a real-world battery degradation case study.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          bayesian\n          \n        \n        \n      \n\n      \n        \n        December 17, 2025\n        \n      \n    \n  \n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        Understanding Bayesian Thinking for Industrial Applications\n      \n      Learn how Bayesian thinking can enhance decision-making in industrial applications. This article lay the foundation of bayesion modelling with Pymc and their practical use cases.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          bayesian\n          \n        \n        \n      \n\n      \n        \n        October 10, 2025\n        \n      \n    \n  \n  \n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Anthony Faustine",
    "section": "2024",
    "text": "2024\n\n\n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        Capabilities vs. Routines: Operationalizing Dynamic Strategy with Corporate Foresight (Part 2/3)\n      \n      We break down the two main theoretical approaches to Dynamic Capabilities‚Äîthe managerial Capabilities view and the process-oriented Routines view‚Äîand introduce Corporate Foresight as a key cultivation mechanism.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          Strategy\n          \n          Leadership\n          \n        \n        \n      \n\n      \n        \n        September 1, 2024\n        \n      \n    \n  \n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        Dynamic Capabilities: Why Reconfiguring Resources is Your Only Path to Long-Term Success (Part 1/3)\n      \n      Explore the fundamental concept of Dynamic Capabilities, why possessing resources is no longer enough, and how leading scholars define the engine of strategic renewal.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          Strategy\n          \n          Leadership\n          \n        \n        \n      \n\n      \n        \n        August 1, 2024\n        \n      \n    \n  \n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        Shared Mental Models: The Invisible Architecture of High-Performance Teams\n      \n      This post explores how shared mental models common understandings of goals, roles, and teamwork‚Äîform the foundation of high performing teams. It also highlights the crucial role of leadership in developing and sustaining these models, especially in virtual and hybrid environments.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          Team leadership\n          \n          Leadership\n          \n        \n        \n      \n\n      \n        \n        July 12, 2024\n        \n      \n    \n  \n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        The Lone Wolf vs. The Pack: Why Your Change Strategy Is Failing\n      \n      This post explores how Kurt Lewin‚Äôs theory of group dynamics reveals that lasting organizational change depends less on individual effort and more on engaging and transforming the collective forces within groups.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          Change Management\n          \n          Leadership\n          \n        \n        \n      \n\n      \n        \n        June 12, 2024\n        \n      \n    \n  \n  \n  \n    \n    \n    \n    \n      \n\n    \n\n    \n    \n      \n        Super-charge Deep learning hyper-parameter search with Optuna\n      \n       Learn how to perform deep learning hyper-parameter search using Pytorch Lightning and Optuna.\n      \n      \n      \n        Read More ‚Üí\n      \n    \n\n    \n    \n      \n        \n        Anthony Faustine\n        \n\n        \n        \n          \n          python\n          \n          deep-learning\n          \n          machine-learning\n          \n        \n        \n      \n\n      \n        \n        March 10, 2024\n        \n      \n    \n  \n  \n\nNo matching items"
  },
  {
    "objectID": "blog/2024/09/index.html",
    "href": "blog/2024/09/index.html",
    "title": "Capabilities vs.¬†Routines: Operationalizing Dynamic Strategy with Corporate Foresight (Part 2/3)",
    "section": "",
    "text": "In Part 1, we established that Dynamic Capabilities are the essential strategic ability for firms to continuously reconfigure their resources to achieve competitive advantage. Here, we dive into the theoretical and practical framework that explains how this continuous adaptation actually happens."
  },
  {
    "objectID": "blog/2024/09/index.html#the-capabilities-view-teece-1997",
    "href": "blog/2024/09/index.html#the-capabilities-view-teece-1997",
    "title": "Capabilities vs.¬†Routines: Operationalizing Dynamic Strategy with Corporate Foresight (Part 2/3)",
    "section": "The Capabilities View (Teece, 1997)",
    "text": "The Capabilities View (Teece, 1997)\nThis influential stream, built on the work of (teece1997dynamic?), views dynamic capabilities as organizational capabilities that are flexible, adaptable, and distinct from everyday routines. The emphasis is on management‚Äôs role in reconfiguring resources and assets to address market changes (teece2012dynamic?; teece2014dynamic?).\nAccording to this perspective, dynamic capabilities are the key competence for sustained growth, enabling firms to align and re-align resources quickly to generate abnormal returns (teece2012dynamic?). The critical ingredients are management capabilities:\n\nManagerial & Entrepreneurial Action: Good strategies, execution, and difficult-to-imitate combinations of organizational, functional, and technological skills (teece1997dynamic?; schwarz2rohrbecf?).\nOrganizational Pillars: The firm‚Äôs capability rests on the vision and leadership skills of managers and the cohesion and flexibility of the organization as a whole (schwarz2rohrbecf?).\n\nThis view also incorporates the six crucial capabilities of entrepreneurial leaders: anticipate, challenge, interpret, decide, align, and learn (day2016adapting?), which must be applied together (schwarz2rohrbecf?)."
  },
  {
    "objectID": "blog/2024/09/index.html#the-routines-view-eisenhardt-martin-2000",
    "href": "blog/2024/09/index.html#the-routines-view-eisenhardt-martin-2000",
    "title": "Capabilities vs.¬†Routines: Operationalizing Dynamic Strategy with Corporate Foresight (Part 2/3)",
    "section": "The Routines View (Eisenhardt & Martin, 2000)",
    "text": "The Routines View (Eisenhardt & Martin, 2000)\nIn contrast, the stream led by (eisenhardt2000dynamic?) focuses on the operational aspect, viewing dynamic capability as context-specific routines or processes that organizations use to achieve strategic fit with their environment.\nThese are observable processes that reside in existing functions like product development, strategic decision-making, resource allocation, alliances, and R&D (eisenhardt2000dynamic?; schwarz2rohrbecf?). This approach highlights the varied, practical capabilities firms use to adapt and evolve in response to changes, providing concrete examples of how dynamic capabilities are enacted."
  },
  {
    "objectID": "blog/2024/09/index.html#the-combined-framework-and-operationalization",
    "href": "blog/2024/09/index.html#the-combined-framework-and-operationalization",
    "title": "Capabilities vs.¬†Routines: Operationalizing Dynamic Strategy with Corporate Foresight (Part 2/3)",
    "section": "The Combined Framework and Operationalization",
    "text": "The Combined Framework and Operationalization\nThe two streams of dynamic capabilities‚ÄîTeece‚Äôs focus on managerial Capabilities and Eisenhardt‚Äôs focus on Routines‚Äîare not opposing theories, but rather complementary views of the same strategic ability.\nThe (teece1997dynamic?) and (eisenhardt2000dynamic?) dynamic capabilities views are complementary and can be combined as illustrated in Figure¬†1. It becomes evident that dynamic capabilities, as perceived by (teece1997dynamic?), need established routines and processes for them to contribute to a competitive advantage and attain sustained firm performance. These routines and processes can be seen as the operationalization of dynamic capabilities, allowing a firm to effectively align and re-align its resources to meet the demands and opportunities of the business environment. Without these routines and processes in place, the ability to develop and acquire new resources and assets may not be fully realized, and the firm may not be able to achieve sustained performance.\nThis perspective aligns with the results presented in (di2014organizational?), which suggests that dynamic capabilities are not only about the development and acquisition of new resources and assets, but also about the ability to put these resources and assets into action through established routines and processes. (di2014organizational?) emphasizes that both latent capabilities ((teece1997dynamic?) view) and observable routines are important for firms to achieve sustainable growth and competitiveness (di2014organizational?).\nFurthermore, (kump2019toward?) argues that firms may possess high levels of certain aspects of dynamic capabilities and have established reliable routines that lead to similar dynamic capabilities outcomes, but they may differ in terms of the concrete methods and structures of how they achieve these dynamic capabilities outcomes. This diversity in implementation of dynamic capabilities can also be a source of competitive advantage (kump2019toward?).\n\n\n\n\n\n\nDynamic capability framework combining capability and routine views source: (pereira2015role?)\n\n\n\n\nFigure¬†1"
  },
  {
    "objectID": "blog/2024/09/index.html#the-role-of-corporate-foresight-cultivating-dynamic-capability",
    "href": "blog/2024/09/index.html#the-role-of-corporate-foresight-cultivating-dynamic-capability",
    "title": "Capabilities vs.¬†Routines: Operationalizing Dynamic Strategy with Corporate Foresight (Part 2/3)",
    "section": "The Role of Corporate Foresight: Cultivating Dynamic Capability",
    "text": "The Role of Corporate Foresight: Cultivating Dynamic Capability\nThe practical question remains: how are these complex capabilities and routines actually cultivated? A growing body of work points to Corporate Foresight as a crucial mechanism.\nA recent study by (schwarz2rohrbecf?) posits that the incorporation of corporate foresight (CF) practices at the organizational level and individual level is likely to have a beneficial effect on the cultivation and resultant outcome of dynamic capabilities. Corporate foresight has been described as a set of intentional and often routine-based practices that organizations adopt to anticipate and prepare for future change. It can also be seen as an individual ability to envision and plan for the future (schwarz2hrbecf?).\nThis concept neatly maps onto the two main theoretical streams: (schwarz2rohrbecf?) conceptualised corporate foresight practices on the organisational level as deliberate practices that an organisation has built and that often are routinised ((eisenhardt2000dynamic?) view) and on an individual level as managerial capabilities ((teece1997dynamic?) view) such as managerial cognition, managerial social capital, and managerial human capital (schwarz2rohrbecf?). The study discovered that the implementation of corporate foresight practices and training can aid in the growth of a company‚Äôs dynamic capabilities (schwarz2rohrbecf?). This finding aligns with (day2016adapting?), who argue that corporate foresight training and practices have a significant impact on dynamic managerial capabilities and dynamic capabilities at the organizational level. Furthermore, it emphasizes that dynamic capabilities are not only based on traditional processes like research and development, strategic partnerships, and decision-making, as previously proposed by (eisenhardt2000dynamic?)."
  },
  {
    "objectID": "blog/2024/08/draft.html",
    "href": "blog/2024/08/draft.html",
    "title": "Dynamic Capabilities: How Top Firms Adapt, Innovate, and Thrive in High-Tech Markets",
    "section": "",
    "text": "Competition in high-technology industries worldwide has shown that accumulating valuable technologies, often protected by aggressive intellectual property strategies, is not sufficient for success (Teece, Pisano, and Shuen 1997). Instead, the most successful firms such as Microsoft, Tesla, Intel to name a few, have developed specific capabilities, including the ability to rapidly and flexibly innovate products and the management skills to coordinate and redeploy market positions and explore new opportunities effectively (Teece, Pisano, and Shuen 1997).\nAs stated by Witschel et al. (2019), only companies with unique abilities can adjust their resources to a constantly changing environment and effectively utilise the opportunities that arise from these changes. These capabilities are developed through the organisation‚Äôs learning process and cannot be obtained from the external environment (Witschel et al. 2019; Matysiak, Rugman, and Bausch 2018). The author further stress that difficult-to-replicate capabilities are essential to sustain long-term competitiveness. These capabilities and timely responsiveness are crucial to achieving competitive advantage in these industries and are referred to as dynamic capabilities."
  },
  {
    "objectID": "blog/2024/08/draft.html#the-capabilities-view-teece-1997",
    "href": "blog/2024/08/draft.html#the-capabilities-view-teece-1997",
    "title": "Dynamic Capabilities: How Top Firms Adapt, Innovate, and Thrive in High-Tech Markets",
    "section": "The Capabilities View (Teece, 1997)",
    "text": "The Capabilities View (Teece, 1997)\nThis influential stream, built on the work of Teece, Pisano, and Shuen (1997), views dynamic capabilities as organizational capabilities that are flexible, adaptable, and distinct from everyday routines. The emphasis is on management‚Äôs role in reconfiguring resources and assets to address market changes (Teece 2012, 2014).\nAccording to this perspective, dynamic capabilities are the key competence for sustained growth, enabling firms to align and re-align resources quickly to generate abnormal returns (Teece 2012). The critical ingredients are management capabilities:\nManagerial & Entrepreneurial Action: Good strategies, execution, and difficult-to-imitate combinations of organizational, functional, and technological skills (Teece, Pisano, and Shuen 1997; Schwarz, Rohrbeck, and Wach 2020).\nOrganizational Pillars: The firm‚Äôs capability rests on the vision and leadership skills of managers and the cohesion and flexibility of the organization as a whole (Schwarz, Rohrbeck, and Wach 2020).\nThis view also incorporates the six crucial capabilities of entrepreneurial leaders: anticipate, challenge, interpret, decide, align, and learn (Day and Schoemaker 2016), which must be applied together (Schwarz, Rohrbeck, and Wach 2020)."
  },
  {
    "objectID": "blog/2024/08/draft.html#the-routines-view",
    "href": "blog/2024/08/draft.html#the-routines-view",
    "title": "Dynamic Capabilities: How Top Firms Adapt, Innovate, and Thrive in High-Tech Markets",
    "section": "The Routines View",
    "text": "The Routines View\nIn contrast, the stream led by Eisenhardt and Martin (2000) focuses on the operational aspect, viewing dynamic capability as context-specific routines or processes that organizations use to achieve strategic fit with their environment.\nThese are observable processes that reside in existing functions like product development, strategic decision-making, resource allocation, alliances, and R&D (Eisenhardt and Martin 2000; Schwarz, Rohrbeck, and Wach 2020). This approach highlights the varied, practical capabilities firms use to adapt and evolve in response to changes, providing concrete examples of how dynamic capabilities are enacted."
  },
  {
    "objectID": "blog/2024/08/draft.html#the-combined-framework-and-operationalization",
    "href": "blog/2024/08/draft.html#the-combined-framework-and-operationalization",
    "title": "Dynamic Capabilities: How Top Firms Adapt, Innovate, and Thrive in High-Tech Markets",
    "section": "The Combined Framework and Operationalization",
    "text": "The Combined Framework and Operationalization\nThe two streams of dynamic capabilities Teece‚Äôs focus on managerial Capabilities and Eisenhardt‚Äôs focus on Routines are not opposing theories, but rather complementary views of the same strategic ability.\nThe Teece, Pisano, and Shuen (1997) and Eisenhardt and Martin (2000) dynamic capabilities views are complementary and can be combined as illustrated in ?@fig-dc-framework. It becomes evident that dynamic capabilities, as perceived by Teece, Pisano, and Shuen (1997), need established routines and processes for them to contribute to a competitive advantage and attain sustained firm performance. These routines and processes can be seen as the operationalization of dynamic capabilities, allowing a firm to effectively align and re-align its resources to meet the demands and opportunities of the business environment. Without these routines and processes in place, the ability to develop and acquire new resources and assets may not be fully realized, and the firm may not be able to achieve sustained performance.\nThis perspective aligns with the results presented in Di Stefano, Peteraf, and Verona (2014), which suggests that dynamic capabilities are not only about the development and acquisition of new resources and assets, but also about the ability to put these resources and assets into action through established routines and processes. Di Stefano, Peteraf, and Verona (2014) emphasizes that both latent capabilities (Teece, Pisano, and Shuen (1997) view) and observable routines are important for firms to achieve sustainable growth and competitiveness (Di Stefano, Peteraf, and Verona 2014).\nFurthermore, Kump et al. (2019) argues that firms may possess high levels of certain aspects of dynamic capabilities and have established reliable routines that lead to similar dynamic capabilities outcomes, but they may differ in terms of the concrete methods and structures of how they achieve these dynamic capabilities outcomes. This diversity in implementation of dynamic capabilities can also be a source of competitive advantage (Kump et al. 2019)."
  },
  {
    "objectID": "blog/2024/08/draft.html#the-role-of-corporate-foresight-cultivating-dynamic-capability",
    "href": "blog/2024/08/draft.html#the-role-of-corporate-foresight-cultivating-dynamic-capability",
    "title": "Dynamic Capabilities: How Top Firms Adapt, Innovate, and Thrive in High-Tech Markets",
    "section": "The Role of Corporate Foresight Cultivating Dynamic Capability",
    "text": "The Role of Corporate Foresight Cultivating Dynamic Capability\nThe practical question remains: how are these complex capabilities and routines actually cultivated? A growing body of work points to Corporate Foresight as a crucial mechanism.\nA recent study by Schwarz, Rohrbeck, and Wach (2020) posits that the incorporation of corporate foresight (CF) practices at the organizational level and individual level is likely to have a beneficial effect on the cultivation and resultant outcome of dynamic capabilities. Corporate foresight has been described as a set of intentional and often routine-based practices that organizations adopt to anticipate and prepare for future change. It can also be seen as an individual ability to envision and plan for the future (Schwarz, Rohrbeck, and Wach 2020).\nThis concept neatly maps onto the two main theoretical streams: Schwarz, Rohrbeck, and Wach (2020) conceptualised corporate foresight practices on the organisational level as deliberate practices that an organisation has built and that often are routinised (Eisenhardt and Martin (2000) view) and on an individual level as managerial capabilities (Teece, Pisano, and Shuen (1997) view) such as managerial cognition, managerial social capital, and managerial human capital (Schwarz, Rohrbeck, and Wach 2020). The study discovered that the implementation of corporate foresight practices and training can aid in the growth of a company‚Äôs dynamic capabilities (Schwarz, Rohrbeck, and Wach 2020). This finding aligns with Day and Schoemaker (2016), who argue that corporate foresight training and practices have a significant impact on dynamic managerial capabilities and dynamic capabilities at the organizational level. Furthermore, it emphasizes that dynamic capabilities are not only based on traditional processes like research and development, strategic partnerships, and decision-making, as previously proposed by Eisenhardt and Martin (2000).\nThe Three Dimensions of Dynamic Capabilities: Sensing, Seizing, and Transforming While the definitions establish what dynamic capabilities are, the stages proposed by Teece, Pisano, and Shuen (1997) explain how they are executed. Dynamic capabilities are broken down into a continuous process consisting of three key stages: the ability to sense and shape opportunities (Sensing), the ability to seize opportunities (Seizing), and the ability to reconfigure resources and capabilities, and transform the organization (Transformation) as illustrated in Figure¬†1 and Figure¬†2.\nThis approach helps to explain how companies can adapt and change in response to their environments to achieve sustained competitive advantages (Matysiak, Rugman, and Bausch 2018; Teece 2012; Augier and Teece 2007). These stages need to be closely aligned with a firm‚Äôs strategy to effectively respond to changes and achieve sustained performance (Teece, Pisano, and Shuen 1997).\nSensing refers to identifying and assessing new business opportunities and threats.\nSeizing implies the act of mobilising resources to address opportunities and threats for the creation and exploitation of competitive advantages.\nTransformation involves managing change by re-configuring a firm‚Äôs core and complementary resources and capabilities in its daily operations to enhance them.\nThese stages work together to help a company adapt to its environment and achieve sustained competitive advantages (Matysiak, Rugman, and Bausch 2018; Teece 2012).\nSensing Dynamic capabilities require a firm to actively sense changes in its external environment to identify opportunities or threats. This is an essential element, as it enables a firm to proactively, rather than re-actively, identify and respond to changes (Schwarz, Rohrbeck, and Wach 2020; Day and Schoemaker 2016). Effectively sensing where resources can be leveraged for advantage is crucial for long-term success, helping the organization save efforts in areas that won‚Äôt yield meaningful benefits (Augier and Teece 2007).\nManagers‚Äô ability to recognize and create opportunities through environmental scanning and sense making is critical for sensing opportunities before they fully materialize (Teece, Pisano, and Shuen 1997; Schwarz, Rohrbeck, and Wach 2020; Day and Schoemaker 2016). Through sensing, an organization gains valuable insights into the rarity of its resources and how they can create competitive advantages. This information is crucial for making strategic decisions about where to focus efforts for seizing and transforming the organization. Sensing helps the organization to forecast industry changes and shifts, enabling it to be proactive in preparing for the future and identifying new growth prospects.\nSeizing Seizing is a critical component because it turns potential insights into tangible action. It allows a firm to commit to strategies and business models via investments to capitalize on the new opportunities identified through sensing. Augier and Teece (2007) notes that seizing involves creating and establishing linkages and structures within the firm to exploit competitive advantages effectively. Without this action, the potential benefits from sensing would be limited (Teece 2012).\nTransformation Identifying and taking advantage of new opportunities is the core of the transforming aspect. This requires firms to quickly recognize new opportunities in the market, adapt their strategies accordingly, and have the necessary resources and capabilities to implement these new strategies. Being able to quickly and effectively identify, respond to, and capitalize on new opportunities can give firms a significant market advantage and help them stay ahead of the competition (Augier and Teece 2007; Teece 2012).\nThe Outcome: Technical vs.¬†Evolutionary Fitness This three-stage process of Sensing-Seizing-Transforming naturally leads to a duality in the results of the firm‚Äôs efforts, aligning perfectly with the classification by Matysiak, Rugman, and Bausch (2018).\nMatysiak, Rugman, and Bausch (2018) classify dynamic capabilities into two categories that represent the dual goals achieved through the S-S-T process: technical fitness and evolutionary fitness.\nTechnical fitness refers to the ability of dynamic capabilities to use a company‚Äôs resources effectively and efficiently to achieve its goals. This corresponds strongly to the Transformation stage‚Äôs outcome‚Äîthe ability to optimize processes, make efficient use of resources, and effectively manage internal change.\nEvolutionary fitness, on the other hand, refers to the rarity and value of dynamic capabilities and their contribution to a company‚Äôs ability to adapt and change in response to environmental pressures and opportunities. This is the ultimate aim of the entire process, specifically driven by the initial Sensing stage (identifying new opportunities) and the Seizing stage (adapting to market conditions and responding to disruptions).\nBoth technical fitness (internal efficiency) and evolutionary fitness (external adaptability) are therefore critical for a company to maintain a sustainable competitive advantage (Matysiak, Rugman, and Bausch 2018). The three-dimensional model shows the actions (how to adapt), while the fitness model measures the results (how well it adapted).\n\n\n\n\n\n\nLinking leadership disciplines to dynamic capabilities source: Schwarz, Rohrbeck, and Wach (2020)\n\n\n\n\nFigure¬†1\n\n\n\n\n\n\n\n\n\nIllustration of dimension of dynamic capabilities source: Nagel (2016)\n\n\n\n\nFigure¬†2"
  },
  {
    "objectID": "blog/2024/08/draft.html#summary",
    "href": "blog/2024/08/draft.html#summary",
    "title": "Dynamic Capabilities: How Top Firms Adapt, Innovate, and Thrive in High-Tech Markets",
    "section": "Summary",
    "text": "Summary\nDynamic capabilities are the essential bridge between a firm‚Äôs existing resources and its long-term competitive success in volatile markets. By encompassing both high-level managerial capabilities (Teece, Pisano, and Shuen (1997)) and established organizational routines (Eisenhardt and Martin (2000)), dynamic capabilities provide the mechanism for companies to not just survive change, but to actively shape their future. The continuous process of Sensing, Seizing, and Transforming is the operational cycle that allows firms to convert market insights into valuable action, ensuring sustained abnormal returns. Mastering this capability is the hallmark of the most successful, resilient, and adaptive organizations."
  },
  {
    "objectID": "blog/2024/06/index.html",
    "href": "blog/2024/06/index.html",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "",
    "text": "Most change efforts fail not because people resist, but because leaders target the wrong unit of change the individual. As Kurt Lewin showed, transformation begins not with lone heroes, but with the social forces of the group. Understanding this shift from motivating individuals to mobilizing teams is the key to making change stick."
  },
  {
    "objectID": "blog/2024/06/index.html#the-lone-wolf-vs.-the-pack",
    "href": "blog/2024/06/index.html#the-lone-wolf-vs.-the-pack",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "",
    "text": "Most change efforts fail not because people resist, but because leaders target the wrong unit of change the individual. As Kurt Lewin showed, transformation begins not with lone heroes, but with the social forces of the group. Understanding this shift from motivating individuals to mobilizing teams is the key to making change stick."
  },
  {
    "objectID": "blog/2024/06/index.html#introduction",
    "href": "blog/2024/06/index.html#introduction",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "Introduction",
    "text": "Introduction\nChange is the only constant in today‚Äôs global business environment. Yet despite endless meetings, emails, and training sessions, many organizations still struggle to make change stick (Al-Haddad and Kotnour 2015). Leaders pour effort into motivating individuals, only to watch employees slip back into old habits.\nThe problem is not a lack of effort, it is a flaw in perspective. Too many change strategies focus on individuals, treating transformation as a personal mindset issue rather than a collective movement. It is the lone wolf approach to change: trying to inspire one person at a time while ignoring the social currents that shape behavior.\nAs social psychologist Kurt Lewin discovered decades ago, the real force behind sustainable change is not found in individual effort, it lies in the group dynamic, the pack. Understanding how groups form, interact, and influence one another can transform the way organizations lead change today.\nThis post revisits Lewin‚Äôs insights on group dynamics and explores how leaders can shift from managing people in isolation to mobilizing the collective energy of their teams."
  },
  {
    "objectID": "blog/2024/06/index.html#the-heart-of-the-matter-target-the-group-not-the-individual",
    "href": "blog/2024/06/index.html#the-heart-of-the-matter-target-the-group-not-the-individual",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "The Heart of the Matter: Target the Group, Not the Individual",
    "text": "The Heart of the Matter: Target the Group, Not the Individual\nAt the core of Kurt Lewin‚Äôs work is a groundbreaking argument: meaningful change doesn‚Äôt happen at the individual level, but at the group level. According to Lewin, group behaviour is shaped by a field of forces and symbolic interactions that dictate the group‚Äôs structure and, in turn, modify individual behaviour (Burnes 2007).\nWhy is the group the primary focus? Because an individual even one open to change is constantly under immense group pressure to conform to existing norms. These group routines and patterns are not just passive habits; they are actively valued and enforced by the group to maintain stability (Burnes 2007).\nThis aligns with modern findings, such as Kotter‚Äôs, that resistance is more likely to arise from the system and its ingrained norms than from isolated individuals (Al-Haddad and Kotnour 2015). Therefore, any effective change strategy must concentrate on influencing the team‚Äôs collective norms, roles, and values."
  },
  {
    "objectID": "blog/2024/06/index.html#the-prerequisite-creating-a-felt-need-for-change",
    "href": "blog/2024/06/index.html#the-prerequisite-creating-a-felt-need-for-change",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "The Prerequisite: Creating a ‚ÄúFelt Need‚Äù for Change",
    "text": "The Prerequisite: Creating a ‚ÄúFelt Need‚Äù for Change\nBefore any change can begin, a critical prerequisite must be met: there must be a felt need. This is the deep, internal realization, both at the individual and group levels that the current way of doing things is no longer sufficient and that change is necessary (Burnes 2004, 2007).\nIf the collective felt-need within a group or organization is low, introducing change becomes problematic. You cannot impose transformation on a group that does not perceive a problem. Lewin argued that successful change begins with engaging group members, helping them understand the why behind the change, and fostering a shared commitment to move forward.\nThis ‚Äúfelt need‚Äù is not merely emotional; it represents a psychological unfreezing of the group‚Äôs current norms and equilibrium. Without this shared readiness, even the best-designed change initiatives are likely to fail."
  },
  {
    "objectID": "blog/2024/06/index.html#the-method-action-research-and-collaborative-change",
    "href": "blog/2024/06/index.html#the-method-action-research-and-collaborative-change",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "The Method: Action Research and Collaborative Change",
    "text": "The Method: Action Research and Collaborative Change\nUnderstanding group behaviour is one thing; changing it is another. Lewin‚Äôs framework is not just theoretical it is profoundly practical. He proposed a participative method known as Action Research, a cyclical process for diagnosing and addressing organisational challenges (Burnes 2004, 2007).\nAction Research emphasises that effective change is achieved with people, not to people. It involves a series of iterative steps that make the change process collaborative and adaptive:\n\nAnalyze the situation: The group collaboratively diagnoses the problem, identifying underlying issues that influence behaviour.\n\nIdentify alternatives: Members discuss and explore possible solutions.\n\nChoose and act: The group selects and implements the most appropriate solution.\n\nReflect and repeat: Results are evaluated, and the process is refined based on collective learning.\n\nThis method reflects Lewin‚Äôs belief that behaviour can only be understood and modified within a group context. It embodies the principle of self-management, where groups play an active role in shaping their own behaviour (Burnes 2004). Change, therefore, becomes a participative and collaborative process ‚Äî one that transforms resistance into ownership."
  },
  {
    "objectID": "blog/2024/06/index.html#the-framework-lewins-3-step-model-for-group-transformation",
    "href": "blog/2024/06/index.html#the-framework-lewins-3-step-model-for-group-transformation",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "The Framework: Lewin‚Äôs 3-Step Model for Group Transformation",
    "text": "The Framework: Lewin‚Äôs 3-Step Model for Group Transformation\nWith the group engaged and a collaborative process in motion, Lewin proposed that organisational change unfolds through his well-known 3-Step Model (Burnes 2004):\n\nUnfreeze: This stage creates the felt need for change by disrupting existing norms, routines, and assumptions. It prepares the group to let go of old patterns and accept that change is necessary.\n\nChange (Move): Guided by the principles of Action Research, the group develops and adopts new behaviours, attitudes, and values. This is the implementation phase, where learning and experimentation take place.\n\nRefreeze: The final stage solidifies the new equilibrium. New group norms are embedded in organisational culture and supported by systems and shared values, ensuring the change endures.\n\nLewin believed that because individuals are inherently influenced by group norms and social forces, sustainable transformation must occur at the group level not solely at the level of individual behaviour (Al-Haddad and Kotnour 2015). As he noted, group routines and patterns have intrinsic value; they‚Äôre not merely the product of opposing forces but serve a positive function in maintaining group identity and cohesion (Burnes 2004, 2007)."
  },
  {
    "objectID": "blog/2024/06/index.html#a-modern-reflection-is-refreezing-still-relevant",
    "href": "blog/2024/06/index.html#a-modern-reflection-is-refreezing-still-relevant",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "A Modern Reflection: Is Refreezing Still Relevant?",
    "text": "A Modern Reflection: Is Refreezing Still Relevant?\nWhile Lewin‚Äôs model remains foundational, modern change scholars question whether the concept of ‚Äúrefreezing‚Äù fits today‚Äôs rapidly changing business environment. In an age of constant disruption and agile methodologies, achieving a stable equilibrium may seem unrealistic.\nContemporary perspectives suggest that organisations should cultivate a culture of continuous learning and adaptability a state of being ‚Äúpermanently unfrozen.‚Äù Nevertheless, Lewin‚Äôs insight into the power of group dynamics remains timeless. Whether or not we ‚Äúrefreeze,‚Äù successful change still depends on engaging the collective forces that shape how people think, feel, and act together."
  },
  {
    "objectID": "blog/2024/06/index.html#conclusion",
    "href": "blog/2024/06/index.html#conclusion",
    "title": "The Lone Wolf vs.¬†The Pack: Why Your Change Strategy Is Failing",
    "section": "Conclusion",
    "text": "Conclusion\nThe enduring genius of Kurt Lewin‚Äôs work is its simple, powerful truth: organizations don‚Äôt change, people do but people change most effectively in groups. Your next change initiative will succeed or fail based on your ability to look past individual behaviours and see the powerful, invisible currents of group norms, pressures, and values. To change the individual, you must first have the courage to engage, understand, and transform the group. So, is your organization empowering lone wolves ‚Äî or building a stronger pack?"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "Thanks for stopping by!\nI am a Lead Data Scientist at the Centre for Intelligent Power (CIP) at Eaton in Dublin, Ireland. My expertise is centered at the intersection of applied machine learning and strategic innovation management.\nBeyond my technical specialization, I function as a strategic AI leader and builder. With a proven background in both industry and academia, I specialize in developing reliable, production-ready AI solution, while fostering team growth by mentoring engineers and leading high-impact, cross-functional projects.\nMy research is dedicated to the strategic intersection of applied machine learning and innovation management. I develop and investigate machine learning solutions aimed at addressing crucial business and sustainability challenges. Complementary to this, I also study the strategic frameworks and management practices necessary for organizations to effectively manage technology and drive sustained success in dynamic market environments."
  },
  {
    "objectID": "about/index.html#current-initiatives",
    "href": "about/index.html#current-initiatives",
    "title": "Anthony Faustine",
    "section": "Current Initiatives",
    "text": "Current Initiatives\n\n\nBlog\n\n\n\n\n\n\n\n\n2025\n\n\n\n\n\nNo matching items\n\nSee all blog posts ‚Üí\n\n\nTalks\n\n\n\n\n\n\n\n\nTalks\n\n\n\n\n\nNo matching items\n\nSee all talks ‚Üí\n\n\nPublications\n\n\n\n\n\n\n\n\nJournal articles\n\n\n\n\n\nNo matching items\n\nSee all publications ‚Üí\n\n\n\n\n\n\n\n\nNoteCopyright\n\n\n\nUnless otherwise noted, all content is ¬© Anthony Faustine and licensed under the Creative Commons."
  },
  {
    "objectID": "blog/2024/07/index.html",
    "href": "blog/2024/07/index.html",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "",
    "text": "Ever been on a team where everything just clicks? communication is effortless, everyone knows their role, and the team anticipates each other‚Äôs moves, achieving goals with remarkable efficiency. This seamless collaboration is not a magic. It is often the result of something called a shared mental model, the invisible architecture that supports high-performance teamwork.\nThis post explores what shared mental models are, why they are a critical predictor of team effectiveness, and how leaders can consciously cultivate them, especially in the complex environments of virtual and hybrid teams."
  },
  {
    "objectID": "blog/2024/07/index.html#introduction",
    "href": "blog/2024/07/index.html#introduction",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "",
    "text": "Ever been on a team where everything just clicks? communication is effortless, everyone knows their role, and the team anticipates each other‚Äôs moves, achieving goals with remarkable efficiency. This seamless collaboration is not a magic. It is often the result of something called a shared mental model, the invisible architecture that supports high-performance teamwork.\nThis post explores what shared mental models are, why they are a critical predictor of team effectiveness, and how leaders can consciously cultivate them, especially in the complex environments of virtual and hybrid teams."
  },
  {
    "objectID": "blog/2024/07/index.html#what-is-a-shared-mental-model",
    "href": "blog/2024/07/index.html#what-is-a-shared-mental-model",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "What is a Shared Mental Model?",
    "text": "What is a Shared Mental Model?\nTeam effectiveness and coordination are critical aspects of a high-performance team. Team members who share a similar and organized understanding of team tasks and goals ‚Äî and who understand each other‚Äôs working environments ‚Äî are more likely to perform well.\nThis shared understanding and knowledge about the mission, goals, and other relevant environments among team members are called mental models. The shared mental model is one of the most frequently used concepts in team cognition (Schelble et al. 2022). (2022) further identify the shared mental model as a critical predictor of team effectiveness.\nLungeanu, DeChurch, and Contractor (2022) define shared mental models as team properties reflecting how team members organize knowledge and understanding about the team‚Äôs purpose, the nature of the work, and how they work together. Thus, team mental models are a collective mental representation among team members of how they interact in performing task-work (Larson and DeChurch 2020).\nThey represent the organized mental representations of the various component pieces relevant to a team‚Äôs overall task (Schelble et al. 2022). As (Schelble et al. 2022) point out, shared mental models measure whether or not team members share a common understanding of their shared tasks, roles, interdependencies, and strategies.\nSchelble et al. (2022) break shared mental models into two types:\n\nTask mental model ‚Äî covers aspects specific to understanding and completing a shared task.\n\nTeam mental model ‚Äî focuses on factors related to cooperation and communication within a team."
  },
  {
    "objectID": "blog/2024/07/index.html#the-impact-on-team-success",
    "href": "blog/2024/07/index.html#the-impact-on-team-success",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "The Impact on Team Success",
    "text": "The Impact on Team Success\nShared mental models can continually develop over time, becoming more effective and influencing various team outcomes, such as objective performance, team viability, member well-being, and strategic alignment (Lungeanu, DeChurch, and Contractor 2022).\nTeams with shared mental models can recognize one another‚Äôs needs and information requirements (Lungeanu, DeChurch, and Contractor 2022; Schelble et al. 2022), which enhances coordination and mutual support.\nWhile this may be more intuitive in physical teams, virtual teams ‚Äî now an integral part of modern work ‚Äî require special attention in developing and maintaining shared mental models among members.\nUnlike face-to-face teams, creating and sustaining mental models is harder in virtual environments. Leaders must therefore compensate for challenges such as communication barriers and cultural differences. These issues can impact relationship building, which is essential for developing and sustaining shared mental models.\nAs underlined in (Larson and DeChurch 2020), face-to-face teams tend to have stronger shared mental models than virtual ones.\nTo improve team effectiveness and performance in virtual settings, leaders should aim to create a conducive environment for shared mental models. This can be achieved by:\n\nCultivating high-quality, interpersonal communication.\n\nCreating psychological safety.\n\nAdopting a leadership style that aligns well with virtual collaboration (Larson and DeChurch 2020)."
  },
  {
    "objectID": "blog/2024/07/index.html#leaderships-role-in-building-a-shared-mind",
    "href": "blog/2024/07/index.html#leaderships-role-in-building-a-shared-mind",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "Leadership‚Äôs Role in Building a Shared Mind",
    "text": "Leadership‚Äôs Role in Building a Shared Mind\nLungeanu, DeChurch, and Contractor (2022) highlight that leadership particularly shared leadership plays a crucial role in creating and shaping shared mental models in teams. This applies to both face-to-face and virtual teams.\nFor instance, (Lungeanu, DeChurch, and Contractor 2022) note that when leadership responsibilities are shared among members, the team tends to show greater commitment and information sharing. This dynamic fosters trust and enhances performance.\nTeams that embrace shared leadership and have diverse skills, experiences, and perspectives are more likely to develop and maintain strong shared mental models.\nFurthermore, connected leadership as opposed to fragmented leadership offers several advantages for improving similarity in team mental models. It promotes accuracy, synchronization of effort, and cohesion or trust.\nFor example, (Lungeanu, DeChurch, and Contractor 2022) argue that hierarchical and coordinated leadership are better at promoting shared mental models than factional or isolated forms of leadership. They also emphasize that boundaries among members of shared leadership groups are permeable, allowing reciprocal leadership processes that reduce conflict and tension."
  },
  {
    "objectID": "blog/2024/07/index.html#conclusion",
    "href": "blog/2024/07/index.html#conclusion",
    "title": "Shared Mental Models: The Invisible Architecture of High-Performance Teams",
    "section": "Conclusion",
    "text": "Conclusion\nUltimately, a shared mental model is not just a nice-to-have; it‚Äôs the cognitive foundation upon which effective teams are built. It serves as the shared ‚Äúmap‚Äù that enables a group of individuals to navigate complex tasks together with clarity and confidence.\nWhile the rise of virtual work presents new challenges, the core principle remains: effective leadership is the catalyst. By fostering open communication, psychological safety, and a connected leadership structure, leaders can intentionally design the conditions for these powerful shared understandings to emerge and thrive."
  },
  {
    "objectID": "blog/2024/08/index.html",
    "href": "blog/2024/08/index.html",
    "title": "Dynamic Capabilities: Why Reconfiguring Resources is Your Only Path to Long-Term Success (Part 1/3)",
    "section": "",
    "text": "Introduction\nCompetition in high-technology industries worldwide has shown that accumulating valuable technologies, often protected by aggressive intellectual property strategies, is not sufficient for success (Teece, Pisano, and Shuen 1997). Instead, the most successful firms such as Microsoft, Tesla, Intel to name a few, have developed specific capabilities, including the ability to rapidly and flexibly innovate products and the management skills to coordinate and redeploy market positions and explore new opportunities effectively (Teece, Pisano, and Shuen 1997).\nAs stated by Witschel et al. (2019), only companies with unique abilities can adjust their resources to a constantly changing environment and effectively utilise the opportunities that arise from these changes. These capabilities are developed through the organisation‚Äôs learning process and cannot be obtained from the external environment (Witschel et al. 2019; Matysiak, Rugman, and Bausch 2018). The author further stress that difficult-to-replicate capabilities are essential to sustain long-term competitiveness. These capabilities and timely responsiveness are crucial to achieving competitive advantage in these industries and are referred to as dynamic capabilities.\n\n\nDynamic Capabilities: The Engine of Strategic Renewal\nThe concept of Dynamic Capabilities is central to understanding how firms not just survive, but thrive, in volatile, fast-paced markets. Simply possessing valuable resources isn‚Äôt enough; success depends on the ability to change those resources when the environment demands it.\nThe foundational definition by Teece, Pisano, and Shuen (1997) establishes dynamic capabilities as the firm‚Äôs ability to integrate, build, and reconfigure internal and external competencies to address rapidly changing environments. This capability enables a firm to develop and acquire new resources and assets and reconfigure them as needed to innovate and stay competitive (Teece 2014). Crucially, there is academic agreement that this capability acts as an internal mechanism for deliberately altering a company‚Äôs current set of resources and aligning them with new strategic objectives, ultimately resulting in a fresh combination or arrangement of organizational assets (Zahra, Petricevic, and Luo 2022; Witschel et al. 2019).\nExpanding on Teece‚Äôs perspective, subsequent definitions emphasize the continuous and global nature of this process. For instance, Zahra, Petricevic, and Luo (2022) refines the concept as the firm‚Äôs ability to effectively and continuously build, bundle, mobilise, integrate, reconfigure, upgrade and protect critical resources to address rapidly changing environments in geographically dispersed markets. Similarly, Witschel et al. (2019) views dynamic capabilities as reflecting an organisation‚Äôs ability to create new and innovative forms of competitive advantage, rooted in its history and market position (Witschel et al. 2019).\nThese capabilities are also fundamentally tied to problem-solving and environmental awareness. Barreto (2010) define them as a firm‚Äôs potential to systematically solve problems, formed by its ability to sense opportunities and threats, make timely and market-oriented decisions, and change its resource base (Barreto 2010; Schwarz, Rohrbeck, and Wach 2020). This highlights the necessity of being proactive, which Wilden et al. (2013) further specifies: the ability to sense changes requires a firm to closely observe customers, suppliers, R&D partners, and changes in technology, suggesting that anticipatory processes and tools play an important role in dynamic capabilities (Schwarz, Rohrbeck, and Wach 2020; Wilden et al. 2013).\nIn essence, while the specific wording varies, all these definitions converge on dynamic capabilities being the strategic, continuous, and difficult-to-replicate ability of a firm to adapt its internal structure and resources in anticipation of or in response to external shifts, securing long-term competitiveness.\n\n\nConclusion\nThe consensus is clear: Dynamic Capabilities are a strategic necessity. They represent the essential managerial ability to ensure a firm‚Äôs assets remain relevant in a complex, fast-moving world. But how does a firm actually build and execute this continuous transformation?\nIn the next post, we will explore the two dominant schools of thought that explain the difference between high-level managerial vision and the concrete processes required to turn vision into reality: the Capabilities vs.¬†Routines debate.\n\n\n\n\n\nReferences\n\nBarreto, Ilƒ±ÃÅdio. 2010. ‚ÄúDynamic Capabilities: A Review of Past Research and an Agenda for the Future.‚Äù Journal of Management 36 (1): 256‚Äì80.\n\n\nMatysiak, Lars, Alan M Rugman, and Andreas Bausch. 2018. ‚ÄúDynamic Capabilities of Multinational Enterprises: The Dominant Logics Behind Sensing, Seizing, and Transforming Matter!‚Äù Management International Review 58 (2): 225‚Äì50.\n\n\nSchwarz, Jan Oliver, Ren√© Rohrbeck, and Bernhard Wach. 2020. ‚ÄúCorporate Foresight as a Microfoundation of Dynamic Capabilities.‚Äù Futures & Foresight Science 2 (2): e28.\n\n\nTeece, David J. 2014. ‚ÄúA Dynamic Capabilities-Based Entrepreneurial Theory of the Multinational Enterprise.‚Äù Journal of International Business Studies 45 (1): 8‚Äì37.\n\n\nTeece, David J, Gary Pisano, and Amy Shuen. 1997. ‚ÄúDynamic Capabilities and Strategic Management.‚Äù Strategic Management Journal 18 (7): 509‚Äì33.\n\n\nWilden, Ralf, Siegfried P Gudergan, Bo Bernhard Nielsen, and Ian Lings. 2013. ‚ÄúDynamic Capabilities and Performance: Strategy, Structure and Environment.‚Äù Long Range Planning 46 (1-2): 72‚Äì96.\n\n\nWitschel, Daliborka, Aaron D√∂hla, Maximilian Kaiser, Kai-Ingo Voigt, and Thilo Pfletschinger. 2019. ‚ÄúRiding on the Wave of Digitization: Insights How and Under What Settings Dynamic Capabilities Facilitate Digital-Driven Business Model Change.‚Äù Journal of Business Economics 89 (8): 1023‚Äì95.\n\n\nZahra, Shaker A., Olga Petricevic, and Yadong Luo. 2022. ‚ÄúToward an Action-Based View of Dynamic Capabilities for International Business.‚Äù Journal of International Business Studies 53 (4): 583‚Äì600. https://doi.org/10.1057/s41267-021-00487-2.\n\nCitationFor attribution, please cite this work as:\nFaustine, Anthony. 2024. ‚ÄúDynamic Capabilities: Why Reconfiguring\nResources Is Your Only Path to Long-Term Success (Part 1/3).‚Äù\nAugust 1, 2024. https://sambaiga.github.io/blog/2024/08/."
  },
  {
    "objectID": "blog/2025/01/draft.html",
    "href": "blog/2025/01/draft.html",
    "title": "A Year of Growth: 13 Books That Transformed My Perspective in 2024",
    "section": "",
    "text": "At the beginning of 2024, I set a personal goal: to read one book each month. This was fueled by my desire to keep learning, expand my thinking, and deepen my self-awareness.\nThe journey had its highs and lows. Some months, I breezed through stories that gripped me from the start. Other times, staying on track demanded discipline and focus, as distractions tested my resolve. Yet, I remained committed, motivated by the belief that each book brought me closer to growth and self-discovery.\nBy the end of the year, I had completed 13 books; a testament to the power of persistence and the commitment I made at the start of this journey. Each book left its mark, shaping my thinking and broadening my perspective in unexpected ways. In this post, I‚Äôll share my reading list and reflect on how these stories and ideas enriched my year. I invite you to join me in exploring the lessons learned and the insights gained from this rewarding experience."
  },
  {
    "objectID": "blog/2025/01/draft.html#introduction",
    "href": "blog/2025/01/draft.html#introduction",
    "title": "A Year of Growth: 13 Books That Transformed My Perspective in 2024",
    "section": "",
    "text": "At the beginning of 2024, I set a personal goal: to read one book each month. This was fueled by my desire to keep learning, expand my thinking, and deepen my self-awareness.\nThe journey had its highs and lows. Some months, I breezed through stories that gripped me from the start. Other times, staying on track demanded discipline and focus, as distractions tested my resolve. Yet, I remained committed, motivated by the belief that each book brought me closer to growth and self-discovery.\nBy the end of the year, I had completed 13 books; a testament to the power of persistence and the commitment I made at the start of this journey. Each book left its mark, shaping my thinking and broadening my perspective in unexpected ways. In this post, I‚Äôll share my reading list and reflect on how these stories and ideas enriched my year. I invite you to join me in exploring the lessons learned and the insights gained from this rewarding experience."
  },
  {
    "objectID": "blog/2025/01/draft.html#book-covered",
    "href": "blog/2025/01/draft.html#book-covered",
    "title": "A Year of Growth: 13 Books That Transformed My Perspective in 2024",
    "section": "Book covered",
    "text": "Book covered\n\nBuild by Tony Fadell\nIn Build , Tony Fadell offers an unconventional guide to product development, entrepreneurship, and leadership, drawing on his extensive experience in Silicon Valley as the creator of the iPod and founder of Nest. The book serves as a practical and insightful roadmap for innovators, designers, and product managers, making it a valuable resource for anyone looking to navigate the complexities of bringing ideas to life.\nFadell emphasizes the importance of purpose-driven product development, advocating for solutions that genuinely improve users‚Äô lives. This focus on purpose resonates with me, as it highlights the need to create products that not only meet market demands but also enhance the quality of life for users. He encourages readers to embrace continuous learning, urging us to remain curious and glean insights from both successes and failures. This mindset is crucial in a rapidly changing environment where adaptability is key.\nAnother key theme in the book is the value of simplicity in design. Fadell urges creators to eliminate unnecessary features and focus on delivering core value. This principle has inspired me to evaluate my own projects and consider how I can streamline processes to enhance user experience without overwhelming them with complexity.\nAdditionally, Fadell champions a leadership style rooted in empathy, fostering a safe environment for experimentation and supporting team growth. This approach is refreshing and underscores the importance of building strong relationships within teams to encourage innovation.\nThrough candid anecdotes, mistakes, and hard-won lessons from his journeys at Apple and Nest, Fadell provides an authentic and relatable guide for aspiring entrepreneurs, product creators, and leaders. Fadell‚Äôs insights have motivated me to approach my own projects with a renewed focus on purpose, simplicity, and empathy, ultimately leading to more impactful and meaningful outcomes.\nKey lesson: successful product development requires a deep understanding of user needs, a relentless pursuit of simplicity, a culture of continuous learning, and a leadership style that fosters creativity and empowers team.\n\n\nEssentialism by Greg McKeown\nIn Essentialism , Greg McKeown presents a disciplined approach to identifying and prioritizing what truly matters in our lives. The book serves as both a wake-up call and a roadmap for individuals like me overwhelmed by competing demands and distractions. McKeown emphasizes the transformative power of less but better encouraging readers to focus on their highest point of contribution.\nKey Lesson: Clarity about priorities opens the door to greater freedom and fulfillment. Life is filled with trade-offs, and the path to true success lies in making intentional, wise choices that align with what truly matters.\nBy systematically eliminating non-essential tasks and distractions, McKeown guides readers in protecting their most valuable resources‚Äîtime, energy, attention, and well-being. This essentialist mindset empowers individuals to make intentional choices and concentrate on activities that genuinely enhance their lives, leading to greater fulfillment and success. I find this approach liberating, as it encourages me to focus on what brings the most value rather than getting caught up in the noise of everyday life.\nThe book also highlights the importance of saying no to non essential commitments, which can be challenging but ultimately leads to a more fulfilling and productive life. Embracing this mindset has inspired me to evaluate my commitments and ensure they align with my core values and long-term goals.\n\n\nThe Magic of Thinking Big by David Schwartz\nThe Magic of Thinking Big  by David J. Schwartz is a timeless reminder that success isn‚Äôt solely reserved for the most gifted. It‚Äôs a testament to the power of mindset. Those who dare to dream big, believe in themselves, and take intentional action are most likely to achieve their goals. This book profoundly impacted my thinking, project me to expand my mindset, overcome limiting beliefs, and cultivate habits that align with my aspirations for greatness.\nFurthermore, the book emphasizes the importance of treating others with respect, learning from setbacks, and embracing continuous learning as essential for a successful and fulfilling life. This resonates with me, as it highlights that growth often comes from our experiences, both positive and negative.\nKey lessons: Success is primarily a product of mindset ‚Äì believing in yourself, cultivating a positive attitude, and taking consistent action towards your goals, regardless of inherent talent or luck.\n\n\nSame As Ever by Morgan Housel\nMorgan Housel‚Äôs Same As Ever  provides a compelling framework for understanding the enduring nature of human behavior. Despite the rapid pace of change in our world, Housel argues that our core emotions, desires, and behaviors remain remarkably consistent.\nHousel emphasizes the concept of rational optimism, which involves recognizing both the potential for growth and the inevitability of challenges. This balanced perspective encourages us to adhere to core principles like integrity and patience while also acknowledging the inherent strengths and weaknesses of human nature. I find this approach refreshing, as it promotes a realistic yet hopeful outlook on life.\nThe book explores how fundamental human emotions‚Äîsuch as fear, greed, and hope continue to drive our actions, regardless of the technological and social changes surrounding us. This idea serves as a reminder that while the world may evolve, the essence of what it means to be human remains constant. Housel‚Äôs insights encourage us to focus on these timeless aspects of human behavior, which can guide our decisions and interactions in an ever-changing landscape.\nBy embracing the enduring truths of human nature, we can navigate the complexities of modern life with greater wisdom and resilience. I appreciate Housel‚Äôs ability to distill these insights into practical guidance, making this book a valuable resource for anyone seeking to foster a balanced and optimistic approach to life.\nKey lesson understanding the enduring nature of human behavior, while embracing rational optimism and core principles is essential for navigating life‚Äôs complexities and fostering meaningful relationships.\n\n\nNever Split the Difference by Chris Voss\nNever Split the Difference  by Chris Voss, a former FBI hostage negotiator, offers unique insights into the art of negotiation that have completely shifted my perspective on the subject. This book illustrates how powerful negotiation can be, not just in high-stakes situations but also in everyday life.\nVoss challenges traditional negotiation strategies that often focus on compromise. Instead, he advocates for empathy, understanding, and subtle psychological tactics to achieve optimal outcomes. This approach has taught me that negotiation is less about winning and more about fostering collaboration and curiosity. By prioritizing these elements, I can engage in conversations that lead to more meaningful and productive results.\nThe book provides practical tools that I can apply in various contexts, whether in professional settings or personal interactions. Voss emphasizes the importance of active listening and emotional intelligence, which are crucial for understanding the needs and motivations of others. This has encouraged me to approach negotiations with a mindset of collaboration rather than confrontation, ultimately leading to better relationships and outcomes.\nThrough engaging anecdotes and real-life examples, Voss illustrates how these techniques can be effectively implemented. His insights have inspired me to rethink my approach to discussions and negotiations, making me more aware of the dynamics at play.\nKey Lesson: True influence comes from understanding others, not overpowering them.\n\n\nThink Again by Adam Grant\nIn Think Again,  Adam Grant profoundly challenges our assumptions about knowledge and belief, urging readers to cultivate a mindset of intellectual humility and curiosity. He defines intellectual humility as recognizing the limitations of our own knowledge and being willing to learn from everyone. This approach fosters open inquiry and leads to better decision-making, which I find incredibly valuable in both personal and professional contexts.\nGrant encourages us to approach life with curiosity and openness, advocating for the importance of questioning our beliefs, considering new evidence, testing our assumptions, and holding our ideas loosely. This flexibility allows us to adapt when presented with new information. I particularly resonate with his assertion that being wrong is not a failure; rather, it is an opportunity for learning. When we discover we‚Äôre mistaken, we are simply becoming less wrong‚Äîa positive step in personal growth.\nFurthermore, Grant highlights the significance of separating our identity from our ideas. This separation enables more objective evaluation and reduces defensive reactions when our beliefs are challenged. He introduces the concept of a challenge network a group of trusted individuals who provide honest feedback and help us reconsider our perspectives. This idea has inspired me to seek out such a network in my own life, as it can foster deeper understanding and growth.\nGrant‚Äôs approach to learning and growth resonates deeply with me, as it emphasizes not merely changing one‚Äôs mind but creating a life where learning and evolving take precedence over the need to always be right. This shift in perspective is crucial for personal growth, fostering innovation, and cultivating meaningful relationships.\nOverall, Think Again serves as a powerful reminder of the importance of intellectual humility and the continuous journey of self-improvement. I look forward to applying these principles as I navigate my own beliefs and interactions with others.\nKey Lesson: Growth comes from being willing to admit what you don‚Äôt know and learning from others.\n\n\nMan‚Äôs Search for Meaning by Viktor E. Frankl\nThis profound memoir and philosophical exploration  by Viktor Frankl delves into the human capacity to find meaning and purpose even in the face of unimaginable suffering. Drawing upon his own harrowing experiences as a prisoner in Nazi concentration camps, Frankl presents a compelling argument for the human will to survive and the enduring power of the human spirit. His reflections on resilience and purpose have encouraged me to view suffering as an opportunity for growth.\nFrankl‚Äôs central thesis is that life has meaning under all circumstances, even the most miserable ones. He asserts that our main drive is not pleasure, as Freud suggested, but rather the pursuit of meaning. This will to meaning serves as a fundamental motivation for human behavior, which resonates deeply with me. It has shifted my perspective on challenges, prompting me to seek out the lessons and insights that can emerge from difficult experiences.\nThroughout the book, Frankl shares personal anecdotes and observations that illustrate how individuals can choose their attitudes and responses, even in the direst situations. This idea of choosing one‚Äôs response is a powerful reminder of our inherent strength and agency, reinforcing the notion that we can find purpose in our suffering.\nMoreover, Frankl emphasizes the importance of having a clear sense of purpose, which can guide us through life‚Äôs adversities. His insights have inspired me to reflect on my own values and aspirations, encouraging me to cultivate a mindset that prioritizes meaning and fulfillment over mere pleasure or comfort.\nOverall, Man‚Äôs Search for Meaning is a transformative read that offers profound insights into the human experience. Frankl‚Äôs ability to articulate the significance of meaning in our lives has not only enriched my understanding of resilience but also motivated me to embrace challenges as opportunities for personal growth and deeper understanding.\nKey Lesson: When you have a ‚Äúwhy‚Äù to live, you can bear almost any ‚Äúhow.‚Äù This powerful insight underscores the importance of having a strong sense of purpose, which can provide the strength needed to endure the most challenging circumstances.\n\n\nThe Anxious Generation by Jonathan Haidt\nIn The Anxious Generation , Jonathan Haidt presents a deeply researched and thought-provoking examination of the alarming rise in anxiety and depression among young people, particularly Generation Z. His compelling evidence indicates a significant increase in mood disorders among adolescents since the early 2010s, which he links to the widespread adoption of smartphones and social media.\nHaidt identifies several key factors contributing to this mental health crisis. First, he discusses excessive social media use, citing studies that reveal a correlation between heavy usage, around 3 to 4 hours daily and increased levels of depression and anxiety, especially among girls. This insight has made me reflect on how pervasive social media is in the lives of young people today and the potential consequences it carries.\nAnother critical aspect of Haidt‚Äôs argument is his critique of overprotective parenting. He highlights the cultural shift towards safetyism which he believes deprives children of essential experiences needed to develop resilience and problem-solving skills. This perspective challenges me to consider how our parenting practices may inadvertently hinder the growth of independence and confidence in young people.\nHaidt also contrasts the current phone based childhood with the previous play based childhood, arguing that this transition has disrupted children‚Äôs social interactions and neurological development. This comparison resonates with me, as it underscores the importance of unstructured play and face-to-face interactions in fostering healthy development.\nOverall, The Anxious Generation serves as a crucial resource for understanding the mental health challenges facing today‚Äôs youth. Haidt‚Äôs analysis not only sheds light on the factors contributing to this crisis but also calls for a reevaluation of how we approach parenting and societal expectations in order to foster a healthier, more resilient generation. His exploration of these issues has prompted me to think critically about how we can better support young people in navigating the complexities of modern life.\nKey Lesson: Overemphasis on safety and protection can hinder young people‚Äôs resilience and ability to cope with adversity, ultimately making them less prepared to face life‚Äôs challenges.\n\n\nThe Psychology of Money by Morgan Housel\nThis book  reshaped the way I think about money, wealth, and financial freedom. Unlike traditional personal finance books that focus on numbers, budgets, or technical strategies, Housel takes a deeply human approach, exploring how our emotions, habits, and perspectives shape our financial decisions.\nHousel‚Äôs insights helped me understand that money isn‚Äôt just about math, it‚Äôs about behavior. The way we think about money is influenced by our upbringing, life experiences, and psychological biases, which often leads to decisions that don‚Äôt align with logic or long-term goals. This perspective has encouraged me to reflect on my own financial habits and the underlying motivations behind them.\nOne of the key takeaways from the book is that the biggest value of money isn‚Äôt to buy luxury goods but to gain control over your time and life the ultimate form of freedom. This idea has profoundly impacted my view of financial success, shifting my focus from accumulating wealth for its own sake to using it as a tool for achieving a more fulfilling and independent life.\nHousel also shares engaging stories that illustrate the strange ways people think about money, making complex concepts accessible and relatable. His emphasis on the importance of patience and long-term thinking has inspired me to adopt a more measured approach to my financial decisions, recognizing that true wealth often comes from consistent, thoughtful actions over time.\nKey lesson: Money is not about what you earn or spend‚Äîit‚Äôs about the freedom to live life on your terms. true wealth isn‚Äôt about flashy cars, expensive vacations, or big houses‚Äîit‚Äôs about what you don‚Äôt see: financial freedom, security, and the ability to make choices without stress.\n\n\nThe Forty Rules of Love by Elif Shafak\nElif Shafak‚Äôs  The Forty Rules of Love beautifully intertwines themes of love, spirituality, and self-discovery. Through the parallel stories of Ella Rubenstein, a contemporary housewife, and the profound spiritual connection between Rumi and Shams of Tabriz, the novel explores the transformative power of love in its various forms; divine, romantic, and platonic.\nShafak emphasizes that true love requires vulnerability, embracing imperfection, and a willingness to let go of control. This perspective has profoundly impacted my understanding of relationships, highlighting that love is not just about connection but also about personal growth and acceptance.\nThe journey of love, as depicted in the novel, is deeply intertwined with spirituality, suggesting that it is a path of surrender, trust, and faith. Furthermore, the novel acknowledges the duality of pain and joy inherent in love, recognizing that pain can be a catalyst for growth and transformation. This insight has encouraged me to embrace the complexities of my own experiences, understanding that challenges can lead to deeper connections and greater self-awareness.\nUltimately, The Forty Rules of Love celebrates the power of stories to heal, connect, and inspire us to live authentically. Shafak‚Äôs narrative encourages readers to embrace their unique paths and find meaning in the human experience. The book has inspired me to reflect on my own relationships and the ways in which love can shape our lives, urging me to approach love with an open heart and a willingness to learn.\nKey Lesson: Love, in all its forms, is a journey of transformation that requires vulnerability and courage.\n\n\nThe Man in the Mirror by Patrick Morley\nThe 25th Anniversary Edition of The Man in the Mirror  by Patrick Morley is a timeless guide that continues to resonate with men seeking purpose and integrity. Even decades later, Morley‚Äôs wisdom remains remarkably relevant, as he has updated the content with fresh insights to address the unique challenges of today‚Äôs world.\nThrough its exploration of 24 key challenges, Morley provides a practical, faith-based framework for navigating issues such as career pressures, spiritual emptiness, broken relationships, and misplaced priorities. This approach has prompted me to reflect on my own life and the various obstacles I face.\nOne of the most powerful messages in The Man in the Mirror is that true success is not about what you achieve, but who you become and how you serve others. This perspective has shifted my understanding of success, urging me to confront my values, goals, and character. Morley also highlights the distractions of modern life, including technology and shifting cultural values, which can easily divert our focus from what truly matters.\nAdditionally, the book encourages men to consider their legacy. Morley emphasizes the importance of leaving a positive impact through service, love, and faith, compelling me to think about how my actions today will resonate in the future.\nOverall, The Man in the Mirror serves as an invaluable resource for any man seeking to cultivate a meaningful life rooted in integrity and purpose. I appreciate the insights Morley offers, and I look forward to applying these principles as I strive to make a lasting, positive difference in my own life and the lives of others.\n\n\nThe 21 Irrefutable Laws of Leadership by John Maxwell\nReading John C. Maxwell‚Äôs  The 21 Irrefutable Laws of Leadership has been one of the most practical and insightful experiences of my year. This timeless guide is incredibly applicable for anyone looking to enhance their leadership skills, whether in the workplace, community, or personal life, as it distills decades of experience into 21 core principles.\nWhat resonated with me the most is Maxwell‚Äôs emphasis that true leadership is about influence, not authority. He highlights the importance of creating genuine connections and inspiring others to follow, which I believe is crucial in any leadership role. A significant aspect of his approach is the concept of servant leadership, which prioritizes the needs of the team. By acknowledging their importance, supporting individual growth, and fostering an empowering environment, leaders can cultivate a more engaged and motivated team.\nMaxwell also stresses that trust is the cornerstone of effective leadership. He argues that trust is built on personal character, consistent values, honesty, and the courage to apologize and make amends when necessary. This insight has made me reflect on my own leadership practices and the importance of being trustworthy in all my interactions.\nFurthermore, I appreciate Maxwell‚Äôs perspective that leadership is a continuous journey of personal development. His assertion that we must commit to lifelong learning and adaptation is a reminder that there is always room for growth, no matter how experienced we may become.\nFinally, Maxwell encourages leaders to focus on their long-term impact. This includes developing other leaders, creating lasting positive change, and leading with authenticity and ethical principles. This holistic approach to leadership inspires me to think beyond immediate results and consider how my actions can contribute to a positive legacy.\nKey lesson: Leadership is not about titles‚Äîit‚Äôs about service and influence.\n\n\nDigital Minimalism: Choosing a Focused Life in a Noisy World by Cal Newport\nIn our hyper-connected world, overflowing with notifications and constant digital distractions, Cal Newport‚Äôs Digital Minimalism offers a refreshing perspective. Rather than demonizing technology, Newport encourages us to cultivate a more intentional relationship with it. This approach has prompted me to rethink how I interact with digital tools in my daily life.\nNewport challenges us to critically examine our technology usage, prioritizing tools that align with our values and eliminating those that detract from our well-being. This idea resonates deeply with me, as it highlights the importance of being mindful about the technology we choose to engage with. His insights have inspired me to assess the impact of various digital platforms on my life, leading to a more discerning approach to my screen time.\nThe book provides practical strategies for implementing a digital minimalist lifestyle, such as engaging in digital decluttering and establishing technology-free time. These strategies have encouraged me to set boundaries around my usage, helping me reclaim time for more meaningful activities and connections.\nMoreover, Newport emphasizes the significance of real-world interactions and the value of focusing on activities that truly enrich our lives. This perspective has motivated me to seek out experiences that foster deeper connections with others and align more closely with my personal values.\nNewport‚Äôs emphasis on intentionality has not only reshaped my view of digital tools but also empowered me to take control of my digital habits, fostering a healthier and more fulfilling relationship with technology.\nKey lesson: technology should serve us, not the other way around. By intentionally choosing how we engage with technology and setting clear boundaries, we can reclaim our time, focus, and ultimately, our sense of purpose."
  },
  {
    "objectID": "blog/2025/01/draft.html#final-thoughts",
    "href": "blog/2025/01/draft.html#final-thoughts",
    "title": "A Year of Growth: 13 Books That Transformed My Perspective in 2024",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis year‚Äôs reading journey has been a transformative experience, filled with diverse perspectives that weave together a powerful narrative about living a meaningful life. Each book offered unique insights, prompting me to re-evaluate my priorities, cultivate resilience, and embrace the power of love in all its forms.\nKey themes emerged throughout my reading, particularly the importance of purpose and intentionality. Books like Build by Tony Fadell and Essentialism by Greg McKeown emphasized the significance of starting with ‚Äúwhy‚Äù and focusing on what truly matters. Fadell‚Äôs exploration of product development highlighted the necessity of identifying and solving the right problems, while McKeown‚Äôs mantra of less but better encouraged me to prioritize ruthlessly and protect my most valuable resources. Digital Minimalism further reinforced the need for intentional living in our digital age.\nSeveral books this year profoundly influenced my thinking, particularly those that emphasized the transformative power of mindset. The Magic of Thinking Big by David J. Schwartz inspired me to believe in myself and embrace a growth mindset, while Think Again by Adam Grant challenged my assumptions and encouraged me to embrace the discomfort of uncertainty. Grant convincingly argues that being wrong isn‚Äôt a failure, but rather an opportunity for learning. When we discover our initial beliefs are incorrect, we‚Äôre simply becoming less wrong, a crucial step in personal growth. He emphasizes the importance of intellectual humility, acknowledging the limitations of our own knowledge and being open to learning from diverse perspectives. This approach fosters open inquiry and leads to better decisionmaking. Both books highlighted the importance of continuous learning and adaptation as essential for personal and professional growth\nResilience emerged as another key theme. Same As Ever by Morgan Housel offered valuable insights into the enduring nature of human behavior, reminding me that while the world changes, core human principles remain constant. The Psychology of Money further emphasized the importance of understanding our emotions and biases in navigating life‚Äôs challenges. Jonathan Haidt‚Äôs The Anxious Generation highlighted the necessity of fostering resilience, independence, and critical thinking in today‚Äôs world. This message resonated deeply with Viktor Frankl‚Äôs Man‚Äôs Search for Meaning, which serves as a powerful testament to the human spirit‚Äôs ability to find meaning and purpose even in the face of unimaginable suffering.\nThe importance of human connection was evident throughout my reading. Never Split the Difference by Chris Voss emphasized the power of empathy and understanding in communication, while The 21 Irrefutable Laws of Leadership by John C. Maxwell highlighted the significance of service and influence in building meaningful relationships.\nFinally, Elif Shafak‚Äôs The Forty Rules of Love beautifully explored the transformative power of love in all its forms, reminding me that love requires vulnerability, courage, and a willingness to embrace imperfection.\nThese books have left a lasting impression, shaping how I think, act, and relate to others. They have reminded me to focus on building a life of purpose, cultivating resilience, and embracing the power of human connection."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vit√¶",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "now/index.html",
    "href": "now/index.html",
    "title": "What I‚Äôm doing now",
    "section": "",
    "text": "As of April 27, 2025, I‚Äôm spending all my time on these things:\n\nStaying at home pretty much 24/7 (STILL) because of the COVID-19 pandemic\nRaising 6 kids (17.5, 15, 12.5, 9.5, 7.5, and 3) and trying to stay sane (family blog)\nLiving in Atlanta and working as an assistant professor in the Department of Public Management and Policy at the Andrew Young School of Policy Studies at Georgia State University\n\nTeaching data visualization, program evaluation, comparative public administration, nonprofit management, and microeconomics at the Andrew Young School of Policy Studies at Georgia State University\n\nWorking as a part time data science mentor for Posit Academy\n\nConverting my dissertation into multiple articles and sending them out to journals + continuing my research on authoritarianism and international NGOs\n\nWorking on several articles on NGO restrictions with Suparna Chaudhry and Marc Dotson\n\nReading some sort of religiously themed text every day (books)"
  },
  {
    "objectID": "research/articles/deepnilmtk-2024/index.html",
    "href": "research/articles/deepnilmtk-2024/index.html",
    "title": "Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines",
    "section": "",
    "text": "Nonintrusive load monitoring (NILM) techniques are increasingly becoming a key instrument for identifying the power consumption of individual appliances based on a single metering point. Particularly, deep learning (DL) models are gaining interest in this regard. However, the challenges brought by the NILM datasets and the nonavailability of common experimental guidelines tend to compromise comparison, research transparency, and replicability. The limited adoption of efficient research instruments and lack of best practices guidelines contribute in huge part to this problem, where no features, encouraging standardized formats for benchmarking, and results sharing are offered. To address these issues, we first present a brief overview of recent best practices for DL and highlight how deep NILM research can benefit from these practices. Furthermore, we suggest a novel open-source toolkit leveraging these practices, i.e., Deep-NILMTK. The proposed toolkit offers a common testing bed for NILM algorithms independently of the underlying deep learning framework with a modular NILM pipeline that can easily be customized. Furthermore, Deep-NILMTK introduces the concept of experiment templating to offer predesigned experiments allowing to enhancing research efficiency. Leveraging this concept and DL best practices, we present a case study of creating an online NILM benchmark repository1 considering eight of the most popular deep NILM algorithms. All sources relative to the tool are made publicly available on Github2 along with the corresponding documentation."
  },
  {
    "objectID": "research/articles/deepnilmtk-2024/index.html#abstract",
    "href": "research/articles/deepnilmtk-2024/index.html#abstract",
    "title": "Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines",
    "section": "",
    "text": "Nonintrusive load monitoring (NILM) techniques are increasingly becoming a key instrument for identifying the power consumption of individual appliances based on a single metering point. Particularly, deep learning (DL) models are gaining interest in this regard. However, the challenges brought by the NILM datasets and the nonavailability of common experimental guidelines tend to compromise comparison, research transparency, and replicability. The limited adoption of efficient research instruments and lack of best practices guidelines contribute in huge part to this problem, where no features, encouraging standardized formats for benchmarking, and results sharing are offered. To address these issues, we first present a brief overview of recent best practices for DL and highlight how deep NILM research can benefit from these practices. Furthermore, we suggest a novel open-source toolkit leveraging these practices, i.e., Deep-NILMTK. The proposed toolkit offers a common testing bed for NILM algorithms independently of the underlying deep learning framework with a modular NILM pipeline that can easily be customized. Furthermore, Deep-NILMTK introduces the concept of experiment templating to offer predesigned experiments allowing to enhancing research efficiency. Leveraging this concept and DL best practices, we present a case study of creating an online NILM benchmark repository1 considering eight of the most popular deep NILM algorithms. All sources relative to the tool are made publicly available on Github2 along with the corresponding documentation."
  },
  {
    "objectID": "research/articles/deepnilmtk-2024/index.html#figure",
    "href": "research/articles/deepnilmtk-2024/index.html#figure",
    "title": "Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines",
    "section": "Figure",
    "text": "Figure\nFigure 1: Simulation results\n\n\n\nFigure 1: Simulation results"
  },
  {
    "objectID": "research/articles/deepnilmtk-2024/index.html#citation",
    "href": "research/articles/deepnilmtk-2024/index.html#citation",
    "title": "Unlocking the Full Potential of Neural NILM: On Automation, Hyperparameters, and Modular Pipelines",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@article{WitesmanHeiss:2016,\n    Author = {Eva Witesman and Anthony Faustine},\n    Doi = {10.1007/s11266-016-9684-5},\n    Journal = {Voluntas: International Journal of Voluntary and Nonprofit Organizations},\n    Month = {8},\n    Number = {4},\n    Pages = {1500--1528},\n    Title = {Nonprofit Collaboration and the Resurrection of Market Failure: How a Resource-Sharing Environment Can Suppress Social Objectives},\n    Volume = {28},\n    Year = {2016}}"
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html",
    "href": "research/articles/mlpf-2024/index.html",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html#important-links",
    "href": "research/articles/mlpf-2024/index.html#important-links",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html#abstract",
    "href": "research/articles/mlpf-2024/index.html#abstract",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "Abstract",
    "text": "Abstract\nPower demand forecasting is becoming a crucial tool for the planning and operation of Low Voltage (LV) distribution systems. Most importantly, the high penetration of Photovoltaics (PV) power generation as part of Distributed Energy Resource (DER)s has transformed the power demand forecasting problem at the distribution level into net-load forecasting. This paper introduces a novel and scalable approach to probabilistic forecasting at LV substation with PV generation. It presents a multi-variates probabilistic forecasting approach, leveraging Quantile Regression (QR). The proposed architecture uses a computationally efficient feed-forward neural net to capture the complex interaction between the historical load demands and covariate variables such as solar irradiance. It is empirically demonstrated that the proposed method can efficiently produce well-calibrated forecasts, both auto-regressively or in a single forward pass. Furthermore, a benchmark against four state-of-the-art forecasting approaches shows that the proposed approach offers a desirable trade-off between forecasting accuracies, calibrated uncertainty, and computation complexity."
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html#important-figures",
    "href": "research/articles/mlpf-2024/index.html#important-figures",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2: Predicted probability of impartiality by levels of equality and democracy\n\n\n\n\n\nFigure 4: Tangents to the prediction function at 25 and 50"
  },
  {
    "objectID": "research/articles/mlpf-2024/index.html#citation",
    "href": "research/articles/mlpf-2024/index.html#citation",
    "title": "Efficiency through Simplicity: MLP-based Approach for Net-Load Forecasting with Uncertainty Estimates in Low-Voltage Distribution Networks",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{10529636,\n  author={Faustine, Anthony and Nunes, Nuno Jardim and Pereira, Lucas},\n  journal={IEEE Transactions on Power Systems}, \n  title={Efficiency Through Simplicity: MLP-Based Approach for Net-Load Forecasting With Uncertainty Estimates in Low-Voltage Distribution Networks}, \n  year={2025},\n  volume={40},\n  number={1},\n  pages={46-56},\n  keywords={Forecasting;Uncertainty;Probabilistic logic;Predictive models;Substations;Load modeling;Distribution networks;Deep Neural Networks (DNN) Feed-forward Neural Network (FFN) Low Voltage (LV) distribution substation;Multilayer Perceptron (MLP);net-load;probabilistic forecasting;Photovoltaics (PV) generation;quantile regression (QR)},\n  doi={10.1109/TPWRS.2024.3400123}}"
  },
  {
    "objectID": "research/articles/nilm-review/index.html",
    "href": "research/articles/nilm-review/index.html",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/nilm-review/index.html#important-links",
    "href": "research/articles/nilm-review/index.html#important-links",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/nilm-review/index.html#abstract",
    "href": "research/articles/nilm-review/index.html#abstract",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "Abstract",
    "text": "Abstract\nThe key advantage of smart meters over traditional metering devices is their ability to transfer consumption information to remote data processing systems. Besides enabling the automated collection of a customer‚Äôs electricity consumption for billing purposes, the data collected by these devices makes the realization of many novel use cases possible. However, the large majority of such services are tailored to improve the power grid‚Äôs operation as a whole. For example, forecasts of household energy consumption or photovoltaic production allow for improved power plant generation scheduling. Similarly, the detection of anomalous consumption patterns can indicate electricity theft and serve as a trigger for corresponding investigations. Even though customers can directly influence their electrical energy consumption, the range of use cases to the users‚Äô benefit remains much smaller than those that benefit the grid in general. In this work, we thus review the range of services tailored to the needs of end-customers. By briefly discussing their technological foundations and their potential impact on future developments, we highlight the great potentials of utilizing smart meter data from a user-centric perspective. Several open research challenges in this domain, arising from the shortcomings of state-of-the-art data communication and processing methods, are furthermore given. We expect their investigation to lead to significant advancements in data processing services and ultimately raise the customer experience of operating smart meters."
  },
  {
    "objectID": "research/articles/nilm-review/index.html#important-figures",
    "href": "research/articles/nilm-review/index.html#important-figures",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 2: Predicted probability of impartiality by levels of equality and democracy\n\n\n\n\n\nFigure 4: Tangents to the prediction function at 25 and 50"
  },
  {
    "objectID": "research/articles/nilm-review/index.html#citation",
    "href": "research/articles/nilm-review/index.html#citation",
    "title": "Watt‚Äôs up at Home? Smart Meter Data Analytics from a Consumer-Centric Perspective",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{10529636,\n  author={Faustine, Anthony and Nunes, Nuno Jardim and Pereira, Lucas},\n  journal={IEEE Transactions on Power Systems}, \n  title={Efficiency Through Simplicity: MLP-Based Approach for Net-Load Forecasting With Uncertainty Estimates in Low-Voltage Distribution Networks}, \n  year={2025},\n  volume={40},\n  number={1},\n  pages={46-56},\n  keywords={Forecasting;Uncertainty;Probabilistic logic;Predictive models;Substations;Load modeling;Distribution networks;Deep Neural Networks (DNN) Feed-forward Neural Network (FFN) Low Voltage (LV) distribution substation;Multilayer Perceptron (MLP);net-load;probabilistic forecasting;Photovoltaics (PV) generation;quantile regression (QR)},\n  doi={10.1109/TPWRS.2024.3400123}}"
  },
  {
    "objectID": "research/articles/weighted-recurrence-nilm/index.html",
    "href": "research/articles/weighted-recurrence-nilm/index.html",
    "title": "Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring",
    "section": "",
    "text": "To this day, hyperparameter tuning remains a cumbersome task in Non-Intrusive Load Monitoring (NILM) research, as researchers and practitioners are forced to invest a considerable amount of time in this task. This paper proposes adaptive weighted recurrence graph blocks (AWRG) for appliance feature representation in event-based NILM. An AWRG block can be combined with traditional deep neural network architectures such as Convolutional Neural Networks for appliance recognition. Our approach transforms one cycle per activation current into an weighted recurrence graph and treats the associated hyper-parameters as learn-able parameters. We evaluate our technique on two energy datasets, the industrial dataset LILACD and the residential PLAID dataset. The outcome of our experiments shows that transforming current waveforms into weighted recurrence graphs provides a better feature representation and thus, improved classification results. It is concluded that our approach can guarantee uniqueness of appliance features, leading to enhanced generalisation abilities when compared to the widely researched V-I image features. Furthermore, we show that the initialisation parameters of the AWRG‚Äôs have a significant impact on the performance and training convergence."
  },
  {
    "objectID": "research/articles/weighted-recurrence-nilm/index.html#abstract",
    "href": "research/articles/weighted-recurrence-nilm/index.html#abstract",
    "title": "Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring",
    "section": "",
    "text": "To this day, hyperparameter tuning remains a cumbersome task in Non-Intrusive Load Monitoring (NILM) research, as researchers and practitioners are forced to invest a considerable amount of time in this task. This paper proposes adaptive weighted recurrence graph blocks (AWRG) for appliance feature representation in event-based NILM. An AWRG block can be combined with traditional deep neural network architectures such as Convolutional Neural Networks for appliance recognition. Our approach transforms one cycle per activation current into an weighted recurrence graph and treats the associated hyper-parameters as learn-able parameters. We evaluate our technique on two energy datasets, the industrial dataset LILACD and the residential PLAID dataset. The outcome of our experiments shows that transforming current waveforms into weighted recurrence graphs provides a better feature representation and thus, improved classification results. It is concluded that our approach can guarantee uniqueness of appliance features, leading to enhanced generalisation abilities when compared to the widely researched V-I image features. Furthermore, we show that the initialisation parameters of the AWRG‚Äôs have a significant impact on the performance and training convergence."
  },
  {
    "objectID": "research/articles/weighted-recurrence-nilm/index.html#figure",
    "href": "research/articles/weighted-recurrence-nilm/index.html#figure",
    "title": "Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring",
    "section": "Figure",
    "text": "Figure\nFigure 1: Simulation results\n\n\n\nFigure 1: Simulation results"
  },
  {
    "objectID": "research/articles/weighted-recurrence-nilm/index.html#citation",
    "href": "research/articles/weighted-recurrence-nilm/index.html#citation",
    "title": "Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring",
    "section": "Citation",
    "text": "Citation\n\n‚ÄÉAdd to Zotero \n\n@ARTICLE{9144492,\n  author={Faustine, Anthony and Pereira, Lucas and Klemenjak, Christoph},\n  journal={IEEE Transactions on Smart Grid}, \n  title={Adaptive Weighted Recurrence Graphs for Appliance Recognition in Non-Intrusive Load Monitoring}, \n  year={2021},\n  volume={12},\n  number={1},\n  pages={398-406},\n  keywords={Feature extraction;Monitoring;Aggregates;Neural networks;Current measurement;Voltage measurement;Energy consumption;Non-intrusive load monitoring;load disaggregation;appliance recognition;weighted recurrence graphs;recurrence plots;V-I trajectories;convolutional neural networks;deep neural networks},\n  doi={10.1109/TSG.2020.3010621}}"
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html",
    "href": "research/conferences/cired-chicago-2024/index.html",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "",
    "text": "Paper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html#important-links",
    "href": "research/conferences/cired-chicago-2024/index.html#important-links",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "",
    "text": "Paper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html#abstract",
    "href": "research/conferences/cired-chicago-2024/index.html#abstract",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "Abstract",
    "text": "Abstract\nThis paper presents a probabilistic forecasting approach tailored for low voltage (LV) substations, offering short-term predictions for three crucial variables: voltage, reactive power, and active power. These parameters play a vital role in the resilience of distribution systems, especially in the presence of Distributed Energy Resources (DERs). Evaluation with simulated data shows that active and reactive power forecasts degrade notably with higher EV penetration, whereas voltage forecasting experiences less degradation across all scenarios."
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html#figure",
    "href": "research/conferences/cired-chicago-2024/index.html#figure",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "Figure",
    "text": "Figure\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution.\n\n\n\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution."
  },
  {
    "objectID": "research/conferences/cired-chicago-2024/index.html#bibtex-citation",
    "href": "research/conferences/cired-chicago-2024/index.html#bibtex-citation",
    "title": "Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@INPROCEEDINGS{10916091,\n  author={Faustine, Anthony and Pereira, Lucas},\n  booktitle={CIRED Chicago Workshop 2024: Resilience of Electric Distribution Systems}, \n  title={Enhancing LV system resilience through probabilistic forecasting of interdependent variables: voltage, reactive and active power}, \n  year={2025},\n  volume={2024},\n  number={},\n  pages={27-31},\n  keywords={},\n  doi={10.1049/icp.2024.2555}}"
  },
  {
    "objectID": "research/conferences/forecast-symposium-2024/index.html",
    "href": "research/conferences/forecast-symposium-2024/index.html",
    "title": "Scalable and Efficient MLP-based Fully Parameterised Quantile for Probabilistic Power Forecasting",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/forecast-symposium-2024/index.html#important-links",
    "href": "research/conferences/forecast-symposium-2024/index.html#important-links",
    "title": "Scalable and Efficient MLP-based Fully Parameterised Quantile for Probabilistic Power Forecasting",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/conferences/forecast-symposium-2024/index.html#abstract",
    "href": "research/conferences/forecast-symposium-2024/index.html#abstract",
    "title": "Scalable and Efficient MLP-based Fully Parameterised Quantile for Probabilistic Power Forecasting",
    "section": "Abstract",
    "text": "Abstract\nIndustrial loads offer challenges for Non-intrusive Load Monitoring (NILM), such as phase imbalance associated with 3-phase lines. However, very little NILM research has been developed so far with this respect. This work presents a load recognition technique for NILM applying low complexity Fortesque Transform (FT). The FT decomposes the unbalanced 3-phase current waveform extracted from 3-phase aggregate power measurements to balance the given load. The 3-phases current waveform is transformed into an image-like representation using a compressedeuclidean distance matrix to improve the recognition ability further. The image representation is used as input to Convolutional Neural Network (CNN) classifier to learn the patterns of labeled data. Experimental evaluation of the industrial aggregated dataset shows that FT improves recognition performance by 5.8%, compared to the case without FT."
  },
  {
    "objectID": "research/conferences/forecast-symposium-2024/index.html#bibtex-citation",
    "href": "research/conferences/forecast-symposium-2024/index.html#bibtex-citation",
    "title": "Scalable and Efficient MLP-based Fully Parameterised Quantile for Probabilistic Power Forecasting",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@INPROCEEDINGS{10096324,\n  author={Faustine, Anthony and Pereira, Lucas},\n  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, \n  title={Applying Symmetrical Component Transform for Industrial Appliance Classification in Non-Intrusive Load Monitoring}, \n  year={2023},\n  volume={},\n  number={},\n  pages={1-5},\n  keywords={Load monitoring;Symmetric matrices;Power measurement;Power demand;Transforms;Signal processing;Convolutional neural networks;NILM;Industrial Appliances;Three-Phase;Fortesque Transform;Symmetrical Components},\n  doi={10.1109/ICASSP49357.2023.10096324}}"
  },
  {
    "objectID": "uses/index.html",
    "href": "uses/index.html",
    "title": "Anthony Faustine",
    "section": "",
    "text": "People often ask me what programs I use for my writing and design. In truth, my workflow tends to look like this or this, but here‚Äôs a more detailed list of all the interconnected programs I use.\nI try to keep this updated fairly regularly. As of April 27, 2025 this is what I‚Äôm using:"
  },
  {
    "objectID": "uses/index.html#writing",
    "href": "uses/index.html#writing",
    "title": "Anthony Faustine",
    "section": "Writing",
    "text": "Writing\n\nI permanently ditched Word as a writing environment in 2008 after starting grad school. I do all my writing in pandoc-flavored Markdown (including e-mails and paper-and-pencil writing)‚Äîit‚Äôs incredibly intuitive, imminently readable, flexible, future proof, and lets me ignore formatting and focus on content.\nThe key to my writing workflow is the magical pandoc, which converts Markdown files into basically anything else. I use Quarto to convert Markdown to HTML, PDF (through LaTeX), Word, and any other output format.\nI do my academic writing in several different programs: for stats-heavy stuff, I use Positron, and for prose-heavy stuff, I use iA Writer or Typora. I used to use Ulysses (and still think it‚Äôs a fantastic app!), but I found that I wasn‚Äôt using it as much in the past few years as I‚Äôve switched to Quarto for my writing.\nI store all my bibliographic references, books, and articles in Zotero (see here for why).\nI read and annotate all my PDFs with Zotero, both on desktop and on iOS, since it can export annotations as clean plain text.\nI store all my notes in Obsidian. Before switching to Obsidian I used Bear, which was great but didn‚Äôt support fancier things like math or syntax highlighting. Before that, I used Evernote, but I abandoned it in September 2018 after 9 years of heavy use, given their ongoing privacy controversies and mass layoffs."
  },
  {
    "objectID": "uses/index.html#development",
    "href": "uses/index.html#development",
    "title": "Anthony Faustine",
    "section": "Development",
    "text": "Development\nScience and research\n\nI post almost everything I write or develop on GitHub.\nI use R and either RStudio or Positron for most of my statistical computing, and I‚Äôm a dedicated devotee of the tidyverse. In the interest of full reproducibility and transparency, I make Quarto websites for each of my projects. See a list of these websites.\nI also use Python occasionally. Every few months I play with pandas and numpy and Jupyter, but I‚Äôm far more comfortable with R for scientific computing.\nI adapted the idea for research haikus from Kirby Nielsen.\nI use The Rogue Scholar to create stable DOIs for each of my blog posts.\nWeb\n\nI run my main web server on a DigitalOcean droplet, and I spin up temporary droplets all the time to offload scraping scripts, complicated R models, and to create on-the-fly VPNs.\nI normally access my remote files through SSH in a terminal, but for more complicated things, I‚Äôve found that Mountain Duck is indispensable.\nMy website uses Quarto.\nI use Let‚Äôs Encrypt for SSL.\nMiscellaneous\n\nI use a system-wide hotkey (ctrl + `) to open iTerm2 from anywhere.\nI use Homebrew to install Unix-y programs.\nI‚Äôm partial to both Fira Code and Consolas for my monospaced fonts."
  },
  {
    "objectID": "uses/index.html#desktop-apps",
    "href": "uses/index.html#desktop-apps",
    "title": "Anthony Faustine",
    "section": "Desktop apps",
    "text": "Desktop apps\nGraphic design\n\nThough I regularly use LaTeX (through pandoc), I adore InDesign and use it to make fancier academic and policy documents. I also used it for all the typesetting I did for BYU‚Äôs Neal A. Maxwell Institute, and continue to use it for the books I typset for By Common Consent Press.\nI use Illustrator all the time to enhance graphics I make in R and to make non-data-driven figures and diagrams.\nI use Lightroom and Photoshop too, but less often nowadays.\nDespite my dislike for Word and Excel, I use PowerPoint for all my presentations. It‚Äôs not my favorite, but in the apocryphal words of Churchill, ‚ÄúPowerPoint is the worst form of slide editor, except for all the others.‚Äù\nProductivity\n\nMy secret for avoiding the siren call of the internet is Focus. I have a blocklist that blocks Bluesky, Mastodon, Facebook, and Instagram from 8:00 AM‚Äì5:00 PM and 9:00 PM‚Äì11:59 PM.\nI was an early convert to Todo.txt and used it for years until my tasks and projects got too unwieldy. I switched to Taskpaper for a while, used 2Do for a couple years, and now I‚Äôm a convert to OmniFocus.\n\nFantastical 2‚Äôs natural language input is a glorious thing.\nI use Timery as an interface to Toggl to track my time during the day\nI keep a log of what I work on (and occasionally do more traditional diary-like entries) with Day One on both iOS and macOS.\nI use Espanso to replace and expand a ton of snippets‚Äîsee them all here, and I use Raycast to run dozens of little scripts that help control my computer with the keyboard.\nI use √úbersicht to show weather, iTunes track information, and my todo lists on my desktop.\nI use Dropbox for syncing stuff across my different devices and use Backblaze to back up all the computers in our house to the cloud.\nWith all these little helper apps, I use Bartender to keep my menubar clean."
  },
  {
    "objectID": "uses/index.html#hardware",
    "href": "uses/index.html#hardware",
    "title": "Anthony Faustine",
    "section": "Hardware",
    "text": "Hardware\n\nI use a 2021 14‚Ä≥ M1 Max MacBook Pro, a 13‚Ä≥ iPad Pro, and an iPhone 13.\nI use a Logitech Spotlight Presentation Remote when presenting, and when I teach in hybrid settings, I use an Insta360 Link motion sensing camera and a wireless lavalier microphone."
  },
  {
    "objectID": "blog/2025/10/bayesian-modelling-01.html",
    "href": "blog/2025/10/bayesian-modelling-01.html",
    "title": "Understanding Bayesian Thinking for Industrial Applications",
    "section": "",
    "text": "Introduction\nA company has recently installed a new, expensive machine. A critical question arises: How long will it last before failure?. The lead engineer, drawing on experience with previous models, estimates a lifespan of approximately 10 years. However, only 3 months of real-world test data are available for this specific unit, and a major warranty and service contract decision must be made immediately.\nThis scenario exemplifies a common challenge in industrial applications: making informed decisions with limited data. Bayesian thinking offers a powerful framework to address such problems by combining prior knowledge with observed data to update our beliefs about uncertain parameters.\nIn this article, we will explore the fundamentals of Bayesian thinking and how it can be applied to industrial scenarios like the one described above. We will cover key concepts such as prior distributions, likelihood functions, and posterior distributions, and demonstrate how to implement Bayesian models using Python‚Äôs PyMC library.\nTo immediately dive into the code and reproduce the models discussed in this article, you can use our accompanying resources:\n\nRepository: Fork bayesian-modelling repository and follow the setup instructions in the README.md file.\nNotebook: Launch the bayesian-modelling-01.ipynb notebook located in the notebook folder to follow along step-by-step.\n\n\n\nThe Building Blocks of Bayesian modeling\nTraditional (Frequentist) statistics relies on large datasets the ‚Äúlong run‚Äù to produce confident conclusions, a limitation in industrial contexts where data are often sparse. New products, machines, or processes typically generate only small samples, while valuable expert knowledge such as an engineer‚Äôs lifespan estimate is excluded from conventional models.\nBayesian inference overcomes these issues by combining prior knowledge with new data, enabling faster and more informed decisions when information is limited. This integration of expertise and evidence defines Bayesian thinking.\nThe process of updating our beliefs is formalized by Bayes‚Äô Theorem. \\[\nP(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}\n\\]\nWhile the formula looks mathematical, its components represent a beautifully intuitive learning cycle.\n\\[\n\\mathrm{Posterior} \\propto \\mathrm{Likelihood} \\times \\mathrm{Prior}\n\\]\nLet us break down how this relates to our machine failure problem; where \\(\\theta\\) is the machine‚Äôs expected failure rate, and \\(D\\) is the 3 months of test data.\n\nPrior, Likelihood, and Posterior\n\nPrior \\(P(\\theta)\\) represents the initial belief before observing any data. It incorporates domain expertise and historical knowledge. For instance, historical records may indicate that similar machines have an average lifespan of approximately ten years, implying a low failure rate. This prior belief defines the starting point for ùúÉ \\(\\theta\\).\nLikelihood \\(P(D \\mid \\theta)\\) quantifies the compatibility between the observed data and a given parameter value. It expresses the probability of observing the test outcomes for different possible failure rates. In this context, the likelihood measures how probable it is to observe zero failures within three months if the true average lifespan were, for example, five or fifteen years.\nPosterior \\(P(\\theta \\mid D)\\) represents the updated belief after incorporating the observed data. It integrates prior knowledge with the evidence provided by the likelihood. In the machine-failure example, the posterior distribution expresses the updated estimate of expected lifespan after combining historical information (e.g., the ten-year prior) with the three months of failure-free operational data.\n\n\n\nPyMC: The Probabilistic Programming Engine\nUnderstanding the relationship \\(\\mathrm{Posterior} \\propto \\mathrm{Likelihood} \\times \\mathrm{Prior}\\) is the conceptual heart of Bayesian analysis. However, calculating the actual posterior distribution, \\(P(\\theta \\mid D)\\), often involves complex, multi-dimensional integration that is impossible to solve analytically for real-world industrial problems. This challenge is addressed through Probabilistic Programming Languages (PPLs) such as PyMC.\nPyMC is an open-source Python library for constructing and fitting Bayesian statistical models using advanced computational algorithms, including Markov Chain Monte Carlo (MCMC) and variational inference. It is one of several modern PPLs available in Python, alongside Pyro and TensorFlow Probability (TFP). This tutorial focuses on PyMC due to its clarity, community support, and extensive documentation.\n\n\n\nCase study: A/B Testing with Small Samples\nTo shift from theory to practical implementation, we will apply the Bayesian building blocks Prior, Likelihood, and Posterior to a concrete industrial problem common in tech and e-commerce: A/B Testing\n\nSuppose you are a data scientist at an e-commerce company. The marketing team just launched a new website feature and wants to know:\n\n\nWhat‚Äôs the true conversion rate?\nIs it better than the old version (which historically has an 8% conversion rate)?\nHow much should we trust this estimate with limited data?\n\n\nDuring the first few days of the feature launch, the company has observed 200 visitors with only 15 conversion.\n\n\nLibrary Imports\nImport the required Python libraries for Bayesian modeling:\n\nNumPy for numerical computations\nPandas for data manipulation\nPyMC for Bayesian statistical modeling\nMatplotlib, arviz and altair for visualization\n\n\n\nShow the code\n#import libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nimport pymc as pm\nimport arviz as az\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import clear_output\nfrom great_tables import GT\nimport matplotlib.pyplot as plt\naz.style.use(\"arviz-white\")\nfrom cycler import cycler\ncolors=['#107591','#f69a48','#00c0bf', '#fdcd49',\"#cf166e\", \n        \"#7035b7\", \"#212121\",\"#757575\", \"#E0E0E0\",\"#FAFAFA\"]\nplt.rcParams.update({\n    \"figure.dpi\": 100,\n    \"axes.labelsize\": 12,\n    \"axes.titlesize\": 12,\n    \"figure.titlesize\": 12,\n        \"font.size\": 12,\n        \"legend.fontsize\": 12,\n        \"xtick.labelsize\": 12,\n        \"ytick.labelsize\": 12,\n        \"axes.linewidth\" : 0.5,\n        \"lines.linewidth\" : 1.,\n        \"legend.frameon\" :False,\n        'axes.prop_cycle': cycler(color=colors)\n        \n})\nimport altair \naltair.themes.enable('carbonwhite')\nimport altair as alt\nalt.data_transformers.enable('default', max_rows=None)\nclear_output()\n\n\n\nDefine key variables and parameters\n\n\nShow the code\n# For reproducibility\nnp.random.seed(42)\n\n# A/B Test Parameters\nvisitors = 200\nconversions = 15\nobserved_conversion_rate = conversions / visitors\nhistorical_baseline = 0.08\n\n\n\n\n\nDefine Prior, likelihood, and Posterior in PyMC\nTo model this problem in PyMC, one must first define the Prior and Likelihood distributions\nPrior: Since the conversion rate \\(\\theta\\), can only range between 0 and 1. The Beta distribution is ideal for modeling parameters that are bounded between 0 and 1, such as probabilities or rates.\nThe Beta distribution is controlled by two parameters, \\(\\alpha\\) and \\(\\beta\\). hese parameters are set to formally encode the prior knowledge: the historical 8% conversion rate. The mean of a \\(\\mathrm{Beta}(\\alpha,\\beta)\\) distribution is \\(\\frac{\\alpha}{ \\alpha + \\beta}\\). Since the historical rate is 8% (or 0.08), we need to choose \\(\\alpha\\) and \\(\\beta\\) such that: \\[\n\\frac{\\alpha}{ \\alpha + \\beta} = 0.08\n\\]\nTo determine the strength of this belief, a number that represents the effective sample size (ESS) of the historical knowledge is chosen. Choosing a hypothetical ESS of 100 trials, we can solve for \\(\\alpha\\) and \\(\\beta\\): - ESS \\(= \\alpha + \\beta = 100\\) - \\(\\alpha\\) (hypothetical successes) \\(=100√ó0.08=8\\) - \\(\\beta\\) (hypothetical failures) \\(=100‚àí8=92\\)\nLikelihood the Likelihood is determined by the process that generated the data. Since there is a fixed number of trials (N=200 visitors) and the number of successes (k=15 conversions) is counted, this is a Binomial distribution.\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n    \n    # Sample from posterior distribution\n    trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True)\n\nclear_output()\n\n\nThis small block of code above defines and runs our entire Bayesian analysis. For those seeing PyMC for the first time, here is what each section is doing:\n\nwith pm.Model() as model: This block acts as a container for all the random variables and data in our model. Everything inside this context belongs to the conversion_model\nconversion_rate = pm.Beta(...): We are telling PyMC that the true conversion_rate is a random variable, and our initial belief is described by the Beta(8,92) distribution.\nlikelihood = pm.Binomial(...): This defines the process that generated our observed data. We link the conversion_rate parameter to the actual observed data (n=visitors, observed=conversions) using the appropriate Binomial distribution.\npm.sample(...): This is where the magic happens! The pm.sample function runs the MCMC sampler (the computational engine) to combine the Prior and the Likelihood, effectively calculating the Posterior distribution. We ask the sampler to draw 2000 samples after a 1000-sample tuning period, running 4 independent chains to ensure reliable results.\n\n\n\n\nModel diagnostics\nRunning pm.sample() generates the raw output, but the job isn‚Äôt done yet. Before we trust the results, we must perform Model Diagnostics to ensure our computational engine (the MCMC sampler) has worked correctly. The single most important diagnostic check is confirming Convergence.\n\nModel Convergencevergence\nIn Bayesian inference, Markov Chain Monte Carlo (MCMC) methods are employed to sample from the complex posterior distribution. These samples are relied upon to accurately estimate quantities like the mean conversion rate or its credible interval.\nConvergence is the guarantee that the MCMC chains have explored the entire distribution and are now producing samples that truly represent the target Posterior distribution, and are not just stuck in a starting location.\n\nAnalogy: Imagine trying to understand the shape of a deep, misty valley (the posterior). If your chains haven‚Äôt converged, they might be stuck high up on a ridge, missing the true, deep center. Diagnostics are the tools we use to confirm the chains have found and are walking across the bottom of the true valley.\n\nPyMC uses the supporting library ArviZ for standardizing and analyzing the results, which provides the following diagnostics and plots\n\nTrace Plots: Visual inspection of parameter samples across iterations.\n\nGood trace plots look like fuzzy caterpillars with no trends or jumps.\n\nR-hat (Gelman-Rubin Statistic): Measures how well multiple chains agree.\n\nR-hat ‚âà 1 means convergence.\nR-hat &gt; 1.01 suggests problems.\n\nEffective Sample Size (ESS): Indicates how many independent samples you effectively have.\n\nLow ESS means poor mixing or autocorrelation. Good ESS is typically &gt; 200 per parameter.\n\n\nThe most efficient way to check convergence numerically is using the ArviZ summary function, specifically asking for the diagnostics az.summary(trace, kind=\"diagnostics\"). Alternatively, az.plot_trace(trace) can be used to get a visual sense of convergence.\n\nExample Diagnostic Output\n\n\nShow the code\ndiag_table=az.summary(trace, kind=\"diagnostics\")[['ess_bulk', 'ess_tail', 'r_hat']]\nGT(diag_table).tab_header(\n    title=\"\",\n    subtitle=\"Conversion Rate Model\"\n).cols_label({\n        'ess_bulk': 'ESS Bulk',\n        'ess_tail': 'ESS Tail.',\n        'r_hat': 'R-hat',\n    })\n\n\n\n\n\n\n\n\n\n\n\nConversion Rate Model\n\n\nESS Bulk\nESS Tail.\nR-hat\n\n\n\n\n3966.0\n5559.0\n1.0\n\n\n\n\n\n\n\n\nThis table immediately indicates that the model is reliable and ready for analysis\n\n(R_hat = 1.0,Goal Achieved): Since R_hat is exactly 1.0, this confirms that the four independent MCMC chains have fully converged and agree on the shape of the posterior distribution. The model is reliable.\nESS bulk ‚Äãand ESS tail (3966.0 and 5559.0, Goal Achieved): Both effective sample sizes are significantly greater than the ‚â•400 minimum threshold. This means there are plenty of high-quality, effectively independent samples to accurately estimate the mean, mode, and credible intervals of the true conversion rate.\n\n\nVisual Check: Trace Plots\nWhile the numbers in the summary table are essential, visually inspecting the MCMC chains confirms the story.\n\n\nShow the code\naz.plot_trace(trace);\n\n\n\n\n\n\n\n\n\nThe trace plot above shows excellent convergence for our conversion rate (\\(\\theta\\)) model, supporting the conclusions from our quantitative diagnostics. The plot is split into two panels 1. Right Panel: MCMC Sampling Behavior\n\nThis panel shows the raw sampled values across iterations for each of our four chains. The sampled values oscillate stably around ‚àº0.075 (7.5%) without any noticeable trends, sudden jumps, or long-term drifts.\nThe different lines (chains) are thoroughly intertwined and overlap completely. This ‚Äúfuzzy caterpillar‚Äù appearance is the visual proof that the sampler is efficiently exploring the parameter space and that all chains have converged to the same distribution.\nThe stable behavior confirms the chain has reached stationarity, meaning it is now sampling from the true, converged Posterior distribution.\n\n\nLeft Panel: Posterior Distribution\n\nThis panel shows the estimated Posterior probability density function (PDF) based on the samples. The shape is smooth and unimodal (single peak), indicating a well-behaved posterior without ambiguity.\nThe peak is clearly centered at a value close to our observed rate (7.5%), which is what we expect when combining a prior (8%) and data (7.5%).\nThe spread of the distribution clearly visualizes our remaining uncertainty about the true conversion rate.\n\n\n\n\n\n\n\nPrior Predictive Checks: Validating Model Assumptions\nWhile we have demonstrated that the convergence of the fitted model a complete Bayesian analysis requires us to first validate the assumptions we made before seeing any data (priors). This validation is accomplished through the Prior Predictive Check.\n\nPrior Predictive Checks helps validate the prior assumptions before fitting the model to data. It show what kind of data the model expects to generate based solely on the prior beliefs.\n\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n\n\nGiven our prior belief of an 8% conversion rate, we can simulate what kind of data we would expect to see if this belief were true. This is done by generating synthetic datasets from the prior distribution and comparing them to the actual observed data. This is acomplished in PyMC by running pm.sample_prior_predictive() function.\n\n\nShow the code\nwith conversion_model:\n    prior_pred = pm.sample_prior_predictive()\n\n\nSampling: [conversion_rate, observations]\n\n\n\n\nShow the code\nfig, ax=plt.subplots(figsize=(4, 2.8))\naz.plot_ppc(prior_pred, group=\"prior\", ax=ax)\nplt.xlabel('conversions')\nplt.ylabel('Density');\n\n\n\n\n\n\n\n\n\nThe generated plot from the prior predictive sampling shows the distributions of simulated data (the number of conversions) created by sampling from the \\(\\text{Beta}(8,92)\\) prior distribution. - The distribution of the simulated data (prior predictive line) appear broadly spread out and relatively flat across a wide range (0 to 40+ conversions). This indicates that the prior allows for many different outcomes, and thus it is not overly restrictive. - The dashed line represents the average predicted number of conversions It is also quite flat and non-committal. - It evident that the prior predictive distribution does not overly concentrate around any specific number of conversions, which is desirable when we want to remain open to various possible outcomes. - So the prior predictive check confirms that our chosen Beta(8,92) prior is reasonable though it also quite weakly informative how?\nRed flags to watch for:\n\nImpossible values: Predictions outside the feasible range (e.g., negative conversion, &gt;200 observations)\nUnrealistic concentrations: If priors are too informative, you might see all predictions clustered in a narrow range\nPoor scaling: Predictions that don‚Äôt match the scale of your problem\n\n\nPosterior Predictive Checks\nPosterior Predictive Checks addresses the important test: Does the model actually make sense given the data observed?\nThis moves the process from confirming the samplers or priors to validating the model itself.\n\nPPCs evaluate model fit by comparing the observed data to simulated data from the posterior distribution. If a model accurately represents the data-generating process, the simulated data should resemble the actual observations.\n\nTo achive this it is important to generate new, simulated data from the posterior distribution and compare it to the actual observed data. This is done using the pm.sample_posterior_predictive function in PyMC.\n\n\nShow the code\nwith pm.Model() as conversion_model:\n    # Prior distribution based on historical performance\n    conversion_rate = pm.Beta(\"conversion_rate\", alpha=8, beta=92)\n    \n    # Likelihood function\n    likelihood = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n    \n    # Sample from posterior distribution\n    trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True)\n\n\nwith conversion_model:\n    posterior_pred = pm.sample_posterior_predictive(trace, random_seed=42,)\n    \nclear_output()\n\n\n\n\nShow the code\nppc_summary = az.summary(posterior_pred, kind='stats', hdi_prob=0.95)\nppc_table = (\n    GT(ppc_summary)\n    .cols_label({\n        'mean': 'Mean',\n        'sd': 'Std-Dev.',\n        'hdi_2.5%': 'HDI 3%',\n        'hdi_97.5%': 'HDI 97%'\n    }).tab_header(\n        title=\"Posterior Predictive Summary Statistics\",\n        subtitle=\"\"\n    )\n)\nppc_table \n\n\n\n\n\n\n\n\nPosterior Predictive Summary Statistics\n\n\n\n\n\nMean\nStd-Dev.\nHDI 3%\nHDI 97%\n\n\n\n\n15.314\n4.806\n6.0\n24.0\n\n\n\n\n\n\n\n\nIt clear that the predicted mean (15.314) is extremely close to the actual observed count (15). This is a strong indication that the conversion_rate model fits the data very well. And thus the choice of the Beta Prior and Binomial Likelihood is appropriate for this data. The model predicts that 95% of the time, the number of conversions will fall between 6 and 24. Since the company observed value of 15 falls well within this 95% HDI, the actual observation is considered highly plausible according to your model.\n\n\nShow the code\nppc_samples = posterior_pred.posterior_predictive['observations'].values.flatten()\npercentile = (ppc_samples &lt;= conversions).mean() * 100\nprint(f\"Observed convervation ({conversions}) is at the {percentile:.1f}th percentile of predictions\")\n\n\nObserved convervation (15) is at the 53.7th percentile of predictions\n\n\nThe value of 53.7 is very close to the ideal 50, which provides further strong evidence (in addition to the mean of 15.314 already seen) that:\n\nThe model is not biased (it is not systematically over- or under-estimating the data).\nThe choice of the Beta-Binomial model is highly appropriate for this data set.\n\n\n\n\nModel Utility and Business Decision\nFollowing the confirmation of the reliability of the Bayesian model through the Posterior Predictive Check (PPC), the focus now shifts from ‚ÄúDoes the model fit the data?‚Äù to the commercially critical question: ‚ÄúIs the model useful for making business decisions?‚Äù\nSpecifically, the derived posterior distribution is utilized to quantify the evidence that the new feature‚Äôs conversion rate (Feature B) is superior to the existing, established conversion rate (Feature A, which has a known baseline rate of 0.08).\nTo achieve this, we need to extract the posterior distribution for the conversion_rate parameter that your model estimated.\n\n\nShow the code\nposterior_samples = trace.posterior.stack(sample=(\"chain\", \"draw\"))\nconversion_samples=posterior_samples['conversion_rate'].values\n\n\nNext, calculate the ‚Äúprobability of superiority‚Äù‚Äîthe probability that the new feature (B) is better than the old (A).\nFinally, the expected uplift in conversion rate can be computed and translated into business value. This involves calculating the difference between the posterior mean conversion rate and the historical rate, then multiplying by the number of visitors and average revenue per conversion.\n\n0.95: Strong Evidence. The new feature is very likely superior. Launching it should be considered.\n0.80: Moderate Evidence. The new feature is likely better, but there is still a 20% chance it is worse. The decision depends on company risk tolerance.\n0.50: No Evidence. The new feature is a toss-up; there is no statistical reason to prefer it over the old feature.\n\n\n\nShow the code\nprob_superiority = (conversion_samples &gt; historical_baseline).mean()\n# Display the result\nprint(f\"The probability that the new feature is better than the old rate (0.08) is: {prob_superiority:.1%}\")\n\n\nThe probability that the new feature is better than the old rate (0.08) is: 39.7%\n\n\nThe calculated Probability of Superiority is approximately 0.40, indicating that there is a 40% chance the new feature‚Äôs conversion rate is better than the old feature‚Äôs with 8% rate. This imply that there is a 60.3% chance the new feature is worse than the baseline.\nSince the probability that the new feature is superior is well below the neutral benchmark of 50%, the data does not support replacing the existing Feature A with the new Feature B based on conversion rate alone. The new feature is highly likely to perform worse.\n\nProbability of Meaningful Improvement.\nDepending on the business context, one might also want to calculate the probability that the new feature is better by a meaningful margin (e.g., &gt;1% improvement). This calculates the chance that the new feature is better than the old feature by a practically significant margin of at least 1 percentage point. This is a crucial business metric. Sometimes a tiny statistical ‚Äúwin‚Äù is not worth the cost of development and deployment. If this probability is low, it confirms the feature is not a major improvement.\n\n\nShow the code\nprob_1pct_improvement = (conversion_samples &gt; historical_baseline+0.01).mean()\nprint(f\" Probability of &gt;1% improvement: {prob_1pct_improvement:.1%}\")\n\n\n Probability of &gt;1% improvement: 18.4%\n\n\n\n\nProbability of Hitting a Target Rate\nThis calculates the chance that the true conversion rate of the new feature is 10% or higher. This is useful if 10% is a specific, ambitious KPI (Key Performance Indicator) or goal set by the marketing or product team. It tells how likely the team is to meet its goal.\n\n\nShow the code\nprob_10pct = (conversion_samples &gt; 0.10).mean()\nprint(f\" Probability of &gt;10% improvement: {prob_10pct:.1%}\")\n\n\n Probability of &gt;10% improvement: 6.3%\n\n\n\nExpected Value of the Change.\nThis is the single-number best estimate of the average change (positive or negative) to expect upon deploying the new feature. This is the foundation for the ‚ÄúExpected Business Impact‚Äù section. If this value is negative, it represents an Expected Loss.\nIf one calculates: expected_uplift√óTotal¬†Visitors√óARPC, the estimated dollar value of the change is obtained.\nThe goal is to translate the statistically determined average change in conversion rate into a clear financial outcome for the business. This calculation provides the most actionable insight for the go/no-go decision on the new feature.\n\n\nShow the code\n# Calculate key business probabilities\nexpected_uplift = conversion_samples.mean() - historical_baseline\n\n# Calculate expected business impact\nmonthly_visitors = 10000\nexpected_additional_conversions = monthly_visitors * expected_uplift\nconversion_value = 50  # Average value per conversion\nexpected_monthly_value = expected_additional_conversions * conversion_value\n\nprint(f\"Expected uplift: {expected_uplift:.3f} ({expected_uplift:.1%} points)\")\nprint(f\"Expected monthly value: ${expected_monthly_value:,.0f}\")\n\n\nExpected uplift: -0.003 (-0.3% points)\nExpected monthly value: $-1,717\n\n\nWe see that the expected uplift is -0.030 (-3.0% points), indicating that, on average, the new feature is expected to decrease the conversion rate by 3 percentage points. The expected financial consequence of deploying the new feature is a loss of $1,717 per month\nThe data suggests that deploying the new feature would likely result in an Expected Monthly Loss of $1,717. This financial quantification is the most compelling reason to reject the new feature based on conversion rate performance.\nThe decision to proceed should only be considered if the feature provides other, unquantified benefits (e.g., improved customer retention, compliance, or brand value) that are estimated to be worth more than $1,717 per month.\n\n\n\nPrior sensitivity analysis\nAssessing how the choice of prior influences the final decision is important. This transparency is a key strength of Bayesian modeling, as it allows testing every assumption. Thus, one must determine how much the choice of prior affects the outcome by comparing several Beta priors. These comparisons will include priors from very optimistic (expecting a \\(15\\%\\) conversion rate) to skeptical (expecting only \\(4\\%\\)), plus an uninformative prior that lets the data speak entirely for itself.\n\n\nShow the code\n\npriors_to_test = [\n    (\"Very Optimistic\", 15, 85),    # Believes 15% conversion rate\n    (\"Optimistic\", 12, 88),         # Believes 12% conversion rate  \n    (\"Historical Based\", 8, 92),    # Our original prior (8%)\n    ('Strong Historical', 80, 920),      # Mean = 0.08, ESS = 1000\n    (\"Skeptical\", 4, 96),           # Believes 4% conversion rate\n    (\"Uninformative\", 1, 1),        # Let data dominate completely\n]\n\nsensitivity_results = []\ntraces_to_comprare = []\n\nfor idx, (prior_name, alpha, beta) in enumerate(priors_to_test):\n    with pm.Model() as model:\n        conversion_rate = pm.Beta(\"conversion_rate\", alpha=alpha, beta=beta)\n        obs = pm.Binomial(\"observations\", n=visitors, p=conversion_rate, observed=conversions)\n        trace = pm.sample(2000, tune=1000, chains=4, random_seed=42, return_inferencedata=True, progressbar=False)\n        \n \n    posterior_samples = trace.posterior.stack(sample=(\"chain\", \"draw\"))\n    conversion_samples=posterior_samples['conversion_rate'].values\n    expected_uplift = conversion_samples.mean() - historical_baseline\n    expected_additional_conversions = monthly_visitors * expected_uplift\n    expected_monthly_value = expected_additional_conversions * conversion_value\n\n\n    posterior_mean = conversion_samples.mean()\n    posterior_std = conversion_samples.std()\n    prior_std = np.sqrt((alpha * beta) / ((alpha + beta)**2 * (alpha + beta + 1)))\n    prob_better = (conversion_samples &gt; historical_baseline).mean()\n    uncertainty_reduction = max(0, (prior_std - posterior_std) / prior_std)\n    \n    df = pd.DataFrame({\"Prior\": prior_name,\n                       \"Posterior\": conversion_samples}\n                       )\n    \n    sensitivity_results.append({\n        'Prior_Belief': prior_name,\n        'Prior_Parameters': f\"Beta({alpha}, {beta})\",\n        'Prior_Mean': f\"{alpha/(alpha+beta):.3f}\",\n        'Posterior_Mean': f\"{posterior_mean:.3f}\",\n        'Prob_Better': f\"{prob_better:.1%}\",\n        \"Expected_uplift\": f\"${expected_uplift:.1%}\",\n        \"Expected_monthly_value\": f\"${expected_monthly_value:,.0f}\",\n        'Uncertainty_Reduction': f\"{uncertainty_reduction:.1%}\",\n        'Data_Influence': 'Strong' if uncertainty_reduction &gt; 0.8 else 'Moderate'\n    })\n    traces_to_comprare.append(df) \nsensitivity_df = pd.DataFrame(sensitivity_results)\ntraces_to_comprare=pd.concat(traces_to_comprare)\nclear_output()\n\n\n\nLet first visualise the different priors\n\n\nShow the code\ndomain = [\"Very Optimistic\", \"Optimistic\", \"Historical Based\", \"Strong Historical\", \"Skeptical\", \"Uninformative\"]\nreference_data = pd.DataFrame([\n    {'x': 0.08, 'label': 'Old Feature CR (8%)', 'color': 'red', 'label_x': 0.08},\n    {'x': 0.075, 'label': 'New Feature CR (7.5%)', 'color': 'green', 'label_x': 0.075}\n])\n\n\nchart = alt.Chart(traces_to_comprare).transform_density(density='Posterior', groupby=['Prior'], as_=['Posterior', 'density'])\\\n    .mark_line(opacity=0.5)\\\n    .encode(x='Posterior:Q', \n            y='density:Q', \n            color=alt.Color(\"Prior:N\").scale(domain=domain, range=colors[:len(domain)])\n            )\n\nreference_lines = alt.Chart(reference_data).mark_rule(\n    strokeDash=[5, 5], # Dashed line\n    size=1, \n).encode(\n    x='x:Q',\n    color=alt.Color('label:N', \n                    scale=alt.Scale(domain=reference_data['label'].tolist(), \n                                    range=reference_data['color'].tolist()),\n                    legend=alt.Legend(title=\"Conversion Rate\"))\n)\n\n\nfinal=chart + reference_lines \nfinal=final.properties(title='Posterior Distributions by Prior Belief', width=700, height=200).configure_axis(\n    # Set grid to False to remove all grid lines\n    grid=False\n).configure_view(\n    # Optional: Remove the surrounding border of the plot area\n    strokeWidth=0 \n)\nfinal.save('posterior_sensitivity.pdf')\nfinal\n\n\n\n\n\n\n\n\nThe figure above show Posterior belief distributions under different prior assumptions. Each curve in the figure represents a posterior distribution for the conversion rate under a different prior assumption. The horizontal axis shows possible conversion rates, and the vertical axis shows how plausible each value is after combining the prior belief with the observed data.\nDespite very different starting assumptions, the posterior estimates converge around 7‚Äì10%, showing that the data provide a stable, consistent signal largely independent of the chosen prior. All reasonable priors converge to the same conclusion: the new feature likely isn‚Äôt better than the old one.\nTo support decision-making, we compile a summary table comparing key metrics across these prior choices.\n\n\nShow the code\n# Create sensitivity analysis table\nsensitivity_table = (\n    GT(sensitivity_df)\n    .tab_header(\n        title=\"Sensitivity Analysis: Impact of Prior Beliefs\",\n        subtitle=\"How Different Starting Assumptions Affect Final Conclusions\"\n    )\n    .cols_label(\n        Prior_Belief=\"Prior Belief\",\n        Prior_Parameters=\"Prior Distribution\", \n        Prior_Mean=\"Prior Mean\",\n        Posterior_Mean=\"Posterior Mean\",\n        Prob_Better=\"Prob. Better\",\n        Data_Influence=\"Data Influence\",\n        Uncertainty_Reduction=\"Uncertainty Reduction\",\n        Expected_uplift=\"Expected Uplift\",\n        Expected_monthly_value=\"Expected Monthly Value\"\n    )\n    .data_color(\n        columns=[\"Prob_Better\"],\n        palette=[\"#C73E1D\", \"#F18F01\", \"#2E8B57\"],\n        domain=[0.0, 0.5, 1.0]\n    )\n    .data_color(\n        columns=[\"Data_Influence\"],\n        palette=[\"#2E8B57\", \"#F18F01\", \"#C73E1D\"],\n        domain=[\"Strong\", \"Moderate\", \"Weak\"]\n    ).data_color(\n        columns=[\"Uncertainty Reduction\"],\n        palette=[\"#C73E1D\", \"#F18F01\", \"#2E8B57\"],\n        domain=[0.0, 80, 100]\n    ).tab_source_note(\n        \"Analysis shows robustness of conclusions to different prior assumptions ‚Ä¢ \"\n        \"Even skeptical priors converge toward data-driven truth\"\n    )\n    .tab_options(\n        table_width=\"100%\",\n    )\n)\nclear_output()\nsensitivity_table\n\n\n\n\n\n\n\n\nSensitivity Analysis: Impact of Prior Beliefs\n\n\nHow Different Starting Assumptions Affect Final Conclusions\n\n\nPrior Belief\nPrior Distribution\nPrior Mean\nPosterior Mean\nProb. Better\nExpected Uplift\nExpected Monthly Value\nUncertainty Reduction\nData Influence\n\n\n\n\nVery Optimistic\nBeta(15, 85)\n0.150\n0.100\n88.2%\n$2.0%\n$9,930\n52.2%\nModerate\n\n\nOptimistic\nBeta(12, 88)\n0.120\n0.090\n71.0%\n$1.0%\n$4,992\n49.1%\nModerate\n\n\nHistorical Based\nBeta(8, 92)\n0.080\n0.077\n39.7%\n$-0.3%\n$-1,717\n44.1%\nModerate\n\n\nStrong Historical\nBeta(80, 920)\n0.080\n0.079\n47.3%\n$-0.1%\n$-263\n10.4%\nModerate\n\n\nSkeptical\nBeta(4, 96)\n0.040\n0.064\n11.8%\n$-1.6%\n$-8,231\n29.0%\nModerate\n\n\nUninformative\nBeta(1, 1)\n0.500\n0.079\n44.8%\n$-0.1%\n$-456\n93.5%\nStrong\n\n\n\nAnalysis shows robustness of conclusions to different prior assumptions ‚Ä¢ Even skeptical priors converge toward data-driven truth\n\n\n\n\n\n\n\n\n\nKey observations:\n\nPrior Pull: The data moderate extreme beliefs. The very optimistic prior (0.150) is pulled down to 0.100, while the skeptical prior (0.040) rises to 0.064‚Äîdemonstrating how new evidence shifts expectations toward a central value.\nPrior Strength vs.¬†Data Influence: The Strong Historical prior (Beta(80, 920)) and Uninformative prior (Beta(1, 1)) yield similar posteriors (~0.079, ~45% Prob_Better), but for opposite reasons. The former is dominated by prior belief; the latter, by data‚Äîshowing that strong evidence can overcome weak or missing priors.\nImpact on Decision Metrics:: The Very Optimistic prior suggests an 88% Prob_Better and +$9.9k expected value, favoring a launch. The Skeptical prior predicts only 12% Prob_Better and ‚Äì$8.2k, suggesting the opposite. Even moderate priors (e.g., 40‚Äì47%) could tip a go/no-go decision depending on the threshold.\n\n\n\n\nCommon bayesian pitfalls and best practices\nWhile Bayesian modeling provides a powerful framework for decision-making under uncertainty, several common mistakes can reduce its effectiveness.\n\nThe first is overconfident priors, where excessively strong prior assumptions prevent new data from influencing the results. This issue is best mitigated by starting with weak or moderately informative priors and refining them as more evidence becomes available.\n\n\nA second pitfall is ignoring prior sensitivity. Conclusions may shift significantly depending on the chosen prior distribution, so it is essential to conduct sensitivity analyses using multiple plausible priors to ensure that insights are robust.\n\n\nThe third major issue involves misinterpreting probability. A statement such as ‚Äú95% probability‚Äù does not imply absolute certainty; rather, it reflects the degree of belief given the available data and assumptions. Probabilities in Bayesian analysis are best interpreted in terms of relative confidence, risk, and decision trade-offs.\n\n\n\nNext steps in the bayesian journey\nWe have laid the groundwork by exploring the core components of the Bayesian approach. The subsequent session will focus on advanced practical implementation. We will apply this framework to critical survival analysis problems, modeling the probability and timing of events such as equipment failure.\n\n\nResources to continue learning\n\nBayesian Modeling and Computation in Python\nBayesian Methods for Hackers\nStatistical Rethinking\nDoing Bayesian Data Analysis\nBayesian Data Analysis\nPyMC Documentation\n\n\n\n\n\nCitationFor attribution, please cite this work as:\nFaustine, Anthony. 2025. ‚ÄúUnderstanding Bayesian Thinking for\nIndustrial Applications.‚Äù October 10, 2025. https://sambaiga.github.io/blog/2025/10/bayesian-modelling-01.html."
  }
]